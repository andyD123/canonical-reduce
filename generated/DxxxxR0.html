<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang xml:lang>
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Canonical Parallel Reduction</title>
  <style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}

ul.task-list[class]{list-style: none;}
ul.task-list li input[type="checkbox"] {
font-size: inherit;
width: 0.8em;
margin: 0 0.8em 0.2em -1.6em;
vertical-align: middle;
}
.display.math{display: block; text-align: center; margin: 0.5rem auto;}

pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } 
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } 
code span.at { color: #204a87; } 
code span.bn { color: #0000cf; } 
code span.cf { color: #204a87; font-weight: bold; } 
code span.ch { color: #4e9a06; } 
code span.cn { color: #8f5902; } 
code span.co { color: #8f5902; font-style: italic; } 
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } 
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } 
code span.dt { color: #204a87; } 
code span.dv { color: #0000cf; } 
code span.er { color: #a40000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #0000cf; } 
code span.fu { color: #204a87; font-weight: bold; } 
code span.im { } 
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } 
code span.kw { color: #204a87; font-weight: bold; } 
code span.op { color: #ce5c00; font-weight: bold; } 
code span.ot { color: #8f5902; } 
code span.pp { color: #8f5902; font-style: italic; } 
code span.sc { color: #ce5c00; font-weight: bold; } 
code span.ss { color: #4e9a06; } 
code span.st { color: #4e9a06; } 
code span.va { color: #000000; } 
code span.vs { color: #4e9a06; } 
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } 
</style>
  <style type="text/css">
body {
font-family: 'Segoe UI', 'DejaVu Serif', Georgia, serif;
max-width: 960px;
margin: 0 auto;
padding: 2em;
line-height: 1.6;
color: #333;
font-size: 11pt;
}
h1, h2, h3, h4, h5 {
font-family: 'Segoe UI', 'DejaVu Sans', Helvetica, sans-serif;
color: #003366;
margin-top: 1.5em;
}
h1 { font-size: 1.6em; border-bottom: 2px solid #003366; padding-bottom: 0.3em; }
h2 { font-size: 1.3em; border-bottom: 1px solid #003366; padding-bottom: 0.2em; }
h3 { font-size: 1.1em; }
h4 { font-size: 1.0em; }
a { color: #0066CC; }

pre {
background-color: #f5f5f5;
border: 1px solid #ccc;
border-radius: 3px;
padding: 0.8em;
overflow-x: auto;
font-size: 0.88em;
line-height: 1.4;
}
code {
font-family: 'DejaVu Sans Mono', 'Consolas', monospace;
font-size: 0.9em;
}
p > code, li > code, td > code {
background-color: #f0f0f0;
padding: 0.1em 0.3em;
border-radius: 2px;
}

table {
border-collapse: collapse;
width: 100%;
margin: 1em 0;
font-size: 0.92em;
}
th, td {
border: 1px solid #999;
padding: 0.4em 0.6em;
text-align: left;
}
th {
background-color: #003366;
color: white;
font-weight: bold;
}
tr:nth-child(even) { background-color: #f5f8fc; }

blockquote {
border-left: 3px solid #003366;
margin: 1em 0;
padding: 0.5em 1em;
background-color: #f5f8fc;
font-style: italic;
}

nav#TOC {
background: #f5f8fc;
border: 1px solid #ccc;
padding: 1em 1.5em;
margin: 1em 0 2em 0;
border-radius: 4px;
}
nav#TOC > ul { padding-left: 0; }
nav#TOC li { margin: 0.2em 0; }
nav#TOC a { text-decoration: none; }

p > em:first-child {

}

@media print {
body { max-width: 100%; font-size: 10pt; }
pre { font-size: 8pt; }
h1, h2, h3 { page-break-after: avoid; }
table { page-break-inside: avoid; }
@page { margin: 1in; }
}

.title-block {
text-align: center;
margin-bottom: 2em;
}
</style>
</head>
<body>
<div style="text-align: center; margin-bottom: 2em; border-bottom: 2px solid #4a6fa5; padding-bottom: 1em;">
<h1 style="color: #4a6fa5; margin-bottom: 0.2em;">Canonical Parallel Reduction</h1>
<h2 style="color: #4a6fa5; margin-top: 0;">A Fixed Expression Structure for Run-To-Run Consistency</h2>
<table style="margin: 1em auto; text-align: left; font-size: 0.95em;">
<tr><td><strong>Document:</strong></td><td>DxxxxR0</td></tr>
<tr><td><strong>Date:</strong></td><td>2026-02-14</td></tr>
<tr><td><strong>Reply-To:</strong></td><td>Andrew Drakeford &lt;andreedrakeford@hotmail.com&gt;</td></tr>
<tr><td><strong>Audience:</strong></td><td>LEWG, SG6 (Numerics), WG14</td></tr>
</table>
</div>
<header id="title-block-header">
<h1 class="title">Canonical Parallel Reduction</h1>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#abstract" id="toc-abstract">Abstract</a></li>
<li><a href="#the-semantic-gap-in-c-reduction-facilities" id="toc-the-semantic-gap-in-c-reduction-facilities">1. The Semantic Gap
in C++ Reduction Facilities</a>
<ul>
<li><a href="#why-this-paper-now" id="toc-why-this-paper-now">1.1 Why
This Paper Now</a></li>
<li><a href="#a-tiny-motivating-example-informative" id="toc-a-tiny-motivating-example-informative">1.2 A tiny motivating
example (informative)</a></li>
<li><a href="#determinism-via-expression-ownership-in-existing-practice" id="toc-determinism-via-expression-ownership-in-existing-practice">1.3
Determinism via Expression Ownership in Existing Practice</a></li>
</ul></li>
<li><a href="#scope-and-non-goals" id="toc-scope-and-non-goals">2. Scope
and Non-Goals</a>
<ul>
<li><a href="#opt-in-reproducibility-and-performance-trade-off-informative" id="toc-opt-in-reproducibility-and-performance-trade-off-informative">2.1
Opt-in reproducibility and performance trade-off (informative)</a></li>
<li><a href="#traversal-and-sizing-requirements-informative" id="toc-traversal-and-sizing-requirements-informative">2.2 Traversal and
sizing requirements (informative)</a></li>
<li><a href="#exception-safety" id="toc-exception-safety">2.3 Exception
safety</a></li>
<li><a href="#complexity" id="toc-complexity">2.4 Complexity</a></li>
<li><a href="#expression-identity-vs.-bitwise-identity" id="toc-expression-identity-vs.-bitwise-identity">2.5 Expression
identity vs. bitwise identity</a></li>
</ul></li>
<li><a href="#design-space-informative" id="toc-design-space-informative">3. Design Space (Informative)</a>
<ul>
<li><a href="#expression-algorithm-and-execution" id="toc-expression-algorithm-and-execution">3.0 Expression, Algorithm,
and Execution</a></li>
<li><a href="#why-not-left-fold" id="toc-why-not-left-fold">3.1 Why Not
Left-Fold?</a></li>
<li><a href="#why-not-blocked-decomposition" id="toc-why-not-blocked-decomposition">3.2 Why Not Blocked
Decomposition?</a></li>
<li><a href="#why-not-a-fixed-standard-constant" id="toc-why-not-a-fixed-standard-constant">3.3 Why Not a Fixed Standard
Constant?</a></li>
<li><a href="#why-not-implementation-defined" id="toc-why-not-implementation-defined">3.4 Why Not
Implementation-Defined?</a></li>
<li><a href="#the-proposed-design" id="toc-the-proposed-design">3.5 The
proposed design</a></li>
<li><a href="#industry-precedents-for-constrained-computation-structure" id="toc-industry-precedents-for-constrained-computation-structure">3.6
Industry precedents for constrained computation structure</a></li>
<li><a href="#the-structural-necessity-of-a-new-algorithm" id="toc-the-structural-necessity-of-a-new-algorithm">3.7 The structural
necessity of a new algorithm</a></li>
<li><a href="#rationale-for-tree-shape-choice" id="toc-rationale-for-tree-shape-choice">3.8 Rationale for tree shape
choice</a></li>
<li><a href="#deferral-of-api-surface" id="toc-deferral-of-api-surface">3.9 Deferral of API surface</a></li>
<li><a href="#relationship-to-executors" id="toc-relationship-to-executors">3.10 Relationship to
Executors</a></li>
</ul></li>
<li><a href="#fixed-expression-structure-canonical-compute-sequence-canonical.reduce" id="toc-fixed-expression-structure-canonical-compute-sequence-canonical.reduce">4.
Fixed Expression Structure (Canonical Compute Sequence)
[canonical.reduce]</a>
<ul>
<li><a href="#canonical-tree-shape-canonical.reduce.tree" id="toc-canonical-tree-shape-canonical.reduce.tree">4.1 Canonical tree
shape [canonical.reduce.tree]</a></li>
<li><a href="#canonical-tree-reduction-with-absent-operands-canonical.reduce.tree.absent" id="toc-canonical-tree-reduction-with-absent-operands-canonical.reduce.tree.absent">4.2
Canonical tree reduction with absent operands
[canonical.reduce.tree.absent]</a></li>
<li><a href="#interleaved-topology-canonical.reduce.interleaved" id="toc-interleaved-topology-canonical.reduce.interleaved">4.3
Interleaved topology [canonical.reduce.interleaved]</a></li>
<li><a href="#two-stage-reduction-semantics-canonical.reduce.twostage" id="toc-two-stage-reduction-semantics-canonical.reduce.twostage">4.4
Two-stage reduction semantics [canonical.reduce.twostage]</a></li>
<li><a href="#integration-of-an-initial-value-if-provided-canonical.reduce.init" id="toc-integration-of-an-initial-value-if-provided-canonical.reduce.init">4.5
Integration of an initial value (if provided)
[canonical.reduce.init]</a></li>
<li><a href="#materialization-of-conceptual-terms-and-reduction-state-introducing-v-and-a-canonical.reduce.types" id="toc-materialization-of-conceptual-terms-and-reduction-state-introducing-v-and-a-canonical.reduce.types">4.6
Materialization of conceptual terms and reduction state (introducing V
and A) [canonical.reduce.types]</a></li>
</ul></li>
<li><a href="#invariance-properties-canonical.reduce.invariance" id="toc-invariance-properties-canonical.reduce.invariance">5. Invariance
Properties [canonical.reduce.invariance]</a></li>
<li><a href="#floating-point-considerations-informative" id="toc-floating-point-considerations-informative">6. Floating-Point
Considerations (Informative)</a></li>
<li><a href="#relationship-to-existing-facilities-informative" id="toc-relationship-to-existing-facilities-informative">7. Relationship
to Existing Facilities (Informative)</a></li>
<li><a href="#motivation-and-use-cases-informative" id="toc-motivation-and-use-cases-informative">8. Motivation and Use
Cases (Informative)</a></li>
<li><a href="#api-design-space-informative" id="toc-api-design-space-informative">9. API Design Space
(Informative)</a>
<ul>
<li><a href="#new-algorithm-approach-illustrative" id="toc-new-algorithm-approach-illustrative">9.1 New Algorithm Approach
(Illustrative)</a></li>
<li><a href="#span-convenience-deriving-l-from-a-byte-span-m-informative" id="toc-span-convenience-deriving-l-from-a-byte-span-m-informative">9.2
Span Convenience: Deriving L from a Byte Span M (Informative)</a></li>
<li><a href="#execution-policy-approach-illustrative" id="toc-execution-policy-approach-illustrative">9.3 Execution Policy
Approach (Illustrative)</a></li>
<li><a href="#trade-offs-informative" id="toc-trade-offs-informative">9.4 Trade-offs (Informative)</a></li>
<li><a href="#naming-considerations-informative" id="toc-naming-considerations-informative">9.5 Naming Considerations
(Informative)</a></li>
<li><a href="#topology-defaults-and-named-presets-informative" id="toc-topology-defaults-and-named-presets-informative">9.6 Topology
Defaults and Named Presets (Informative)</a></li>
</ul></li>
<li><a href="#what-lewg-is-being-asked-to-agree-to-this-paper" id="toc-what-lewg-is-being-asked-to-agree-to-this-paper">What LEWG is
being asked to agree to (this paper)</a></li>
<li><a href="#polls-for-lewg-direction-this-paper" id="toc-polls-for-lewg-direction-this-paper">Polls for LEWG Direction
(this paper)</a>
<ul>
<li><a href="#poll-1-semantics-first-scope" id="toc-poll-1-semantics-first-scope">Poll 1 — Semantics-first
scope</a></li>
<li><a href="#poll-2-core-semantic-contract" id="toc-poll-2-core-semantic-contract">Poll 2 — Core semantic
contract</a></li>
<li><a href="#poll-2a-canonical-tree-shape" id="toc-poll-2a-canonical-tree-shape">Poll 2A — Canonical tree
shape</a></li>
<li><a href="#poll-3-proceed-to-api-surface-clarified" id="toc-poll-3-proceed-to-api-surface-clarified">Poll 3 — Proceed to API
surface (clarified)</a></li>
<li><a href="#poll-4a-straw-poll-api-approach-new-algorithm" id="toc-poll-4a-straw-poll-api-approach-new-algorithm">Poll 4A — Straw
poll: API approach (new algorithm)</a></li>
<li><a href="#poll-4b-straw-poll-api-approach-execution-policy" id="toc-poll-4b-straw-poll-api-approach-execution-policy">Poll 4B —
Straw poll: API approach (execution policy)</a></li>
</ul></li>
<li><a href="#acknowledgements" id="toc-acknowledgements">Acknowledgements</a></li>
<li><a href="#revision-history" id="toc-revision-history">Revision
History</a></li>
<li><a href="#references" id="toc-references">References</a>
<ul>
<li><a href="#c-standard-references" id="toc-c-standard-references">C++
Standard References</a></li>
<li><a href="#wg21-papers" id="toc-wg21-papers">WG21 Papers</a></li>
<li><a href="#industry-references" id="toc-industry-references">Industry
References</a></li>
<li><a href="#academic-references" id="toc-academic-references">Academic
References</a></li>
</ul></li>
<li><a href="#appendix-a-illustrative-wording-informative" id="toc-appendix-a-illustrative-wording-informative">Appendix A:
Illustrative Wording (Informative)</a>
<ul>
<li><a href="#a.1-example-algorithm-specification" id="toc-a.1-example-algorithm-specification">A.1 Example Algorithm
Specification</a></li>
<li><a href="#a.2-semantics-as-if" id="toc-a.2-semantics-as-if">A.2
Semantics (as-if)</a></li>
</ul></li>
<li><a href="#appendix-b-implementation-guidance-informative" id="toc-appendix-b-implementation-guidance-informative">Appendix B:
Implementation Guidance (Informative)</a>
<ul>
<li><a href="#b.1-two-degrees-of-parallelism" id="toc-b.1-two-degrees-of-parallelism">B.1 Two Degrees of
Parallelism</a></li>
<li><a href="#b.2-the-efficient-simd-pattern" id="toc-b.2-the-efficient-simd-pattern">B.2 The Efficient SIMD
Pattern</a></li>
<li><a href="#b.3-thread-level-partitioning" id="toc-b.3-thread-level-partitioning">B.3 Thread-Level
Partitioning</a></li>
<li><a href="#b.4-gpu-implementation" id="toc-b.4-gpu-implementation">B.4 GPU Implementation</a></li>
<li><a href="#b.5-cross-platform-consistency" id="toc-b.5-cross-platform-consistency">B.5 Cross-Platform
Consistency</a></li>
<li><a href="#b.6-when-physical-width-logical-width" id="toc-b.6-when-physical-width-logical-width">B.6 When Physical Width ≠
Logical Width</a></li>
<li><a href="#b.7-performance-characteristics" id="toc-b.7-performance-characteristics">B.7 Performance
Characteristics</a></li>
</ul></li>
<li><a href="#appendix-c-prior-art-and-industry-practice-informative" id="toc-appendix-c-prior-art-and-industry-practice-informative">Appendix
C: Prior Art and Industry Practice (Informative)</a>
<ul>
<li><a href="#c.1-kokkos-ecosystem-hpc-portability" id="toc-c.1-kokkos-ecosystem-hpc-portability">C.1 Kokkos Ecosystem (HPC
Portability)</a></li>
<li><a href="#c.2-intel-oneapi-threading-building-blocks-onetbb" id="toc-c.2-intel-oneapi-threading-building-blocks-onetbb">C.2 Intel
oneAPI Threading Building Blocks (oneTBB)</a></li>
<li><a href="#c.3-nvidia-thrust-and-cub" id="toc-c.3-nvidia-thrust-and-cub">C.3 NVIDIA Thrust and CUB</a></li>
<li><a href="#c.4-academic-and-research-solutions" id="toc-c.4-academic-and-research-solutions">C.4 Academic and Research
Solutions</a></li>
</ul></li>
<li><a href="#appendix-d-standard-wording-for-sequential-evaluation-order-informative" id="toc-appendix-d-standard-wording-for-sequential-evaluation-order-informative">Appendix
D: Standard Wording for Sequential Evaluation Order (Informative)</a>
<ul>
<li><a href="#d.1-stdaccumulate" id="toc-d.1-stdaccumulate">D.1
std::accumulate</a></li>
<li><a href="#d.2-stdrangesfold_left-c23" id="toc-d.2-stdrangesfold_left-c23">D.2 std::ranges::fold_left
(C++23)</a></li>
<li><a href="#d.3-sequential-vs.-parallel-contrasting-guarantees" id="toc-d.3-sequential-vs.-parallel-contrasting-guarantees">D.3
Sequential vs. Parallel: Contrasting Guarantees</a></li>
<li><a href="#d.4-init-placement-comparison-with-stdaccumulate" id="toc-d.4-init-placement-comparison-with-stdaccumulate">D.4 Init
Placement: Comparison with std::accumulate</a></li>
<li><a href="#d.5-important-caveat-evaluation-order-bitwise-identity" id="toc-d.5-important-caveat-evaluation-order-bitwise-identity">D.5
Important Caveat: Evaluation Order ≠ Bitwise Identity</a></li>
<li><a href="#d.6-why-compilers-cannot-reassociate-mandated-evaluation-order" id="toc-d.6-why-compilers-cannot-reassociate-mandated-evaluation-order">D.6
Why Compilers Cannot Reassociate Mandated Evaluation Order</a></li>
</ul></li>
<li><a href="#appendix-e-init-placement-rationale-informative" id="toc-appendix-e-init-placement-rationale-informative">Appendix E:
Init Placement Rationale (Informative)</a>
<ul>
<li><a href="#e.1-design-options-considered" id="toc-e.1-design-options-considered">E.1 Design Options
Considered</a></li>
<li><a href="#e.2-analysis" id="toc-e.2-analysis">E.2 Analysis</a></li>
<li><a href="#e.3-why-option-a-was-chosen" id="toc-e.3-why-option-a-was-chosen">E.3 Why Option A Was
Chosen</a></li>
<li><a href="#e.4-why-not-treat-init-as-element-0" id="toc-e.4-why-not-treat-init-as-element-0">E.4 Why Not Treat init as
Element 0?</a></li>
<li><a href="#e.5-migration-path-for-users-expecting-accumulate-style-semantics" id="toc-e.5-migration-path-for-users-expecting-accumulate-style-semantics">E.5
Migration Path for Users Expecting accumulate-style Semantics</a></li>
</ul></li>
<li><a href="#appendix-f-design-evolution-informative" id="toc-appendix-f-design-evolution-informative">Appendix F: Design
Evolution (Informative)</a>
<ul>
<li><a href="#f.1-core-design-decisions" id="toc-f.1-core-design-decisions">F.1 Core Design Decisions</a></li>
<li><a href="#f.2-key-design-iterations" id="toc-f.2-key-design-iterations">F.2 Key Design Iterations</a></li>
<li><a href="#f.3-industry-context" id="toc-f.3-industry-context">F.3
Industry Context</a></li>
</ul></li>
<li><a href="#appendix-g-detailed-design-rationale-informative" id="toc-appendix-g-detailed-design-rationale-informative">Appendix G:
Detailed Design Rationale (Informative)</a>
<ul>
<li><a href="#g.1-why-the-convenience-spelling-uses-bytes" id="toc-g.1-why-the-convenience-spelling-uses-bytes">G.1 Why the
Convenience Spelling Uses Bytes</a></li>
<li><a href="#g.2-why-interleaved-topology-supports-efficient-simd" id="toc-g.2-why-interleaved-topology-supports-efficient-simd">G.2 Why
Interleaved Topology Supports Efficient SIMD</a></li>
<li><a href="#g.3-information-density-and-spatial-locality" id="toc-g.3-information-density-and-spatial-locality">G.3 Information
Density and Spatial Locality</a></li>
<li><a href="#g.4-cross-architecture-expression-parity" id="toc-g.4-cross-architecture-expression-parity">G.4 Cross-Architecture
Expression-Parity</a></li>
<li><a href="#g.5-the-golden-reference-l-1" id="toc-g.5-the-golden-reference-l-1">G.5 The Golden Reference (L =
1)</a></li>
<li><a href="#g.6-divergence-from-stdaccumulate" id="toc-g.6-divergence-from-stdaccumulate">G.6 Divergence from
std::accumulate</a></li>
<li><a href="#g.7-init-placement-determinism" id="toc-g.7-init-placement-determinism">G.7 Init Placement
Determinism</a></li>
</ul></li>
<li><a href="#appendix-h-performance-feasibility-informative" id="toc-appendix-h-performance-feasibility-informative">Appendix H:
Performance Feasibility (Informative)</a>
<ul>
<li><a href="#h.1-prototype-test-conditions" id="toc-h.1-prototype-test-conditions">H.1 Prototype test
conditions</a></li>
<li><a href="#h.2-representative-observations" id="toc-h.2-representative-observations">H.2 Representative
observations</a></li>
<li><a href="#h.3-interpretation" id="toc-h.3-interpretation">H.3
Interpretation</a></li>
</ul></li>
<li><a href="#appendix-i-rationale-for-semantic-span-presets-informative" id="toc-appendix-i-rationale-for-semantic-span-presets-informative">Appendix
I: Rationale for Semantic Span Presets (Informative)</a>
<ul>
<li><a href="#i.1-rationale-for-128-bytes-small-span" id="toc-i.1-rationale-for-128-bytes-small-span">I.1 Rationale for 128
Bytes (Small Span)</a></li>
<li><a href="#i.2-rationale-for-1024-bytes-large-span" id="toc-i.2-rationale-for-1024-bytes-large-span">I.2 Rationale for 1024
Bytes (Large Span)</a></li>
<li><a href="#i.3-cross-domain-verification" id="toc-i.3-cross-domain-verification">I.3 Cross-Domain
Verification</a></li>
</ul></li>
<li><a href="#appendix-j-indicative-api-straw-man-informative" id="toc-appendix-j-indicative-api-straw-man-informative">Appendix J:
Indicative API Straw Man (Informative)</a>
<ul>
<li><a href="#j.1-design-goal" id="toc-j.1-design-goal">J.1 Design
goal</a></li>
<li><a href="#j.2-favored-approach-standard-fixed-preset-constants-option-3" id="toc-j.2-favored-approach-standard-fixed-preset-constants-option-3">J.2
Favored approach: standard-fixed preset constants (Option 3)</a></li>
<li><a href="#j.3-straw-man-algorithm-api-span-coordinate" id="toc-j.3-straw-man-algorithm-api-span-coordinate">J.3 Straw-man
algorithm API (span coordinate)</a></li>
<li><a href="#j.4-straw-man-algorithm-api-lane-coordinate" id="toc-j.4-straw-man-algorithm-api-lane-coordinate">J.4 Straw-man
algorithm API (lane coordinate)</a></li>
<li><a href="#j.5-notes-on-naming-and-evolution" id="toc-j.5-notes-on-naming-and-evolution">J.5 Notes on naming and
evolution</a></li>
<li><a href="#j.6-range-overloads-straw-man" id="toc-j.6-range-overloads-straw-man">J.6 Range overloads
(straw-man)</a></li>
</ul></li>
<li><a href="#appendix-k-demonstrator-godbolts-informative" id="toc-appendix-k-demonstrator-godbolts-informative">Appendix K:
Demonstrator Godbolts (Informative)</a>
<ul>
<li><a href="#k.1-demonstrator-set-gross-platform-runs" id="toc-k.1-demonstrator-set-gross-platform-runs">K.1 Demonstrator set
(gross platform runs)</a></li>
<li><a href="#k.2-what-the-demonstrators-are-intended-to-prove" id="toc-k.2-what-the-demonstrators-are-intended-to-prove">K.2 What the
demonstrators are intended to prove</a></li>
<li><a href="#k.3-expected-verification-outputs-for-the-published-demonstrators" id="toc-k.3-expected-verification-outputs-for-the-published-demonstrators">K.3
Expected verification outputs (for the published demonstrators)</a></li>
<li><a href="#k.3.1-cancellation-stress-dataset-recommended" id="toc-k.3.1-cancellation-stress-dataset-recommended">K.3.1
Cancellation stress dataset (recommended)</a></li>
<li><a href="#k.4-recommended-compiler-explorer-settings" id="toc-k.4-recommended-compiler-explorer-settings">K.4 Recommended
Compiler Explorer settings</a></li>
<li><a href="#k.5-interpreting-the-performance-tables-gross-impacts" id="toc-k.5-interpreting-the-performance-tables-gross-impacts">K.5
Interpreting the performance tables (gross impacts)</a></li>
<li><a href="#k.6-relationship-to-repository-evidence" id="toc-k.6-relationship-to-repository-evidence">K.6 Relationship to
repository evidence</a></li>
</ul></li>
<li><a href="#appendix-l-ranges-compatibility-informative" id="toc-appendix-l-ranges-compatibility-informative">Appendix L: Ranges
Compatibility (Informative)</a>
<ul>
<li><a href="#l.1-a-range-surface-does-not-change-the-semantic-contract" id="toc-l.1-a-range-surface-does-not-change-the-semantic-contract">L.1 A
range surface does not change the semantic contract</a></li>
<li><a href="#l.2-determining-n-without-hidden-allocation" id="toc-l.2-determining-n-without-hidden-allocation">L.2 Determining N
without hidden allocation</a></li>
<li><a href="#l.3-working-with-non-sized-single-pass-sources" id="toc-l.3-working-with-non-sized-single-pass-sources">L.3 Working with
non-sized, single-pass sources</a></li>
<li><a href="#l.4-projection-parameter" id="toc-l.4-projection-parameter">L.4 Projection parameter</a></li>
</ul></li>
<li><a href="#appendix-m-detailed-motivation-and-use-cases-informative" id="toc-appendix-m-detailed-motivation-and-use-cases-informative">Appendix
M: Detailed Motivation and Use Cases (Informative)</a>
<ul>
<li><a href="#m.1-ci-regression-testing" id="toc-m.1-ci-regression-testing">M.1 CI Regression Testing</a></li>
<li><a href="#m.2-distributed-training-checkpoints" id="toc-m.2-distributed-training-checkpoints">M.2 Distributed Training
Checkpoints</a></li>
<li><a href="#m.3-regulatory-audit-trails" id="toc-m.3-regulatory-audit-trails">M.3 Regulatory Audit
Trails</a></li>
<li><a href="#m.4-scientific-reproducibility" id="toc-m.4-scientific-reproducibility">M.4 Scientific
Reproducibility</a></li>
<li><a href="#m.5-exascale-hpc-with-kokkos" id="toc-m.5-exascale-hpc-with-kokkos">M.5 Exascale HPC with
Kokkos</a></li>
<li><a href="#m.6-cross-platform-development" id="toc-m.6-cross-platform-development">M.6 Cross-Platform
Development</a></li>
<li><a href="#m.7-reference-and-debugging-mode" id="toc-m.7-reference-and-debugging-mode">M.7 Reference and Debugging
Mode</a></li>
</ul></li>
<li><a href="#appendix-n-multi-threaded-implementation-via-ordered-state-merge-informative" id="toc-appendix-n-multi-threaded-implementation-via-ordered-state-merge-informative">Appendix
N: Multi-Threaded Implementation via Ordered State Merge
(Informative)</a>
<ul>
<li><a href="#n.1-overview" id="toc-n.1-overview">N.1 Overview</a></li>
<li><a href="#n.2-stack-state-representation" id="toc-n.2-stack-state-representation">N.2 Stack State
Representation</a></li>
<li><a href="#n.3-the-push-operation" id="toc-n.3-the-push-operation">N.3 The Push Operation</a></li>
<li><a href="#n.4-the-fold-operation" id="toc-n.4-the-fold-operation">N.4 The Fold Operation</a></li>
<li><a href="#n.5-power-of-2-aligned-partitioning" id="toc-n.5-power-of-2-aligned-partitioning">N.5 Power-of-2 Aligned
Partitioning</a></li>
<li><a href="#n.6-partition-strategy" id="toc-n.6-partition-strategy">N.6 Partition Strategy</a></li>
<li><a href="#n.7-the-merge-operation" id="toc-n.7-the-merge-operation">N.7 The Merge Operation</a></li>
<li><a href="#n.8-correctness-argument" id="toc-n.8-correctness-argument">N.8 Correctness argument</a></li>
<li><a href="#n.9-complete-algorithm" id="toc-n.9-complete-algorithm">N.9 Complete Algorithm</a></li>
<li><a href="#n.10-complexity-analysis" id="toc-n.10-complexity-analysis">N.10 Complexity Analysis</a></li>
<li><a href="#n.11-simd-optimization-8-block-unrolling" id="toc-n.11-simd-optimization-8-block-unrolling">N.11 SIMD
Optimization: 8-Block Unrolling</a></li>
<li><a href="#n.12-performance-observations" id="toc-n.12-performance-observations">N.12 Performance
Observations</a></li>
<li><a href="#n.13-implementation-notes" id="toc-n.13-implementation-notes">N.13 Implementation Notes</a></li>
<li><a href="#n.14-summary" id="toc-n.14-summary">N.14 Summary</a></li>
</ul></li>
<li><a href="#appendix-x-recursive-bisection-balanced-tree-construction-informative" id="toc-appendix-x-recursive-bisection-balanced-tree-construction-informative">Appendix
X: Recursive Bisection (“Balanced”) Tree Construction (Informative)</a>
<ul>
<li><a href="#x.1-definition" id="toc-x.1-definition">X.1
Definition</a></li>
<li><a href="#x.2-equivalence-on-power-of-two-sizes" id="toc-x.2-equivalence-on-power-of-two-sizes">X.2 Equivalence on
power-of-two sizes</a></li>
<li><a href="#x.3-differences-on-non-power-of-two-sizes" id="toc-x.3-differences-on-non-power-of-two-sizes">X.3 Differences on
non-power-of-two sizes</a></li>
<li><a href="#x.4-rationale-for-selecting-iterative-pairwise" id="toc-x.4-rationale-for-selecting-iterative-pairwise">X.4 Rationale
for selecting iterative pairwise</a></li>
</ul></li>
</ul>
</nav>
<h2 id="abstract">Abstract</h2>
<p>C++ today offers two endpoints for reduction:</p>
<ul>
<li><code>std::accumulate</code>: a specified left-fold expression,
inherently sequential.</li>
<li><code>std::reduce</code>: scalable, but permits reassociation; for
non-associative operations (e.g., floating-point addition), the returned
value may vary.</li>
</ul>
<p>This paper specifies a <strong>canonical reduction expression
structure</strong>: for a given input order and topology coordinate
(lane count <code>L</code>), the expression — its parenthesization and
operand order — is unique and fully specified. Implementations are free
to schedule evaluation using parallelization, vectorization, or any
other strategy, provided the returned value matches that of the
specified expression.</p>
<p>The proposal standardizes the expression structure only. API design
is deferred.</p>
<p>This proposal generalizes the Standard Library technique used by
<code>std::accumulate</code>: determinism is obtained by fixing the
abstract expression structure, not by constraining execution strategy or
floating-point arithmetic (see Appendix D). Bitwise identity of results
additionally depends on the floating-point evaluation model (§6).</p>
<p><strong>Reading guide.</strong> The normative content of this paper
is §4–§5 (~8 pages). Everything else is informative rationale and
appendices.</p>
<table>
<colgroup>
<col style="width: 19%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 51%" />
</colgroup>
<thead>
<tr class="header">
<th>Reader</th>
<th>Read</th>
<th>Skim</th>
<th>Reference as needed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>LEWG reviewer</strong></td>
<td>§1, §2, §4, §5, Polls</td>
<td>§3 (design rationale)</td>
<td>Appendices</td>
</tr>
<tr class="even">
<td><strong>Implementer</strong></td>
<td>Add Appendix B, N</td>
<td>§3.8 (tree shape rationale)</td>
<td></td>
</tr>
<tr class="odd">
<td><strong>Numerical analyst</strong></td>
<td>Add §6, Appendix C, K</td>
<td>§3.8 (throughput data)</td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="the-semantic-gap-in-c-reduction-facilities">1. The Semantic Gap
in C++ Reduction Facilities</h2>
<p>C++ specifies two reduction semantics, but there is a gap:</p>
<table style="width:100%;">
<colgroup>
<col style="width: 17%" />
<col style="width: 36%" />
<col style="width: 17%" />
<col style="width: 28%" />
</colgroup>
<thead>
<tr class="header">
<th>Facility</th>
<th>Expression Specified</th>
<th>Scalable</th>
<th>Semantic Model</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>std::accumulate</code></td>
<td>✓ (left-fold)</td>
<td>✗</td>
<td>Sequential, specified grouping</td>
</tr>
<tr class="even">
<td><code>std::reduce</code></td>
<td>✗ (unspecified)</td>
<td>✓</td>
<td>Parallel, unspecified grouping</td>
</tr>
<tr class="odd">
<td><strong>Canonical reduction</strong> (this proposal)</td>
<td>✓ (canonical tree)</td>
<td>✓</td>
<td>Parallel, specified grouping</td>
</tr>
</tbody>
</table>
<p>A summary comparison with HPC frameworks and industry practice
appears in §7.</p>
<p><code>std::accumulate</code> specifies a left-fold expression and
therefore a deterministic result for any given <code>binary_op</code>,
but it is inherently sequential. <code>std::reduce</code> enables
scalable execution by permitting reassociation; as a consequence, the
abstract expression is not specified, and for non-associative operations
the returned value may vary.</p>
<p>This proposal closes that gap by defining a <strong>canonical,
parallel-friendly expression structure</strong>: a deterministic lane
partitioning determined by the topology coordinate <code>L</code>
together with a canonical iterative-pairwise tree over lane results, as
defined in §4.</p>
<p>Because the expression is fixed, the facility also supports
<strong>heterogeneous verification</strong>: the same topology
coordinate can be used to reproduce an accelerator-produced result on a
CPU by evaluating the identical abstract expression, provided the
floating-point evaluation models are aligned.</p>
<p>[<em>Note:</em> In this paper, <strong>canonical</strong> refers
strictly to the abstract expression structure (parenthesization and
operand order). Implementations retain freedom in evaluation schedule;
the facility provides a stable, specified baseline topology. <em>—end
note</em>]</p>
<h3 id="why-this-paper-now">1.1 Why This Paper Now</h3>
<p>Each standard cycle without a specified parallel reduction compounds
fragmentation. Frameworks like Kokkos, TBB, CUB, and oneMKL each provide
their own solutions with different semantics. A standard semantic
foundation enables convergence.</p>
<p><strong>Why standardize, not just use a library?</strong> Today, you
cannot write a generic library component that demands a specified
reduction expression from the standard toolkit. Standardizing the
semantic definition lets vendors and users align on a common
contract.</p>
<h3 id="a-tiny-motivating-example-informative">1.2 A tiny motivating
example (informative)</h3>
<p>Consider the same 6 floating-point inputs evaluated with the same
<code>+</code> operator:</p>
<p><code>[1e16, 1, 1, -1e16, 1, 1]</code></p>
<p>Because floating-point addition is not associative, different
parenthesizations can legitimately produce different results.</p>
<ul>
<li>A <strong>left-to-right fold</strong> (as with
<code>std::accumulate</code>) groups as
<code>(((((1e16 + 1) + 1) + -1e16) + 1) + 1)</code> which may yield a
different result (e.g., <code>4</code> on many IEEE-754 double
implementations) because small terms can survive until after the large
cancellation.</li>
<li>A plausible <strong>tree reduction</strong> (as may happen inside
<code>std::reduce</code>) can group as
<code>((1e16 + 1) + (1 + -1e16)) + (1 + 1)</code>, in which some
<code>+1</code> terms may be lost early, yielding a different result
(e.g., <code>2</code>), depending on evaluation strategy and rounding
behavior.</li>
</ul>
<p>Today, both outcomes are consistent with the Standard because
<code>std::reduce</code> does not fix the abstract expression (it
requires associativity/commutativity to be well-defined). This paper’s
goal is to define a <strong>single standard-fixed expression</strong>
(“canonical”) so that the returned value is reproducible within a fixed
consistency domain (fixed input order, fixed topology coordinate
<code>L</code>, and a fixed floating‑point evaluation model), while
still permitting parallel execution.</p>
<p>Appendix K.3.1 provides a cancellation‑heavy dataset in which
<code>std::reduce</code> exhibits run‑to‑run variability while the
canonical reduction remains stable for fixed <code>L</code>, and in
which varying <code>L</code> intentionally yields different results
(confirming that topology selection is semantic, not a hint).</p>
<h3 id="determinism-via-expression-ownership-in-existing-practice">1.3
Determinism via Expression Ownership in Existing Practice</h3>
<p>The C++ Standard already achieves deterministic results for
reductions by <strong>fixing the abstract expression structure</strong>,
not by restricting floating‑point arithmetic or execution strategy.</p>
<p><code>std::accumulate</code> is the canonical example. Its
specification mandates a left‑to‑right fold, fully determining the
parenthesization and left/right operand order of the reduction
expression. As a consequence, implementations are not permitted to
reassociate operations, even when the supplied <code>binary_op</code> is
non‑associative. This guarantee is structural, not numerical: the
Standard does not promise bitwise identity across platforms or builds,
but it does fully specify the abstract expression being evaluated.</p>
<p>This proposal applies the same semantic technique to parallel
reduction. Rather than permitting reassociation (as
<code>std::reduce</code> explicitly does), it standardizes a single
canonical reduction expression that admits parallel evaluation. The
novelty of this proposal lies in the topology of the expression, not in
the mechanism by which determinism is obtained. Appendix D analyzes the
<code>std::accumulate</code> precedent in detail and explains why the
same conformance model applies to the canonical reduction specified in
§4.</p>
<h2 id="scope-and-non-goals">2. Scope and Non-Goals</h2>
<p>This paper proposes <strong>semantics only</strong>. It seeks LEWG
validation of the fixed expression structure defined in §4 before
committing to API design. This proposal introduces no new requirements
on <code>binary_op</code> beyond invocability and convertibility, and
does not modify existing algorithms.</p>
<p>This is not “just <code>std::reduce</code> + an execution policy”:
<code>std::reduce</code> deliberately leaves the abstract evaluation
expression unspecified (and requires associativity/commutativity for
meaning under parallelization), whereas this paper standardizes a
<strong>single fixed canonical expression</strong> that enables opt-in
reproducible results across implementations while still permitting
parallel execution.</p>
<p>The facility defines the returned value for a chosen topology
coordinate (lane count <code>L</code>). Execution strategy remains
unconstrained (§4).</p>
<p>Reproducibility under this facility is defined relative to a chosen
topology coordinate <code>L</code>. Different topology selections
intentionally define different abstract expressions and may therefore
produce different results for non-associative operations. Determinism is
guaranteed for a fixed input sequence, fixed topology coordinate
<code>L</code>, and fixed floating-point evaluation model.</p>
<p>For clarity: §4–§5 are normative and define the semantic contract of
the facility. All other sections and all appendices are informative and
do not introduce additional requirements. Appendix K records external
demonstrator programs (Compiler Explorer links) used to validate the
semantics on multiple architectures; they are semantic witnesses, not
reference implementations and not part of the proposal.</p>
<p><strong>In scope (this paper):</strong></p>
<ul>
<li>Semantic definition of the canonical compute sequence (fixed
abstract expression)</li>
<li>Parameterization by lane count <code>L</code>, with a derived span
spelling <code>M</code> (§9.2)</li>
<li>Invariance guarantees (§5)</li>
<li>Rationale for design choices</li>
</ul>
<p><strong>Deferred to subsequent revision (pending LEWG
direction):</strong></p>
<ul>
<li>API surface (new algorithm vs execution policy vs both)</li>
<li>Range-based overloads</li>
<li>Scheduler/executor integration (sender/receiver-based execution
models)</li>
</ul>
<h3 id="opt-in-reproducibility-and-performance-trade-off-informative">2.1
Opt-in reproducibility and performance trade-off (informative)</h3>
<p>This facility is an <strong>opt-in</strong> choice for users who
require a stable, specified abstract expression structure (e.g., for
debugging, verification, regression testing, or reproducible numerical
workflows). It is not intended to replace existing high-throughput
facilities. Users who prioritize maximum throughput over expression
identity should continue to use <code>std::reduce</code> (or
domain-specific facilities) where unspecified reassociation is
acceptable.</p>
<h3 id="traversal-and-sizing-requirements-informative">2.2 Traversal and
sizing requirements (informative)</h3>
<p>Evaluation of the canonical expression requires a well-defined
<code>N</code> (the number of elements in the input range). The
iterative pairwise algorithm is single-pass, but the normative
definition (§4) is stated in terms of <code>K = ceil(N/L)</code>, so the
specification as written requires <code>N</code> to be known. No
implicit materialization or allocation is performed by the facility. See
Appendix L for further discussion of ranges compatibility.</p>
<h3 id="exception-safety">2.3 Exception safety</h3>
<p>Exception handling follows the corresponding Standard Library rules
for the selected execution mode:</p>
<ul>
<li>When evaluated without an execution policy, if
<code>binary_op</code> throws, the exception is propagated to the caller
([algorithms.general]).</li>
<li>When evaluated with an execution policy: if execution of
<code>binary_op</code> exits via an uncaught exception and the policy is
one of the standard execution policies, <code>std::terminate</code> is
called ([algorithms.parallel.exceptions]).</li>
</ul>
<p>The expression-equivalence (returned-value) guarantee applies only
when evaluation completes normally. If <code>std::terminate</code> is
called under a policy-based evaluation, the state of any outputs and any
externally observable side effects is unspecified.</p>
<h3 id="complexity">2.4 Complexity</h3>
<p>Evaluation of the canonical reduction for an input range of
<code>N</code> elements performs <code>O(N)</code> applications of
<code>binary_op</code>. Specifically, the canonical expression contains
exactly <code>N − 1</code> applications of <code>binary_op</code> when
<code>N &gt; 0</code>; absent-operand positions (§4.2.2) do not induce
additional applications.</p>
<p>This matches the work complexity required of
<code>std::reduce</code>. Only work complexity is normative in this
paper. The canonical abstract expression has O(log N) height; this paper
does not require implementations to realize that depth without auxiliary
storage. No guarantees are made about evaluation depth, degree of
parallelism, or auxiliary storage.</p>
<p>[<em>Note:</em> The natural shift-reduce evaluation strategy (§4.2.3)
maintains a stack of depth O(log K) per lane, where K = ceil(N/L). For L
lanes this implies O(L · log(N/L)) intermediate values of type A as
working storage. This is modest in practice (e.g., 8 lanes × 30 stack
entries for a billion elements) but is not zero. <em>—end note</em>]</p>
<h3 id="expression-identity-vs.-bitwise-identity">2.5 Expression
identity vs. bitwise identity</h3>
<p>This facility guarantees <strong>expression identity</strong>: for
fixed input order and topology coordinate, the abstract reduction
expression is identical across conforming implementations. Bitwise
identity of results additionally depends on the floating-point
evaluation model; §6 discusses the distinction in detail.</p>
<h2 id="design-space-informative">3. Design Space (Informative)</h2>
<p>This section catalogs key design alternatives for a reproducible
reduction facility and explains why this proposal chooses a
user-selectable, interleaved-lane topology with a standard-defined
canonical expression.</p>
<p>The goal is to close the “grouping gap” for parallel reductions by
providing a facility that:</p>
<ol type="1">
<li>Specifies a single abstract expression (parenthesization and operand
order) for a chosen topology.</li>
<li>Achieves scalable depth (O(log N)) suitable for parallel
execution.</li>
<li>Is topology-stable: the expression is not implicitly determined by
thread count or implementation strategy.</li>
<li>Remains implementable efficiently on modern hardware (SIMD,
multicore, GPU).</li>
</ol>
<p>The following alternatives are evaluated against these
requirements.</p>
<h3 id="expression-algorithm-and-execution">3.0 Expression, Algorithm,
and Execution</h3>
<p>Every reduction computes an <em>abstract expression</em>: a
parenthesized combination of operands with a defined left/right operand
order. This expression exists independently of how it is evaluated in
time. In C++, the abstract expression has historically been implicit —
specified only indirectly through algorithm wording — and has never been
named as a distinct semantic concern.</p>
<p>In practice, three concerns determine the behavior of a
reduction:</p>
<ul>
<li><strong>Expression structure</strong> — the abstract computation
being performed: grouping, parenthesization, and operand order.</li>
<li><strong>Algorithm</strong> — the semantic contract that determines
which aspects of the expression are specified or intentionally left
unspecified.</li>
<li><strong>Execution</strong> — how evaluation of the expression is
scheduled: sequentially, in parallel, vectorized, or otherwise.</li>
</ul>
<p>The C++ Standard Library already relies on this separation, but does
not articulate it explicitly. As a result, expression structure is
routinely conflated with execution strategy, producing persistent
confusion.</p>
<p><strong>Existing facilities through this lens.</strong> Seen through
this model, existing facilities differ primarily in <em>expression
ownership</em>, not in parallelism:</p>
<ul>
<li><code>std::accumulate</code> specifies a left-to-right fold. The
algorithm fully defines the abstract expression, yielding a
deterministic result for a given input order and operation.</li>
<li><code>std::reduce</code> explicitly declines to specify the
expression structure. It defines a <em>generalized sum</em>, permitting
reassociation to enable scalable execution.</li>
<li>Execution policies operate exclusively on the execution dimension.
They constrain scheduling and concurrency, but do not define or refine
the abstract expression being evaluated. In particular, even
<code>execution::seq</code> does not impose a specific grouping or
forbid reassociation; it affects <em>how</em> an algorithm runs, not
<em>what</em> expression it computes.</li>
</ul>
<p>This explains why <code>std::reduce(execution::seq, ...)</code> is
still permitted to produce different results for non-associative
operations: the algorithm has not specified the expression, and the
policy does not add semantic guarantees.</p>
<p><strong>Why execution policies cannot carry expression
semantics.</strong> It is natural to ask whether a canonical reduction
could be expressed as an execution policy rather than a new algorithm.
Under the current standard model, execution policies are deliberately
non-semantic with respect to the returned value:</p>
<ol type="1">
<li>Policies are designed to <em>relax</em> constraints (permitting
concurrency, vectorization), not to <em>add</em> new semantic guarantees
about grouping or parenthesization.</li>
<li>Policies compose freely and orthogonally. Encoding topology in a
policy would require dominance rules to resolve conflicts between
policies that specify different topologies — a semantic hierarchy the
standard does not have.</li>
<li><code>std::reduce</code> already proves the limitation:
<code>std::reduce(execution::seq, ...)</code> still has generalized-sum
semantics. If <code>seq</code> were sufficient to fix the expression,
<code>std::reduce(seq, ...)</code> would collapse into
<code>std::accumulate</code> — but the standard explicitly keeps them
distinct.</li>
<li>A topology parameter is a <em>semantic</em> parameter that
intentionally changes observable results for non-associative operations.
That is fundamentally different from an execution hint.</li>
</ol>
<p>Encoding expression structure in a policy would therefore require a
fundamental change to the execution-policy model. This proposal
intentionally avoids that scope.</p>
<p><strong>Consequence.</strong> Because expression structure is a
semantic concern that execution policies cannot express, and because
views and range adaptors can reorder traversal but cannot constrain
combination (§3.7), expression ownership must reside in the algorithm.
This proposal makes the abstract expression explicit and assigns
ownership of it to the algorithm, restoring a clean separation between
<em>what</em> is computed (expression), <em>what</em> is guaranteed
(algorithm), and <em>how</em> it is executed (execution policy).</p>
<p>This separation also clarifies the relationship to executors
(§3.10).</p>
<h3 id="why-not-left-fold">3.1 Why Not Left-Fold?</h3>
<p>A strict left-to-right fold has a fixed evaluation order:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Left-fold</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>T result <span class="op">=</span> init<span class="op">;</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="kw">auto</span> it <span class="op">=</span> first<span class="op">;</span> it <span class="op">!=</span> last<span class="op">;</span> <span class="op">++</span>it<span class="op">)</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a> result <span class="op">=</span> op<span class="op">(</span>result<span class="op">,</span> <span class="op">*</span>it<span class="op">);</span></span></code></pre></div>
<p>This is <code>std::accumulate</code>. It provides run-to-run
stability for a given input order, but it cannot parallelize effectively
because each operation depends on the prior result. The reduction depth
is O(N) rather than O(log N), making it unsuitable for scalable parallel
execution.</p>
<p>A left-fold therefore solves stability but not scalability, and it
already exists in the standard library.</p>
<h3 id="why-not-blocked-decomposition">3.2 Why Not Blocked
Decomposition?</h3>
<p>A common parallelization strategy is blocked decomposition: assign
contiguous chunks to workers and reduce each chunk:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Blocked (illustrative)</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co">// Thread 0: E[0..N/4)</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">// Thread 1: E[N/4..N/2)</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">// Thread 2: E[N/2..3N/4)</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">// Thread 3: E[3N/4..N)</span></span></code></pre></div>
<p>Blocked decomposition can be efficient, but it does not by itself
provide a topology-stable semantic contract. To obtain a fully specified
abstract expression, the standard would need to specify:</p>
<ul>
<li>the exact chunk boundaries (including handling of non-power-of-two
sizes),</li>
<li>how chunk results are combined (the second-stage reduction tree),
and</li>
<li>how these choices relate to the execution policy and degree of
parallelism.</li>
</ul>
<p>Absent such specification, the resulting expression is naturally
coupled to execution strategy (thread count, scheduler, partitioner),
which varies across implementations and runs. Fully specifying these
details effectively defines a new algorithm and topology — at which
point the question becomes: which topology should be standardized?</p>
<p>This proposal selects a topology that is simple to specify
canonically and maps well to common hardware structures (see §3.5).</p>
<h3 id="why-not-a-fixed-standard-constant">3.3 Why Not a Fixed Standard
Constant?</h3>
<p>Another approach is to standardize a fixed topology constant. This
can appear attractive for simplicity and uniformity, but it ages
poorly:</p>
<ul>
<li>If the fixed constant is too small, future wider hardware may be
underutilized.</li>
<li>If the fixed constant is too large, current narrow hardware may
incur unnecessary work or overhead.</li>
<li>Any fixed value becomes a standard revision pressure point as
hardware evolves.</li>
</ul>
<p>A fixed constant trades long-term efficiency and flexibility for
initial simplicity and becomes an ongoing “default value” debate. This
proposal instead makes topology selection an explicit part of the
semantic contract (§3.5).</p>
<h3 id="why-not-implementation-defined">3.4 Why Not
Implementation-Defined?</h3>
<p>Allowing implementations to choose the reduction topology is
precisely the status quo problem. For parallel reductions such as
<code>std::reduce</code>, the Standard permits reassociation, and
therefore permits different abstract expressions across implementations
and settings.</p>
<p>An implementation-defined topology does not close the grouping gap;
it merely re-labels it. A reproducibility facility must standardize the
expression structure for a chosen topology, rather than leaving that
structure implementation-defined.</p>
<h3 id="the-proposed-design">3.5 The proposed design</h3>
<p>The facility proposed here is defined by a <strong>family of
canonical expressions</strong> parameterized by a user-selected topology
coordinate. For a chosen coordinate, the Standard defines a single
abstract expression, and the returned value is the result of evaluating
that expression (as-if), independent of execution strategy.</p>
<p><strong>Primary topology coordinate.</strong> The canonical
expression is defined in terms of the <strong>lane count
<code>L</code></strong>. For intuition: <code>L = 1</code> degenerates
to a single canonical tree over the input sequence (no lane
interleaving). Larger <code>L</code> introduces SIMD/GPU-friendly lane
parallelism, but still denotes one Standard-defined canonical expression
fully determined by <code>(N, L)</code>. The normative definition of the
two-stage reduction (lane partitioning, per-lane canonical tree,
cross-lane canonical tree) appears in §4.</p>
<p><strong>Why interleaved lanes, not contiguous blocks?</strong> A
contiguous-block partition (elements <code>[0, N/L)</code> to lane 0,
<code>[N/L, 2N/L)</code> to lane 1, etc.) would also be topology-stable.
The interleaved layout is chosen because it maps directly to SIMD
register filling: a single aligned vector load of <code>L</code>
consecutive elements places one element into each lane simultaneously.
Contiguous blocks would require gathering from <code>L</code> distant
memory locations to fill a register. Additionally, interleaving
guarantees that all lanes contain the same number of elements (to within
one), so all lanes execute the same canonical tree shape — enabling SIMD
lockstep execution without per-lane branching. This uniform tree shape
across lanes is what makes the two-stage decomposition (§4.4) efficient
in practice.</p>
<p>A byte span <code>M</code> may be provided as a <em>derived
convenience</em> by mapping <code>L = M / sizeof(value_type)</code>, but
because <code>sizeof(value_type)</code> is implementation-defined (and
may vary across platforms/ABIs), specifying <code>M</code> does not, in
general, select the same canonical expression across implementations.
For layout-stable arithmetic types, the <code>M</code> spelling is a
convenient way to align topology with SIMD register width; it does not
change the semantic definition of the expression (§9.2).</p>
<h3 id="industry-precedents-for-constrained-computation-structure">3.6
Industry precedents for constrained computation structure</h3>
<p>A common counter-argument is that because bitwise identity across
different ISAs (e.g., x86 vs. ARM) cannot be guaranteed by the C++
Standard alone, the library should not attempt to provide run-to-run
stability guarantees. This “all-or-nothing” view overlooks existing
industry practice.</p>
<p>A widely used mechanism for improving reproducibility is to
<strong>constrain computation structure</strong> (topology, kernel
choice, or execution path) to remove sources of run-to-run variability
introduced by parallel execution. This proposal targets one such source:
unspecified reassociation inside standard parallel reductions. Fixing
the abstract expression is therefore a necessary building block for
reproducible parallel reductions; additional conditions (e.g.,
floating-point evaluation model and environment constraints) remain
outside the scope of this paper.</p>
<h4 id="examples-informative">3.6.1 Examples (informative)</h4>
<table>
<colgroup>
<col style="width: 15%" />
<col style="width: 15%" />
<col style="width: 38%" />
<col style="width: 29%" />
</colgroup>
<thead>
<tr class="header">
<th>Library</th>
<th>Feature</th>
<th>Mechanism (documented)</th>
<th>Scope (typical)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Intel oneMKL</td>
<td>Conditional Numerical Reproducibility (CNR)</td>
<td>Constrains execution paths / pins to a logical ISA</td>
<td>Reproducibility across specified CPU configurations</td>
</tr>
<tr class="even">
<td>NVIDIA CUB</td>
<td>Deterministic reduction variants</td>
<td>Uses a fixed reduction order for deterministic mode</td>
<td>Reproducibility on the same architecture</td>
</tr>
<tr class="odd">
<td>PyTorch / TensorFlow</td>
<td>Deterministic algorithms flags</td>
<td>Disables nondeterministic kernels / selects deterministic
kernels</td>
<td>Reproducible training runs (scope varies)</td>
</tr>
</tbody>
</table>
<h4 id="references-informative">3.6.2 References (informative)</h4>
<p><strong>Intel oneMKL CNR:</strong> Intel documents that parallel
algorithms can produce different results based on thread counts and ISA,
and provides CNR modes to constrain execution paths. See: Intel oneMKL
Developer Guide, “Introduction to Conditional Numerical Reproducibility”
[IntelCNR].</p>
<p><strong>NVIDIA CUB:</strong> NVIDIA distinguishes between “fast”
nondeterministic reductions and “deterministic” variants that use a
fixed-order reduction. See: NVIDIA CUB API Documentation
[NvidiaCUB].</p>
<p>These libraries address reproducibility through different mechanisms
and with different scope. This proposal does not claim to replicate
their exact guarantees, but draws on the same insight: fixing the
computation structure is a useful building block for
reproducibility.</p>
<h4 id="conclusion">3.6.3 Conclusion</h4>
<p>By fixing the abstract expression structure, this proposal provides a
<strong>necessary</strong> (but not sufficient) condition for
reproducibility. Sufficient conditions depend on the program’s
floating-point evaluation model and environment. Without this proposal,
even a user with strict control over their floating-point environment
cannot achieve reproducible parallel results in general, because the
standard library itself permits unspecified reassociation in parallel
reductions.</p>
<h3 id="the-structural-necessity-of-a-new-algorithm">3.7 The structural
necessity of a new algorithm</h3>
<p>A central tenet of this proposal is that run-to-run stability of the
returned value is a <strong>property of the evaluation</strong>, not the
data source. To achieve both logarithmic scalability and topological
determinism, the algorithm itself must “own” the reduction tree. This
logic cannot be injected into existing algorithms via a View or a Range
adaptor.</p>
<h4 id="why-stdreduce-views-cannot-provide-this-contract">3.7.1 Why
std::reduce + Views cannot provide this contract</h4>
<ul>
<li>Views can reorder iteration; they cannot constrain combination. A
view may present elements in a deterministic order, but it cannot force
the algorithm to evaluate a particular parenthesization.</li>
<li><code>std::reduce</code> explicitly permits reassociation.
Therefore, even with a deterministic view and a fixed input order,
<code>std::reduce</code> may legally evaluate a different abstract
expression (see [numeric.ops.reduce]).</li>
</ul>
<div class="sourceCode" id="cb3"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">// A deterministic view does not make std::reduce deterministic:</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> view <span class="op">=</span> data <span class="op">|</span> views<span class="op">::</span>transform<span class="op">(</span>f<span class="op">);</span> <span class="co">// preserves iteration order</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> r1 <span class="op">=</span> <span class="bu">std::</span>reduce<span class="op">(</span><span class="bu">std::</span>execution::par<span class="op">,</span> view<span class="op">.</span>begin<span class="op">(),</span> view<span class="op">.</span>end<span class="op">(),</span> init<span class="op">,</span> op<span class="op">);</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">// r1 may still use an implementation-chosen reassociation.</span></span></code></pre></div>
<h4 id="consequence-the-algorithm-must-own-the-expression">3.7.2
Consequence: the algorithm must own the expression</h4>
<p>To provide the guarantee defined in §4, the reduction facility itself
must specify and “own” the abstract expression tree. As established in
§3.0, expression structure is a semantic concern that cannot reside in
execution policies (which constrain scheduling, not grouping), views
(which reorder traversal, not combination), or executors (which
determine <em>how</em> an expression is evaluated, not <em>what</em> it
is). The algorithm is the only standard mechanism that can own
expression semantics. This is the semantic gap addressed by this
proposal.</p>
<h3 id="rationale-for-tree-shape-choice">3.8 Rationale for tree shape
choice</h3>
<p>Given that a new algorithm must own its expression tree (§3.7.2), the
next design question is: which tree construction rule should the
Standard specify? Two well-known candidates exist: iterative pairwise
(shift-reduce) and recursive bisection. This subsection records the
considerations that inform the choice.</p>
<p><strong>Arguments favoring iterative pairwise
(recommended):</strong></p>
<ol type="1">
<li><p><strong>Industry alignment:</strong> SIMD and GPU implementations
commonly use iterative pairwise because it maps directly to hardware
primitives (warp shuffle-down, vector lane pairing). This includes
NVIDIA CUB’s deterministic reduction modes.</p></li>
<li><p><strong>Implementation naturalness:</strong> For implementers
familiar with GPU and SIMD programming, iterative pairwise matches the
mental model of “pair adjacent lanes, carry the odd one” — the same
pattern used in existing high-performance reduction kernels.</p></li>
<li><p><strong>Adoption ease:</strong> Libraries that already implement
deterministic reductions using iterative pairwise would require no
algorithmic changes to conform to this specification. This lowers the
barrier to adoption.</p></li>
<li><p><strong>Direct SIMD mapping:</strong> The iterative pairwise
pattern corresponds directly to shuffle-down operations on GPU warps and
SIMD vector lanes, enabling efficient implementation without index
remapping.</p></li>
</ol>
<p><strong>Arguments favoring recursive bisection:</strong></p>
<ol type="1">
<li><p><strong>Specification clarity:</strong> The recursive definition
is three lines with no special cases. The iterative definition requires
explicit handling of odd-count carry logic.</p></li>
<li><p><strong>Tree symmetry:</strong> For non-power-of-two k, recursive
bisection produces more balanced subtrees. The “heavier” subtree (with
more elements) is always on the right, following the
<code>m = floor(k/2)</code> split.</p></li>
<li><p><strong>Academic precedent:</strong> Recursive bisection
corresponds to the “pairwise summation” algorithm analyzed in numerical
analysis literature ([Higham2002] §4).</p></li>
</ol>
<p><strong>Why the choice matters:</strong></p>
<p>For non-associative operations (e.g., floating-point addition), the
two algorithms produce different numerical results for non-power-of-two
k. Once standardized, the choice cannot be changed without breaking
existing code that depends on specific results.</p>
<p><strong>Why the choice is bounded:</strong></p>
<p>Both algorithms have identical O(log k · ε) error bounds. Both
produce identical trees for power-of-two k. The practical impact is
limited to: - Non-power-of-two per-lane element counts (when
<code>ceil(N/L)</code> is not a power of two) - Non-power-of-two lane
counts (when L itself is not a power of two) - Cross-lane reduction when
N does not evenly divide L</p>
<p>For the common case of power-of-two L and large N, the per-lane trees
are dominated by power-of-two cases where both algorithms agree.</p>
<p><strong>Visual comparison (k = 7):</strong> For power-of-two k, both
algorithms produce identical trees. The difference is visible only for
non-power-of-two k. The following side-by-side comparison for k = 7
illustrates the full extent of the difference:</p>
<pre class="text"><code>Iterative Pairwise (IPR)              Recursive Bisection
─────────────────────────             ─────────────────────────

         op                                    op
       /    \                                /    \
      op      op                           op      op
     /  \    /  \                         /  \    /  \
    op   op  op  e₆                     e₀   op  op   op
   / \ / \ / \                              / \ / \  / \
  e₀ e₁ e₂ e₃ e₄ e₅                      e₁ e₂ e₃ e₄ e₅ e₆

((e₀⊕e₁)⊕(e₂⊕e₃))⊕((e₄⊕e₅)⊕e₆)    (e₀⊕(e₁⊕e₂))⊕((e₃⊕e₄)⊕(e₅⊕e₆))</code></pre>
<p>Both trees have depth 3 (= ⌈log₂ 7⌉). Both perform exactly 6
applications of <code>binary_op</code>. The error bounds are identical.
The only difference is which element carries: IPR carries
<code>e₆</code> (the last element, locally determined), while recursive
bisection splits at <code>floor(7/2) = 3</code>, changing the pairing of
<code>e₀</code> (a globally determined property — the pairing of the
first element depends on the total count).</p>
<p><strong>This paper specifies iterative pairwise</strong> because it
aligns with existing practice in high-performance libraries that already
provide deterministic reduction modes. This minimizes disruption for
implementers and users who have existing workflows built around these
libraries.</p>
<p><strong>Streaming evaluation: the fundamental structural
difference.</strong> The most significant distinction between the two
candidates is not performance or symmetry — it is that iterative
pairwise is an <em>online</em> algorithm and recursive bisection is a
<em>batch</em> algorithm.</p>
<p>Recursive bisection’s first operation is to compute a midpoint:
<code>mid = N/2</code>. The entire split tree is determined top-down
from the global sequence length. Evaluation cannot begin until N is
known, and the split structure depends on a global property of the
input.</p>
<p>Iterative pairwise (shift-reduce) processes elements incrementally:
each element is shifted onto an O(log N) stack, and reductions are
triggered by a branchless bit-test on the running count
(<code>ntz(count)</code>). The stack captures the complete computation
state at any point. N is not needed until termination, when remaining
stack entries are collapsed.</p>
<p>This structural property has concrete consequences for how C++ is
evolving:</p>
<ol type="1">
<li><strong>Forward ranges without <code>size()</code>:</strong> IPR can
begin reducing immediately with a single forward pass. Recursive
bisection requires either a counting pass or random access to compute
split points.</li>
<li><strong>Chunked executor evaluation:</strong> An executor delivering
data in chunks can feed each chunk into the shift-reduce loop. The stack
state is the continuation — evaluation can pause and resume at any chunk
boundary. Recursive bisection requires the global N before reduction can
begin, forcing a synchronization barrier.</li>
<li><strong>P2300 sender/receiver composition:</strong> P2300
(<code>std::execution</code>, adopted for C++26) explicitly separates
<em>what</em> to execute from <em>where</em> to execute it, and senders
describe composable units of asynchronous work. A streaming reduction
using IPR is naturally expressible as a sender that consumes elements as
they flow through a pipeline. Recursive bisection requires knowing the
complete input before constructing the expression — it cannot compose
incrementally.</li>
<li><strong>Heterogeneous and distributed execution:</strong> Data
arriving asynchronously from GPU kernels, network reduction, or
distributed aggregation can be consumed incrementally by IPR. Recursive
bisection must buffer the complete sequence before computation
begins.</li>
</ol>
<p>In a standard moving toward sender/receiver pipelines and composable
asynchronous execution, the tree shape that can operate without global
knowledge of N is the one structurally aligned with the future of C++
execution. This is not a minor implementation convenience — it is an
architectural property that determines whether the canonical expression
can participate in streaming pipelines at all.</p>
<p>[<em>Note:</em> The current normative definition (§4) is stated in
terms of <code>K = ceil(N/L)</code>, so the specification as written
requires a well-defined N (§2.2). However, the streaming property is
inherent to the iterative pairwise algorithm: lane assignment
(<code>i mod L</code>) is known per-element, and the shift-reduce
procedure within each lane builds the tree incrementally without
foreknowledge of the lane’s element count. N is needed only to determine
when to stop and collapse remaining stack entries. Future API revisions
may exploit this property to weaken iterator and sizing requirements.
<em>—end note</em>]</p>
<p><strong>Performance comparison: iterative pairwise vs recursive
bisection.</strong> Because both trees have identical depth for
power-of-two sizes and nearly identical depth otherwise, their
throughput is similar on modern hardware. Recursive bisection can be
implemented with a direct unrolled mapping for small sub-problems (e.g.,
a flat switch for N ≤ 32 eliminates recursive call overhead), and
unaligned SIMD loads on contemporary microarchitectures are essentially
free when data does not cross a cache line boundary — reducing the
alignment advantage that earlier analyses relied on [Dalton2014]. The
iterative formulation retains structural advantages (loop-based with a
branchless bit-test for reduction, natural streaming order), but these
translate to modest rather than dramatic throughput differences against
a well-engineered recursive implementation. This approximate throughput
equivalence between the two candidates means that performance alone
cannot distinguish them — and the decision appropriately falls to the
axes where they do differ: industry practice alignment, executor
compatibility, and the fact that iterative pairwise is what existing
SIMD and GPU reduction libraries already ship. The paper does not claim
iterative pairwise is faster than recursive bisection; it claims that,
at equivalent performance and identical error bounds, the tree that
matches existing practice is the safer standard choice.</p>
<p><strong>Executor compatibility.</strong> Once the expression is
separated from execution (§3.0), the question becomes: which canonical
expression can executors naturally evaluate? Iterative pairwise produces
a local, lane-structured expression that maps directly to executor
chunking and work-graph execution without requiring the full tree to be
materialized. Recursive bisection produces a globally-recursive
structure that is harder to realize incrementally. In executor terms,
iterative pairwise defines an expression that executors can evaluate
without first building the tree.</p>
<p><strong>Standard regret.</strong> Once standardized, the tree shape
becomes part of the language contract and cannot be changed without
breaking code that depends on specific results. The worst consequence of
choosing iterative pairwise is not incorrectness or inefficiency — it is
commitment. For power-of-two sizes the two candidates are identical; the
commitment applies only to non-power-of-two boundary cases, where
iterative pairwise matches existing SIMD/GPU practice. This is the
expression shape least likely to be regretted as execution hardware
evolves.</p>
<p><strong>Upper bound on regret.</strong> Regardless of tree shape, any
balanced binary reduction has the same O(log N · ε) error bound
[Higham2002]. No alternative tree can improve the asymptotic accuracy.
On the throughput side, iterative pairwise already achieves 89% of the
theoretical peak [Dalton2014]. Even if a superior tree shape were
discovered in the future, the maximum possible throughput improvement
over IPR is therefore bounded at approximately 11%. The regret is
capped: the committee is not choosing between a good answer and an
unknown potentially-much-better answer — it is choosing between 89% and
at most 100%, with identical error bounds. That is a narrow window in
which to find regret.</p>
<p><strong>Measured throughput cost.</strong> The practical performance
cost of iterative pairwise (shift-reduce) summation has been measured by
Dalton, Wang &amp; Blainey [Dalton2014]. Their SIMD-optimized
implementation achieves 89% of the throughput of unconstrained naïve
summation when data is not resident in L1 cache (86% from L2, 90%
streaming from memory), while providing the O(log N · ε) error bound of
pairwise summation — and twice the throughput of the best-tuned
compensated sums (Kahan-Babuška).</p>
<p>The more telling comparison is against the current deterministic
baseline. Today, the only standard facility with a fully specified
expression is <code>std::accumulate</code>, which is a strict left fold
with a loop-carried dependency chain. It cannot utilize SIMD
parallelism: on AVX-512 hardware, a left fold occupies 1 of 8
<code>double</code> lanes (~12% SIMD utilization) or 1 of 16
<code>float</code> lanes (~6%). The canonical iterative pairwise tree,
by contrast, is inherently SIMD-friendly — all lanes active, with
measured throughput at ~89% of peak. This represents approximately a
<strong>7× improvement</strong> in the deterministic reduction path for
<code>double</code> on AVX-512 (and ~14× for <code>float</code>). The
question for LEWG is therefore not “what is the cost of owning the
expression?” but “how much performance does a specified expression
<em>recover</em> compared to the only specified expression we have
today?” The answer is: nearly all of it.</p>
<h3 id="deferral-of-api-surface">3.9 Deferral of API surface</h3>
<p>This paper acknowledges the importance of a Ranges-first model in
modern C++, but the specific API surface (new algorithm, new execution
policy, or range-based overload) is deliberately deferred to a
subsequent revision. Expression structure is orthogonal to API surface;
fixing it first allows API alternatives to be evaluated against a stable
semantic baseline. Once LEWG reaches consensus on this semantic
foundation, a follow-on revision can propose an API that aligns with
modern library patterns.</p>
<h3 id="relationship-to-executors">3.10 Relationship to Executors</h3>
<p>The expression/algorithm/execution separation described in §3.0
aligns naturally with the sender/receiver execution model adopted for
C++26 (P2300).</p>
<p>P2300 (<code>std::execution</code>) addresses the execution concern:
<em>where</em> work runs, <em>when</em> it runs, <em>how</em> it is
scheduled, and what progress guarantees apply. Its stated design
principle — “address the concern of <em>what</em> to execute separately
from the concern of <em>where</em>” — is precisely the separation this
proposal formalizes for reduction expressions. Senders and receivers are
intentionally agnostic to the abstract computation being performed. In
the absence of a specified expression, different schedulers may
legitimately induce different groupings for a reduction, leading to
scheduler-dependent results. This is not a defect in the execution
model; it reflects the fact that the algorithm has not fixed the
computation.</p>
<p>By fixing the abstract expression in the algorithm, this proposal
provides executors with a stable semantic target:</p>
<ul>
<li>The <strong>algorithm</strong> defines <em>what</em> computation
must be performed.</li>
<li>The <strong>executor</strong> determines <em>how</em> that
computation is evaluated.</li>
</ul>
<p>Different schedulers — CPU thread pool, GPU stream, or distributed
sender chain — may execute the canonical expression using different
physical strategies, but they are required to produce the same returned
value for a fixed expression and floating-point evaluation model.</p>
<p>In this sense, the proposal is orthogonal and complementary to P2300.
It does not constrain execution; it enables cross-scheduler consistency
by giving the execution model a deterministic expression to execute. The
streaming property of iterative pairwise (§3.8) is particularly relevant
here: the canonical expression can be evaluated incrementally as data
flows through a sender pipeline, without requiring a synchronization
barrier to determine N before reduction begins. This also explains why
expression semantics could never have resided in execution policies or
schedulers: those mechanisms are designed for the execution dimension,
and encoding expression structure in them would require conflating the
two concerns that this proposal separates.</p>
<h2 id="fixed-expression-structure-canonical-compute-sequence-canonical.reduce">4.
Fixed Expression Structure (Canonical Compute Sequence)
[canonical.reduce]</h2>
<p><strong>Normative.</strong> Sections §4–§5 specify the semantic
contract of this proposal. All other sections are informative.</p>
<p>The contract in this section specifies the <strong>returned value
only</strong>; the evaluation schedule (including the relative ordering
of independent subexpressions) is not specified. For
<code>N &gt; 0</code>, the standard‑fixed abstract expression contains
exactly <code>N − 1</code> applications of <code>binary_op</code>; this
paper does not specify which subexpression is evaluated first or how
evaluation is scheduled.</p>
<p>This section is the core of the proposal. It defines the fixed
abstract expression — the exact grouping and left/right operand order —
that determines the returned value for a given input order,
<code>binary_op</code>, and topology coordinate <code>L</code>. A
conforming implementation shall produce a result as-if it evaluates this
standard-fixed abstract expression; implementations remain free to
schedule evaluation using threads, vectorization, work-stealing, GPU
kernels, etc., provided the returned value matches.</p>
<p>[<em>Note:</em> The semantic contract in §4 is defined in terms of
the returned value. It does not specify scheduling, does not guarantee a
deterministic number of invocations of <code>binary_op</code>, and does
not guarantee deterministic side effects. <em>—end note</em>]</p>
<p><strong>Overview: Two-stage canonical reduction</strong></p>
<p>The canonical reduction proceeds in two stages over an interleaved
lane partition. The input sequence of <code>N</code> elements is
distributed across <code>L</code> lanes by index modulo (element
<code>i</code> → lane <code>i mod L</code>). In Stage 1, each lane
independently reduces its elements using a single canonical tree shape.
In Stage 2, the <code>L</code> lane results are themselves reduced using
the same canonical tree rule. Both stages use
<code>CANONICAL_TREE_EVAL</code> (§4.2.3), ensuring a fully determined
expression from input to result.</p>
<pre class="text"><code>Example: N = 12, L = 4 (3 elements per lane)

Input:  e₀  e₁  e₂  e₃  e₄  e₅  e₆  e₇  e₈  e₉  e₁₀ e₁₁
Lane:    0   1   2   3   0   1   2   3   0   1   2   3

                    ┌─── Stage 1: Per-lane canonical trees ───┐

        Lane 0          Lane 1          Lane 2          Lane 3
          op              op              op              op
         /  \            /  \            /  \            /  \
        op  e₈          op  e₉          op  e₁₀         op  e₁₁
       / \             / \             / \             / \
      e₀  e₄         e₁  e₅         e₂  e₆         e₃  e₇

        ↓ R₀            ↓ R₁            ↓ R₂            ↓ R₃

                    ┌─── Stage 2: Cross-lane canonical tree ──┐

                              op
                            /    \
                          op      op
                         /  \    /  \
                        R₀  R₁  R₂  R₃

                              ↓
                           result</code></pre>
<p>The lane count <code>L</code> is the single topology parameter that
determines the shape of the entire computation. When <code>L = 1</code>,
the two-stage structure collapses to a single canonical tree over the
entire input sequence (one lane, no cross-lane reduction). Sections
§4.1–§4.4 define each component precisely; §4.3.4 addresses ragged tails
when <code>N</code> is not a multiple of <code>L</code>.</p>
<p>This specification defines the canonical expression structure only;
it does not prescribe an evaluation strategy. However, the iterative
pairwise formulation can be evaluated efficiently across threads while
preserving the canonical tree. When each thread processes a
power-of-two-sized chunk of the input, the shift-reduce process within
that chunk collapses to a single completed subtree — the minimum
possible merge state. Adjacent chunk results can then be combined in
index order, recovering the canonical expression regardless of thread
count. A detailed parallel realization strategy is described in Appendix
N.</p>
<h3 id="canonical-tree-shape-canonical.reduce.tree">4.1 Canonical tree
shape [canonical.reduce.tree]</h3>
<p>To mirror the style used in the Numerics clauses
(e.g. <code>GENERALIZED_SUM</code>), this paper introduces definitional
functions used solely to specify required parenthesization and
left/right operand order. These functions do not propose Standard
Library names.</p>
<p>For a fixed input order, lane count <code>L</code>, and
<code>binary_op</code>, the abstract expression (parenthesization and
left/right operand order) is uniquely determined by:</p>
<ol type="1">
<li><strong>Interleaved lanes</strong>: for a lane count <code>L</code>,
the input positions are partitioned into <code>L</code> logical
subsequences (lanes) based on <code>i % L</code>, preserving input order
within each lane (§4.3), followed by a second canonical reduction over
lane results (§4.4).</li>
<li><strong>A canonical iterative pairwise tree</strong> defined by a
standard-fixed algorithm (§4.2.3).</li>
</ol>
<p>[<em>Note:</em> The iterative pairwise-with-carry evaluation order
used by this paper corresponds to the well-known
<strong>shift-reduce</strong> (carry-chain / binary-counter stack)
formulation of pairwise summation described by Dalton, Wang, and Blainey
[Dalton2014]. <em>—end note</em>]</p>
<p>A near-balanced tree over a non-power-of-two number of leaves
implicitly contains operand positions for which no input element is
present. This paper makes that <em>absence</em> explicit in the semantic
model: when the tree geometry requires combining two operands but one
operand is absent, <code>binary_op</code> is not applied and the present
operand is propagated unchanged (§4.2.2). This avoids padding values and
imposes no identity-element requirements on <code>binary_op</code>.</p>
<h3 id="canonical-tree-reduction-with-absent-operands-canonical.reduce.tree.absent">4.2
Canonical tree reduction with absent operands
[canonical.reduce.tree.absent]</h3>
<h4 id="canonical-split-rule-pairing-count-per-round-canonical.reduce.tree.split">4.2.1
Canonical split rule (pairing count per round)
[canonical.reduce.tree.split]</h4>
<p>Given a working sequence of length <code>n</code>, define the number
of pairs formed in each round:</p>
<ul>
<li>let <code>h = floor(n / 2)</code>.</li>
</ul>
<p>The split rule <code>h = floor(n / 2)</code> is normative and governs
the number of pairs formed in each round of the iterative pairwise
algorithm (§4.2.3). It does not by itself determine the tree shape; the
complete tree is determined by the iterative pairing and carry logic
defined in §4.2.3.</p>
<h4 id="lifted-combine-on-absent-operands-canonical.reduce.tree.combine">4.2.2
Lifted combine on absent operands [canonical.reduce.tree.combine]</h4>
<p>Let <code>maybe&lt;A&gt;</code> be a conceptual domain with values
either <code>present(a)</code> (for <code>a</code> of type
<code>A</code>) or <code>∅</code> (“absent”). The
<code>maybe&lt;A&gt;</code> and <code>∅</code> notation are purely
definitional devices used to specify the handling of non-power-of-two
inputs without requiring an identity element; they do not appear in any
proposed interface and implementations need not model absence
explicitly.</p>
<p>Define the lifted operation <code>COMBINE(op, u, v)</code> on
<code>maybe&lt;A&gt;</code>:</p>
<ul>
<li><code>COMBINE(op, ∅, x) = x</code></li>
<li><code>COMBINE(op, x, ∅) = x</code></li>
<li><code>COMBINE(op, ∅, ∅) = ∅</code></li>
<li><code>COMBINE(op, present(x), present(y)) = present( op(x, y) )</code></li>
</ul>
<p>[<em>Note:</em> In implementation terms, <code>COMBINE</code> is the
familiar SIMD tail-masking or epilogue pattern: when an input sequence
does not fill the last group evenly, the implementation skips the
missing positions rather than fabricating values. The formalism above
specifies this behavior precisely without prescribing the implementation
technique (predicated lanes, scalar epilogue, masked operations, etc.).
<em>—end note</em>]</p>
<p>This lifted operation does not require <code>binary_op</code> to have
an identity element. Absence is a property of the expression geometry
(whether an operator application exists), not a property of
<code>binary_op</code>.</p>
<p>The use of <code>∅</code> is a definitional device. For any given
<code>(N, L)</code>, the locations of absent leaves are fully determined
(they occur only in the ragged tail, §4.3.4). Implementations can
therefore handle the tail using an epilogue or masking, without
introducing per-node conditionals in the main reduction.</p>
<h4 id="canonical-tree-evaluation-iterative-pairwise-canonical.reduce.tree.eval">4.2.3
Canonical tree evaluation: Iterative Pairwise
[canonical.reduce.tree.eval]</h4>
<p>This subsection defines the canonical tree-building algorithm using
iterative pairwise reduction. The algorithm pairs adjacent elements
left-to-right in each round, carrying the odd trailing element to the
next round. This corresponds to the shift-reduce summation pattern
described by Dalton, Wang, and Blainey [Dalton2014] and matches the
shuffle-down pattern used in CUDA CUB and GPU warp reductions (see §3.8
for rationale).</p>
<p><strong>Definition.</strong> Define
<code>CANONICAL_TREE_EVAL(op, Y[0..k))</code>, where
<code>k &gt;= 1</code> and each <code>Y[t]</code> is in
<code>maybe&lt;A&gt;</code>:</p>
<pre><code>CANONICAL_TREE_EVAL(op, Y[0..k)):
    if k == 1:
        return Y[0]
    
    // Iteratively pair adjacent elements until one remains
    let W = Y[0..k)  // working sequence (conceptual copy)
    while |W| &gt; 1:
        let n = |W|
        let h = floor(n / 2)
        // Pair elements: W&#39;[i] = COMBINE(op, W[2i], W[2i+1]) for i in [0, h)
        let W&#39; = [ COMBINE(op, W[2*i], W[2*i + 1]) for i in [0, h) ]
        // If n is odd, carry the last element
        if n is odd:
            W&#39; = W&#39; ++ [ W[n-1] ]
        W = W&#39;
    return W[0]</code></pre>
<p>When <code>CANONICAL_TREE_EVAL</code> returns <code>∅</code>, it
denotes that no present operands existed in the evaluated subtree.</p>
<p><strong>Shift-reduce state table (k = 8):</strong></p>
<p>The following table illustrates the iterative pairwise algorithm step
by step for <code>k = 8</code> elements, adapted from [Dalton2014]
Figure 4. Each step either <em>shifts</em> (pushes an element onto the
stack) or <em>reduces</em> (combines the top two stack entries of the
same tree level). Lowercase letters denote intermediate results at
successively higher levels of the tree: <code>a</code> terms are sums of
two elements, <code>b</code> terms are sums of two <code>a</code> terms,
and so on.</p>
<pre class="text"><code>Sequence                    Stack                 Operation
─────────────────────────── ───────────────────── ────────────────────
e₀ e₁ e₂ e₃ e₄ e₅ e₆ e₇   ∅                     shift e₀
e₁ e₂ e₃ e₄ e₅ e₆ e₇      e₀                    shift e₁
e₂ e₃ e₄ e₅ e₆ e₇         e₀ e₁                 reduce a₀ = op(e₀, e₁)
e₂ e₃ e₄ e₅ e₆ e₇         a₀                    shift e₂
e₃ e₄ e₅ e₆ e₇            a₀ e₂                 shift e₃
e₄ e₅ e₆ e₇               a₀ e₂ e₃              reduce a₁ = op(e₂, e₃)
e₄ e₅ e₆ e₇               a₀ a₁                 reduce b₀ = op(a₀, a₁)
e₄ e₅ e₆ e₇               b₀                    shift e₄
e₅ e₆ e₇                  b₀ e₄                 shift e₅
e₆ e₇                     b₀ e₄ e₅              reduce a₂ = op(e₄, e₅)
e₆ e₇                     b₀ a₂                 shift e₆
e₇                         b₀ a₂ e₆              shift e₇
∅                          b₀ a₂ e₆ e₇           reduce a₃ = op(e₆, e₇)
∅                          b₀ a₂ a₃              reduce b₁ = op(a₂, a₃)
∅                          b₀ b₁                 reduce c₀ = op(b₀, b₁)
∅                          c₀                    done</code></pre>
<p>The final result <code>c₀</code> is the value of the canonical
expression. The number of reductions after the <em>n</em>-th shift is
determined by the number of trailing zeros in the binary representation
of <em>n</em> [Dalton2014].</p>
<p><strong>Tree diagram (k = 8):</strong></p>
<p>The state table above produces the following canonical expression
tree — a perfectly balanced binary tree for power-of-two
<code>k</code>:</p>
<pre class="text"><code>                    op (c₀)
                  /        \
           op (b₀)          op (b₁)
           /     \          /     \
       op (a₀) op (a₁)  op (a₂) op (a₃)
        / \     / \       / \     / \
       e₀ e₁  e₂ e₃    e₄ e₅  e₆ e₇</code></pre>
<p>Expression:
<code>((e₀ ⊕ e₁) ⊕ (e₂ ⊕ e₃)) ⊕ ((e₄ ⊕ e₅) ⊕ (e₆ ⊕ e₇))</code></p>
<p><strong>Tree diagram (k = 7, non-power-of-two):</strong></p>
<p>When <code>k</code> is not a power of two, the odd trailing element
is carried forward, producing a slightly unbalanced tree. This is where
the carry logic in the algorithm definition above determines the
canonical shape:</p>
<pre class="text"><code>                 op
               /    \
              op      op
             /  \    /  \
            op   op  op  e₆
           / \ / \ / \
          e₀ e₁ e₂ e₃ e₄ e₅</code></pre>
<p>Expression:
<code>((e₀ ⊕ e₁) ⊕ (e₂ ⊕ e₃)) ⊕ ((e₄ ⊕ e₅) ⊕ e₆)</code></p>
<p>The left subtree is identical to the <code>k = 8</code> case with the
last element removed. The carry of <code>e₆</code> at round 1 (odd
<code>n = 7</code>) places it as the right child of the right subtree’s
right branch.</p>
<h4 id="canonical-tree-diagrams-informative">4.2.4 Canonical tree
diagrams (informative)</h4>
<p>The following diagrams illustrate the fixed abstract expression
structure only; they do not imply any particular evaluation order,
scheduling, or implementation strategy.</p>
<pre class="text"><code>Legend (informative)

present(x)  : a present operand holding value x (type A)
∅           : an absent operand position (no input element exists there)
combine(u,v) : lifted combine:
              - combine(∅, x) = x
              - combine(x, ∅) = x
              - combine(∅, ∅) = ∅
              - combine(x, y) = op(x, y) when both present
op(a,b)     : the user-supplied binary_op, applied only when both operands exist</code></pre>
<p>Example: absence propagation with k = 5, Y = [ X0, X1, X2, ∅, ∅ ]</p>
<pre class="text"><code>              COMBINE          COMBINE(op(op(X0,X1), X2), ∅) = op(op(X0,X1), X2)
             /       \
         COMBINE      ∅       carried from round 2 (odd n=3)
          /    \
        op    COMBINE          COMBINE(X2, ∅) = X2
       / \     / \
      X0  X1  X2  ∅</code></pre>
<p>Result: <code>op(op(X0, X1), X2)</code> — two <code>binary_op</code>
calls from three present elements. The two <code>∅</code> positions at
different tree levels each induce no application of
<code>binary_op</code>; the lifted COMBINE rule absorbs them
uniformly.</p>
<p>A near-balanced tree is not necessarily full; missing leaves may
induce absent operands at internal combine points as the tree reduces.
The lifted combine rule handles this uniformly.</p>
<h3 id="interleaved-topology-canonical.reduce.interleaved">4.3
Interleaved topology [canonical.reduce.interleaved]</h3>
<p>Let <code>E[0..N)</code> denote the input elements in iteration
order, and let <code>X[0..N)</code> denote the corresponding conceptual
terms of the reduction expression (materialization and the reduction
state type are defined in §4.6).</p>
<h4 id="lane-partitioning-by-index-modulo-canonical.reduce.interleaved.partition">4.3.1
Lane partitioning by index modulo
[canonical.reduce.interleaved.partition]</h4>
<p>For each lane index <code>j</code> in <code>[0, L)</code>,
define:</p>
<pre class="text"><code>I_j = &lt; i in [0, N) : (i mod L) == j &gt;, ordered by increasing i.
X_j = &lt; X[i] : i in I_j &gt;.</code></pre>
<p>This preserves the original input order within each lane
(equivalently, <code>X_j</code> contains positions
<code>j, j+L, j+2L, ...</code> that are less than <code>N</code>).</p>
<h4 id="fixed-length-lane-leaves-single-shape-per-lane-canonical.reduce.interleaved.leaves">4.3.2
Fixed-length lane leaves (single shape per lane)
[canonical.reduce.interleaved.leaves]</h4>
<p>Define <code>K = ceil(N / L)</code> when <code>N &gt; 0</code>. (The
<code>N == 0</code> case is handled by §4.5 and does not form
lanes.)</p>
<p>For each lane <code>j</code> in <code>[0, L)</code>, define a
fixed-length conceptual sequence <code>Y_j[0..K)</code> of
<code>maybe&lt;A&gt;</code> leaves:</p>
<ul>
<li>for <code>t</code> in <code>[0, K)</code>:
<ul>
<li>let <code>i = j + t*L</code></li>
<li>if <code>i &lt; N</code>: <code>Y_j[t] = present( X[i] )</code></li>
<li>otherwise: <code>Y_j[t] = ∅</code></li>
</ul></li>
</ul>
<p>Thus <strong>all lanes use the same canonical tree shape over
<code>K</code> leaf positions</strong>. Lanes with fewer than
<code>K</code> elements simply have trailing <code>∅</code> leaves;
these do not introduce padding values and do not require
identity-element properties of <code>binary_op</code>.</p>
<p>[<em>Note:</em> Implementations must not pad absent operand positions
with a constant value (e.g., <code>0.0</code> for addition) unless that
value is the identity element for the specific <code>binary_op</code>
and argument types. The lifted <code>COMBINE</code> rules (§4.2.2)
define the correct handling of absent positions for arbitrary
<code>binary_op</code>. The demonstrators in Appendix K use zero-padding
only because they test <code>std::plus&lt;double&gt;</code>, for which
<code>0.0</code> is the identity. <em>—end note</em>]</p>
<p>When <code>N &lt; L</code>, some lane indices <code>j</code>
correspond to no input positions: <code>Y_j[t] == ∅</code> for all
<code>t</code>, yielding <code>R_j == ∅</code>. No applications of
<code>binary_op</code> are induced for such lanes under the lifted
<code>COMBINE</code> rules.</p>
<p>[<em>Note:</em> When <code>L &gt; N</code>, <code>K = 1</code> and
each lane holds at most one element. Stage 1 performs no applications of
<code>binary_op</code> (each lane result is either a single present
value or <code>∅</code>). Stage 2 then applies
<code>CANONICAL_TREE_EVAL</code> over the <code>L</code> lane results,
of which only <code>N</code> are present; the <code>COMBINE</code> rules
propagate the <code>L − N</code> absent entries without invoking
<code>binary_op</code>. The result is therefore equivalent to
<code>CANONICAL_TREE_EVAL</code> applied directly to the <code>N</code>
input elements. In this regime <code>L</code> has no observable effect
on the returned value. This is intentional: no diagnostic is required,
and implementations need not special-case it. <em>—end note</em>]</p>
<h4 id="interleaving-layout-diagrams-informative">4.3.3 Interleaving
layout diagrams (informative)</h4>
<p>Example: <code>N = 10</code>, <code>L = 4</code> ⇒
<code>K = ceil(10/4) = 3</code></p>
<pre class="text"><code>Input order (i):   0   1   2   3   4   5   6   7   8   9
Elements X[i]:    X0  X1  X2  X3  X4  X5  X6  X7  X8  X9
Lane (i mod L):    0   1   2   3   0   1   2   3   0   1

Lanes preserve input order within each lane:

Lane 0: X0  X4  X8
Lane 1: X1  X5  X9
Lane 2: X2  X6
Lane 3: X3  X7</code></pre>
<p>Fixed-length leaves with absence (no padding values):</p>
<pre class="text"><code>Y_0: [ present(X0), present(X4), present(X8) ]
Y_1: [ present(X1), present(X5), present(X9) ]
Y_2: [ present(X2), present(X6), ∅           ]
Y_3: [ present(X3), present(X7), ∅           ]</code></pre>
<h4 id="ragged-tail-handling-canonical.reduce.interleaved.ragged">4.3.4
Ragged tail handling [canonical.reduce.interleaved.ragged]</h4>
<p>When <code>N</code> is not a multiple of <code>L</code>, the final
group of input elements is incomplete: some lanes receive one fewer
element than others, producing a ragged trailing edge across the lane
partition. The canonical expression handles this uniformly through the
<code>maybe&lt;A&gt;</code> formalism defined in §4.2.2. Every lane uses
the same tree shape over <code>K = ceil(N/L)</code> leaf positions, but
lanes whose element count falls short of <code>K</code> have trailing
<code>∅</code> leaves. The <code>COMBINE</code> rules propagate these
absences without invoking <code>binary_op</code> and without requiring
padding values or identity elements from the caller.</p>
<p><strong>Example:</strong> <code>N = 11</code>, <code>L = 4</code>, so
<code>K = ceil(11/4) = 3</code>.</p>
<p>The input is distributed across lanes by <code>i mod L</code>:</p>
<pre class="text"><code>Input:   X0  X1  X2  X3 | X4  X5  X6  X7 | X8  X9  X10
Block:   ──── full ──── | ──── full ──── | ─ ragged ──</code></pre>
<p>Lane assignment:</p>
<pre class="text"><code>Lane 0:  X0,  X4,  X8       (3 elements — full)
Lane 1:  X1,  X5,  X9       (3 elements — full)
Lane 2:  X2,  X6,  X10      (3 elements — full)
Lane 3:  X3,  X7,  ∅        (2 elements + 1 absent)</code></pre>
<p>All four lanes evaluate the same canonical tree shape over
<code>K = 3</code> leaf positions. For lanes 0–2, every leaf is present
and the tree evaluates normally. For lane 3, the tree encounters an
absent leaf:</p>
<pre class="text"><code>      COMBINE
       /    \
     op      ∅
    /   \
  X3    X7</code></pre>
<p><code>COMBINE(op(X3, X7), ∅) = op(X3, X7)</code> — no
<code>binary_op</code> application occurs for the absent position. The
result is identical to reducing only the present elements
<code>[X3, X7]</code>.</p>
<p>This mechanism generalizes to any <code>(N, L)</code> pair. The
number of ragged lanes is <code>L - (N mod L)</code> when
<code>N mod L ≠ 0</code>; these lanes each have exactly one trailing
<code>∅</code>. The remaining <code>N mod L</code> lanes have all
<code>K</code> positions present. When <code>N</code> is a multiple of
<code>L</code>, no lanes are ragged and no <code>∅</code> entries
arise.</p>
<p>In implementation terms, the ragged tail corresponds to the familiar
SIMD epilogue or tail-masking pattern: the final group of elements is
narrower than the full lane width, and the implementation must avoid
reading or combining nonexistent data. The <code>maybe&lt;A&gt;</code>
formalism specifies the required behavior without prescribing the
implementation technique (masking, scalar epilogue, predicated lanes,
etc.).</p>
<h3 id="two-stage-reduction-semantics-canonical.reduce.twostage">4.4
Two-stage reduction semantics [canonical.reduce.twostage]</h3>
<p>Let <code>L &gt;= 1</code> be the lane count and let <code>op</code>
denote the supplied <code>binary_op</code>.</p>
<h4 id="stage-1-per-lane-canonical-reduction-single-tree-shape-canonical.reduce.twostage.perlane">4.4.1
Stage 1 — Per-lane canonical reduction (single tree shape)
[canonical.reduce.twostage.perlane]</h4>
<p>For each lane index <code>j</code> in <code>[0, L)</code>,
define:</p>
<pre class="text"><code>R_j = CANONICAL_TREE_EVAL(op, Y_j)  // returns maybe&lt;A&gt;</code></pre>
<h4 id="stage-2-canonical-reduction-over-lane-results-in-increasing-lane-index-canonical.reduce.twostage.crosslane">4.4.2
Stage 2 — Canonical reduction over lane results (in increasing lane
index) [canonical.reduce.twostage.crosslane]</h4>
<p>Define a conceptual sequence <code>Z[0..L)</code> by:</p>
<ul>
<li><code>Z[j] = R_j</code> for <code>j</code> in
<code>[0, L)</code>.</li>
</ul>
<p>Then define:</p>
<pre class="text"><code>R_all = CANONICAL_TREE_EVAL(op, Z)  // returns maybe&lt;A&gt;</code></pre>
<p>When <code>N &gt; 0</code>, at least one lane contains a present
operand, therefore <code>R_all</code> is <code>present(r)</code> for
some <code>r</code> of type <code>A</code>. The interleaved reduction
result is that <code>r</code>.</p>
<p>Therefore, the overall expression is uniquely determined by the
canonical tree rule applied first within each lane and then across lanes
in increasing lane index order. When <code>L = 1</code>, there is a
single lane containing all <code>N</code> elements; Stage 2 receives one
input and returns it unchanged, so the result is simply
<code>CANONICAL_TREE_EVAL(op, Y_0)</code>.</p>
<p><strong>Summary definition.</strong> For convenience, define the
composite definitional function:</p>
<pre class="text"><code>CANONICAL_INTERLEAVED_REDUCE(L, op, X[0..N)):
    Partition X into L lanes by index modulo (§4.3.1).
    Form fixed-length leaf sequences Y_j[0..K) for each lane j (§4.3.2).
    For each lane j: R_j = CANONICAL_TREE_EVAL(op, Y_j).
    Form Z[0..L) where Z[j] = R_j.
    Return CANONICAL_TREE_EVAL(op, Z).</code></pre>
<p>When <code>N == 0</code>, <code>CANONICAL_INTERLEAVED_REDUCE</code>
is not invoked; the result is determined by §4.5.</p>
<h4 id="two-stage-diagrams-informative">4.4.3 Two-stage diagrams
(informative)</h4>
<p>Stage 1 summary (<code>N = 10</code>, <code>L = 4</code>,
<code>K = 3</code>; all lanes use the same shape; <code>∅</code>
propagates):</p>
<pre class="text"><code>Y_0 = [X0, X4, X8] --(canonical tree k=3)--&gt; R_0
Y_1 = [X1, X5, X9] --(canonical tree k=3)--&gt; R_1
Y_2 = [X2, X6, ∅ ] --(canonical tree k=3)--&gt; R_2
Y_3 = [X3, X7, ∅ ] --(canonical tree k=3)--&gt; R_3</code></pre>
<p>Stage 2 (cross-lane canonical reduction; same rules apply):</p>
<pre class="text"><code>Example: L = 4, Z = [R0, R1, R2, R3]

       combine
       /      \
    combine   combine
    /    \    /    \
   R0    R1  R2    R3</code></pre>
<p>Conceptual completeness: the same lifted rule handles absence in
Stage 2:</p>
<pre class="text"><code>Example: Z = [ R0, ∅, R2, ∅ ]

       combine
       /      \
    combine   combine
    /    \    /    \
   R0    ∅   R2    ∅

combine(R0, ∅) = R0
combine(R2, ∅) = R2
combine(R0, R2) = op(R0, R2)</code></pre>
<h4 id="equivalence-to-reducing-only-present-terms-informative">4.4.4
Equivalence to reducing only present terms (informative)</h4>
<p>For any lane <code>j</code>,
<code>CANONICAL_TREE_EVAL(op, Y_j)</code> evaluates the same abstract
expression as applying the canonical split rule (§4.2.1) to the
subsequence <code>X_j</code> containing only present terms, with the
understanding that absent operand positions do not create
<code>binary_op</code> applications. The explicit absence notation does
not affect the returned value; it makes the “absent operand” behavior
precise and enables a single tree shape for all lanes.</p>
<h3 id="integration-of-an-initial-value-if-provided-canonical.reduce.init">4.5
Integration of an initial value (if provided)
[canonical.reduce.init]</h3>
<p>If an initial value <code>init</code> is provided, the abstract
expression is:</p>
<ul>
<li>If <code>N == 0</code>: return <code>init</code>.</li>
<li>Otherwise:
<ul>
<li>let <code>R =</code> the value extracted from
<code>R_all = CANONICAL_TREE_EVAL(op, Z)</code> in §4.4.2</li>
<li>let <code>I</code> be a value of type <code>A</code> initialized
from <code>init</code> (where <code>A</code> is defined in §4.6)</li>
<li>return <code>op(I, R)</code>.</li>
</ul></li>
</ul>
<p>The placement of <code>init</code> is normative. In particular,
<code>init</code> is not interleaved into lanes and does not participate
in the canonical tree expression. Combining <code>init</code> with the
tree result in a single final application of <code>binary_op</code>
ensures that the canonical tree shape is independent of whether an
initial value is provided. The left‑operand placement is consistent with
existing fold‑style conventions; because this proposal does not require
commutativity of <code>binary_op</code>, the position of
<code>init</code> is specified. In particular,
<code>std::accumulate</code> places <code>init</code> as the left
operand at every step (<code>op(op(init, x₀), x₁)...</code>); this
proposal preserves that convention so that non-commutative operations
produce consistent results when migrating from
<code>std::accumulate</code> to the canonical reduction.</p>
<p>[<em>Note:</em> Whether a convenience form without an explicit
<code>init</code> is provided, and what default it uses, is an API
decision deferred to a future revision. <em>—end note</em>]</p>
<pre class="text"><code>With init (conceptual):

Result = op( init, CANONICAL_INTERLEAVED_REDUCE(L, op, X[0..N)) )

init is not interleaved into lanes and is combined once with the overall result.</code></pre>
<p><em>Informative contrast:</em></p>
<pre class="text"><code>std::accumulate:
  (((init op X0) op X1) op X2) ... op XN-1

This proposal:
  init op ( fixed canonical tree expression over X0..XN-1 )</code></pre>
<h3 id="materialization-of-conceptual-terms-and-reduction-state-introducing-v-and-a-canonical.reduce.types">4.6
Materialization of conceptual terms and reduction state (introducing V
and A) [canonical.reduce.types]</h3>
<p>The preceding sections (§4.1–§4.4) define the canonical expression
structure over abstract sequences. This section specifies the type rules
that bridge the abstract expression to C++ evaluation.</p>
<p>Let:</p>
<ul>
<li><code>V</code> be the value type of the input sequence
elements,</li>
<li><code>A</code> be the reduction state type:
<ul>
<li>if an initial value <code>init</code> of type T is provided,
<code>A = remove_cvref_t&lt;T&gt;</code>;</li>
<li>otherwise <code>A = V</code>.</li>
</ul></li>
</ul>
<p>Define the conceptual term sequence <code>X[0..N)</code> of type A by
converting each input element:</p>
<p><strong><code>X[i] = static_cast&lt;A&gt;(E[i])</code></strong> for
<code>i in [0, N)</code>.</p>
<p>All applications of <code>binary_op</code> within the definitional
functions in §4 operate on values of type A.</p>
<p><strong>Constraints:</strong> Let <code>A</code> be the reduction
state type defined above.</p>
<p>When an initial value <code>init</code> is provided, the
initialization <code>A{init}</code> shall be well-formed.</p>
<ul>
<li>Each conceptual term <code>X[i]</code> is formed by conversion to
<code>A</code> as specified above.</li>
<li>If an initial value <code>init</code> is provided, it is
materialized as a value <code>I</code> of type <code>A</code>
initialized from <code>init</code> and participates in the expression as
<code>op(I, R)</code> per §4.5.</li>
<li><code>binary_op</code> shall be invocable with two arguments of type
<code>A</code>, and the result shall be convertible to
<code>A</code>.</li>
</ul>
<p>[<em>Note:</em> How <code>V</code> is derived from the input —
whether as <code>iter_value_t&lt;InputIt&gt;</code> for an iterator-pair
interface, <code>range_value_t&lt;R&gt;</code> for a range interface, or
otherwise — is an API decision deferred to a future revision. The
semantic contract requires only that <code>V</code> is well-defined and
that the conversion <code>static_cast&lt;A&gt;(E[i])</code> is
well-formed. Proxy reference types (e.g.,
<code>std::vector&lt;bool&gt;::reference</code>) and their interaction
with <code>V</code> are likewise API-level concerns. <em>—end
note</em>]</p>
<h2 id="invariance-properties-canonical.reduce.invariance">5. Invariance
Properties [canonical.reduce.invariance]</h2>
<p>This proposal does <strong>not</strong> impose associativity or
commutativity requirements on <code>binary_op</code>. Instead of
permitting implementations to reassociate or reorder (which can make
results unspecified for non-associative operations), this proposal
defines a single canonical abstract expression for fixed
<code>(N, L)</code>. Determinism is obtained by fixing the expression,
not by restricting <code>binary_op</code>.</p>
<p>See §2.3 for exception/termination behavior; in particular, when
<code>std::terminate</code> is called under a policy-based evaluation,
the state of outputs and any externally observable side effects is
unspecified.</p>
<p>See §2.4 for complexity; guarantees are intentionally limited to work
complexity, consistent with <code>std::reduce</code>.</p>
<p>For a chosen <strong>topology coordinate</strong> (lane count
<code>L</code>), the fixed expression structure provides:</p>
<p><strong>Topological determinism:</strong> For fixed input order,
<code>binary_op</code>, lane count <code>L</code>, and <code>N</code>,
the abstract expression (grouping and left/right operand order) is fully
specified by §4. It does not depend on implementation choices, SIMD
width, thread count, or scheduling decisions.</p>
<p><strong>Layout invariance:</strong> Results are independent of memory
alignment and physical placement, given the same input sequence as
observed through the iterator range.</p>
<p><strong>Execution independence:</strong> Implementations may evaluate
independent subtrees in any order or concurrently. Only the grouping is
specified, not the schedule.</p>
<p><strong>Cross-invocation reproducibility:</strong> Given the same
topology coordinate <code>L</code>, input sequence,
<code>binary_op</code>, and floating-point evaluation model, the
returned value is stable across invocations (it is the value of the same
specified expression under the same evaluation model).</p>
<p><strong>Scope of guarantee (returned value):</strong> The run-to-run
stability guarantee applies to the <strong>returned value</strong> of
the reduction. If <code>binary_op</code> performs externally observable
side effects, the order and interleaving of those side effects is not
specified by this paper and may vary between invocations.</p>
<p><strong>Constraints on <code>binary_op</code>:</strong> Let
<code>A</code> be the reduction state type (§4.6). The only requirements
on <code>binary_op</code> are invocability with two arguments of type
<code>A</code> and convertibility of the result to <code>A</code>. No
associativity, commutativity, or identity-element requirements are
imposed. Because the canonical expression is fixed for a given
<code>(N, L)</code>, determinism is obtained by fixing the expression
structure, not by restricting <code>binary_op</code>.</p>
<p>The remaining requirements on iterators, value types, and side
effects match those of the corresponding <code>std::reduce</code>
facility ([numeric.ops.reduce]):</p>
<ul>
<li>When evaluated without an execution policy, <code>binary_op</code>
shall not invalidate iterators or subranges, nor modify elements in the
input range.</li>
<li>When evaluated with an execution policy, <code>binary_op</code> is
an element access function subject to the requirements in
[algorithms.parallel.exec].</li>
</ul>
<p>When evaluated without an execution policy, <code>binary_op</code> is
invoked as part of a normal library algorithm call; this paper does not
require concurrent evaluation. When evaluated with an execution policy,
the requirements of [algorithms.parallel.exec] additionally apply.</p>
<p>The run-to-run stability guarantee applies to the returned value when
<code>binary_op</code> is functionally deterministic — that is, when it
returns the same result for the same operand values. If
<code>binary_op</code> reads mutable global state, uses random number
generation, or is otherwise non-deterministic, the returned value may
vary even with fixed topology coordinate and input.</p>
<p>[<em>Note:</em> Functional determinism of <code>binary_op</code> is
not a formal requirement (the standard cannot enforce functional
purity), but an observation about when the stability guarantee is
meaningful. <em>—end note</em>]</p>
<p>Cross-platform reproducibility requires users to ensure an identical
topology coordinate <code>L</code> and an equivalent floating-point
evaluation model (§2.5, §6).</p>
<h2 id="floating-point-considerations-informative">6. Floating-Point
Considerations (Informative)</h2>
<p>This section discusses what is and is not guaranteed about
floating-point results under the canonical expression defined in §4.</p>
<p><strong>Terminology:</strong> This paper uses <strong>floating-point
evaluation model</strong> to mean the combination of the program’s
runtime floating-point environment (e.g., <code>&lt;cfenv&gt;</code>
rounding mode) and the translation/target choices that affect how
floating-point expressions are evaluated (e.g., contraction/FMA, excess
precision, subnormal handling, fast-math).</p>
<p><strong>What is specified:</strong> For a given topology coordinate
<code>L</code>, input sequence, <code>binary_op</code>, and
<code>init</code>, the result is the value obtained by evaluating the
canonical compute sequence defined in §4, in the floating-point
evaluation model in effect for the program.</p>
<p><strong>What this enables:</strong> By removing library-permitted
reassociation, repeated executions of the same program under a stable
evaluation model can obtain the same result independent of thread count,
scheduling, or SIMD width.</p>
<p><strong>What it does not attempt to specify:</strong>
Cross-architecture bitwise identity is not a goal of this paper. Users
who require bitwise identity must additionally control the relevant
evaluation-model factors and ensure that <code>sizeof(V)</code> (and
thus lane count) is stable across the intended platforms.</p>
<p><strong>Relationship to P3375 (Reproducible floating-point
results):</strong> Davidson’s P3375 [P3375R2] proposes a
<code>strict_float</code> type that specifies sufficient conformance
with ISO/IEC 60559:2020 to guarantee reproducible floating-point
arithmetic across implementations. This proposal and P3375 are
complementary: this paper fixes the <em>expression structure</em>
(parenthesization and operand order) of a parallel reduction, while
P3375 addresses the <em>evaluation model</em> (rounding, contraction,
intermediate precision) for individual operations. Together, they would
provide both necessary conditions for cross-platform bitwise
reproducibility of parallel reductions. Neither paper alone is
sufficient.</p>
<p><strong>Relationship to <code>std::simd</code> (P1928):</strong> This
proposal is orthogonal to <code>std::simd</code>.
<code>std::simd::reduce</code> performs a horizontal reduction within a
single SIMD value with unspecified order; this facility defines a
deterministic expression structure over an arbitrary-length input range.
An implementation may use <code>std::simd</code> operations internally,
but the semantic contract does not depend on <code>std::simd</code>.</p>
<p>The C++ <code>&lt;cfenv&gt;</code> floating-point environment covers
rounding mode and exception flags; many other factors that affect
floating-point results (such as contraction/FMA and intermediate
precision) are translation- or target-dependent and are not fully
specified by the C++ abstract machine. This proposal therefore
guarantees expression identity, not universal bitwise identity.</p>
<h2 id="relationship-to-existing-facilities-informative">7. Relationship
to Existing Facilities (Informative)</h2>
<p>This paper specifies a canonical expression structure for parallel
reduction. The goal is to complete the reduction “semantic spectrum” in
the Standard Library: from specified but sequential, to parallel but
unspecified, to parallel and specified.</p>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 44%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr class="header">
<th>Facility</th>
<th>Parallel</th>
<th>Expression specified</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>std::accumulate</code></td>
<td>No</td>
<td>Yes (left-fold)</td>
<td>Fully specified; sequential</td>
</tr>
<tr class="even">
<td><code>std::reduce</code></td>
<td>Yes</td>
<td>No (generalized sum)</td>
<td>Unspecified grouping; results may vary for non-associative ops</td>
</tr>
<tr class="odd">
<td>HPC frameworks (Kokkos, etc.)</td>
<td>Yes</td>
<td>No</td>
<td>Strategy-dependent grouping; FP results may vary</td>
</tr>
<tr class="even">
<td>oneTBB <code>parallel_reduce</code></td>
<td>Yes</td>
<td>No</td>
<td>Join order varies; scheduler-dependent results</td>
</tr>
<tr class="odd">
<td><strong>Canonical reduction</strong> (this proposal)</td>
<td>Yes</td>
<td>Yes (§4 tree)</td>
<td>Fixed parenthesization for chosen topology coordinate L; free
scheduling</td>
</tr>
</tbody>
</table>
<p><strong>What this proposal adds:</strong> a standard-specified
expression for parallel reduction, closing the third cell in the table
above.</p>
<p>[<em>Note:</em> This paper uses
<code>canonical_reduce_lanes&lt;L&gt;(...)</code> as the primary
illustrative spelling, reflecting that lane count <code>L</code> is the
semantic topology coordinate. For layout-stable numeric types, an API
may additionally provide a span spelling
<code>canonical_reduce&lt;M&gt;(...)</code> as a convenience (where
<code>L = M / sizeof(V)</code> when well-formed; see §9.2). No Standard
Library API is proposed in this paper. <em>—end note</em>]</p>
<h2 id="motivation-and-use-cases-informative">8. Motivation and Use
Cases (Informative)</h2>
<p>This proposal is motivated by workloads where run-to-run stability
matters, but existing parallel reductions are intentionally free to
choose an evaluation order (and thus may vary with scheduling). Typical
use cases include:</p>
<ul>
<li><strong>CI regression testing:</strong> A reduction that is stable
across runs eliminates intermittent test failures and enables “golden
value” comparisons.</li>
<li><strong>Debugging and bisection:</strong> A stable result makes it
practical to reproduce and minimize numerical regressions without
chasing schedule-dependent drift.</li>
<li><strong>Auditability / reproducible analytics:</strong> Regulatory
or scientific workflows often require re-running computations and
obtaining the same result within a defined environment.</li>
<li><strong>Large-scale simulation and ML training:</strong> Stable
aggregation of large sums (e.g., gradients, risk factors) improves
repeatability of model training and scenario analysis.</li>
<li><strong>Heterogeneous execution:</strong> A single semantic contract
that can be implemented on CPU and GPU enables consistent verification,
even when the execution strategy differs.</li>
</ul>
<p>Detailed examples (with code) are collected in Appendix M.</p>
<h2 id="api-design-space-informative">9. API Design Space
(Informative)</h2>
<p>This section sketches possible directions for exposing the canonical
expression defined in §4. The intent is to build consensus on the
semantic contract before committing to an API surface.</p>
<p>A key design point is that the <strong>semantic topology parameter is
lane count L</strong>. A byte span M is a numerics convenience
coordinate that derives <code>L = M / sizeof(V)</code> when well-formed
(§9.2). An API may choose to expose one or both coordinates.</p>
<p>Two primary approaches exist.</p>
<h3 id="new-algorithm-approach-illustrative">9.1 New Algorithm Approach
(Illustrative)</h3>
<p>Expose the semantics as a new algorithm (illustrative spelling
only):</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Illustrative only: name/signature not proposed in this paper</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="co">// Lane-based topology (portable across ABIs for a fixed L)</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="dt">size_t</span> L<span class="op">,</span> <span class="kw">class</span> InputIt<span class="op">,</span> <span class="kw">class</span> T<span class="op">,</span> <span class="kw">class</span> BinaryOp<span class="op">&gt;</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="kw">constexpr</span> T canonical_reduce_lanes<span class="op">(</span>InputIt first<span class="op">,</span> InputIt last<span class="op">,</span> T init<span class="op">,</span> BinaryOp op<span class="op">);</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="co">// Span-based topology (numerics convenience; derives L from sizeof(V))</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="dt">size_t</span> M<span class="op">,</span> <span class="kw">class</span> InputIt<span class="op">,</span> <span class="kw">class</span> T<span class="op">,</span> <span class="kw">class</span> BinaryOp<span class="op">&gt;</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="kw">constexpr</span> T canonical_reduce<span class="op">(</span>InputIt first<span class="op">,</span> InputIt last<span class="op">,</span> T init<span class="op">,</span> BinaryOp op<span class="op">);</span></span></code></pre></div>
<p><strong>Rationale:</strong> The choice of topology affects observable
results for non-associative operations (e.g., floating-point addition).
This argues for an API whose contract explicitly includes the topology
coordinate, rather than treating topology as an implementation
detail.</p>
<h3 id="span-convenience-deriving-l-from-a-byte-span-m-informative">9.2
Span Convenience: Deriving L from a Byte Span M (Informative)</h3>
<p>Some environments prefer selecting topology using a byte-based span
coordinate <code>M</code> rather than specifying the lane count
<code>L</code> directly. Such a coordinate is derived only and does not
change the semantic definition in §4, which is defined entirely in terms
of <code>L</code>.</p>
<p>Let <code>value_type</code> denote
<code>iter_value_t&lt;InputIt&gt;</code>. When a span coordinate
<code>M</code> is used and is well-formed for <code>value_type</code>,
derive:</p>
<ul>
<li><code>L = M / sizeof(value_type)</code>.</li>
</ul>
<p>Interpret the span spelling using the same definitional function
name:</p>
<p><strong><code>CANONICAL_INTERLEAVED_REDUCE(M, value_type, op, X) = CANONICAL_INTERLEAVED_REDUCE(L, op, X)</code></strong>,
with <code>L</code> as above.</p>
<p>M is well-formed for <code>value_type</code> only when:</p>
<ul>
<li><code>M &gt;= sizeof(value_type)</code>, and</li>
<li><code>M % sizeof(value_type) == 0</code>.</li>
</ul>
<p>A future API should reject invalid M rather than silently rounding to
a different L.</p>
<p>[<em>Note:</em> While the span spelling <code>M</code> provides
convenient alignment with SIMD register widths for layout-stable
arithmetic types, users requiring cross-platform expression identity
(e.g., for UDTs or heterogeneous verification) must specify topology via
lane count <code>L</code>. The mapping
<code>L = M / sizeof(value_type)</code> is platform-dependent when
<code>sizeof(value_type)</code> varies across targets. <em>—end
note</em>]</p>
<h4 id="mapping-the-span-spelling-m-to-lanes-l-informative">Mapping the
span spelling M to lanes L (informative)</h4>
<p>The span spelling selects topology by deriving
<code>L = M / sizeof(value_type)</code> when well-formed.</p>
<pre class="text"><code>Example: M = 64 bytes

Case A: value_type = float  (sizeof(float) = 4) =&gt; L = 64 / 4 = 16 lanes
Case B: value_type = double (sizeof(double) = 8) =&gt; L = 64 / 8 = 8 lanes

Interleaving rule (semantic): element i belongs to lane (i % L).

For L = 8 (e.g. double, M=64):
Input indices:    0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ...
Lane index (i%8): 0 1 2 3 4 5 6 7 0 1  2  3  4  5  6  7 ...

Lanes (preserving original order within each lane):
  lane 0: X[0], X[8], X[16], ...
  lane 1: X[1], X[9], X[17], ...
  lane 2: X[2], X[10], X[18], ...
  ...
  lane 7: X[7], X[15], X[23], ...</code></pre>
<p>When topology is selected via <code>M</code>, expression identity
across platforms additionally depends on <code>sizeof(value_type)</code>
being stable within the intended reproducibility domain. Users who
require topology identity independent of representation should specify
<code>L</code> directly.</p>
<p><strong>Convenience spelling example (informative):</strong></p>
<div class="sourceCode" id="cb28"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Primary spelling: select topology via lane count L</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> r_lanes <span class="op">=</span> canonical_reduce_lanes<span class="op">&lt;</span><span class="dv">8</span><span class="op">&gt;(</span>first<span class="op">,</span> last<span class="op">,</span> init<span class="op">,</span> op<span class="op">);</span> <span class="co">// L = 8</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co">// Convenience spelling: select topology via a byte span M (numeric domains)</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co">// For value_type = double, sizeof(double) == 8, so M = 64 derives L = 8:</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> r_span <span class="op">=</span> canonical_reduce<span class="op">&lt;</span><span class="dv">64</span><span class="op">&gt;(</span>first<span class="op">,</span> last<span class="op">,</span> init<span class="op">,</span> op<span class="op">);</span> <span class="co">// M = 64 bytes</span></span></code></pre></div>
<h3 id="execution-policy-approach-illustrative">9.3 Execution Policy
Approach (Illustrative)</h3>
<p>Expose the semantics as a new execution policy (illustrative spelling
only):</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Illustrative only: policy type/spelling not proposed in this paper</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="dt">size_t</span> M<span class="op">&gt;</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="kw">struct</span> canonical_policy <span class="op">{</span> <span class="co">/* ... */</span> <span class="op">};</span></span></code></pre></div>
<p>This approach integrates naturally with the existing parallel
algorithms vocabulary. However, as established in §3.0, execution
policies in the current standard are designed to constrain
<em>scheduling</em>, not <em>expression structure</em>. Encoding
topology in a policy would require the policy to carry semantic
guarantees about the returned value — a role that policies do not
currently play. It also raises unresolved questions about policy
composition: what happens when two policies specify conflicting
topologies, or when a topology-carrying policy composes with one that
permits reassociation?</p>
<h3 id="trade-offs-informative">9.4 Trade-offs (Informative)</h3>
<p>The expression/algorithm/execution analysis in §3.0 informs this
trade-off:</p>
<ul>
<li>A dedicated algorithm places expression ownership where the standard
already locates semantic contracts: in the algorithm. The topology
coordinate is explicit, composition questions do not arise, and the
facility can be taught as “this algorithm computes <em>this</em>
expression” — parallel to how <code>std::accumulate</code> computes a
left fold.</li>
<li>A policy-based approach may reduce algorithm surface area but
conflates expression and execution — the very conflation that §3.0
identifies as the source of existing confusion around
<code>std::reduce</code> and <code>execution::seq</code>. It would also
require new rules for policy dominance and semantic interaction that do
not exist in the current standard.</li>
</ul>
<h3 id="naming-considerations-informative">9.5 Naming Considerations
(Informative)</h3>
<p>This paper uses <code>canonical_reduce_lanes&lt;L&gt;(...)</code> as
the primary illustrative spelling, reflecting that lane count
<code>L</code> is the semantic topology coordinate. A span spelling
<code>canonical_reduce&lt;M&gt;(...)</code> may also appear as a
numerics convenience (where <code>L = M / sizeof(V)</code> when
well-formed). Final naming should communicate that:</p>
<ul>
<li>the return value is defined by a specified abstract expression;
and</li>
<li>the topology coordinate is part of the semantic contract.</li>
</ul>
<h3 id="topology-defaults-and-named-presets-informative">9.6 Topology
Defaults and Named Presets (Informative)</h3>
<p>If an eventual API offers a no-argument default topology selection,
it should avoid “silent semantic drift” across targets or toolchain
versions.</p>
<p>In this paper, a <strong>topology preset</strong> (also referred to
as a <strong>span preset</strong>) is a named, standard-fixed constant
used to select the topology coordinate—typically the interleave span M
(bytes), and thereby the derived lane count L for layout-stable numeric
types—in a way that remains stable across targets and toolchain
versions.</p>
<table>
<colgroup>
<col style="width: 22%" />
<col style="width: 47%" />
<col style="width: 30%" />
</colgroup>
<thead>
<tr class="header">
<th>Option</th>
<th>Example spelling</th>
<th>Trade-off</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>No default</td>
<td><code>canonical_reduce&lt;M&gt;(...)</code> required</td>
<td>Most explicit; no surprises; but higher user burden</td>
</tr>
<tr class="even">
<td>Implementation-defined default</td>
<td><code>canonical_reduce(...)</code> selects an implementation-defined
topology</td>
<td>Easy to teach; can silently change returned values across
targets/versions</td>
</tr>
<tr class="odd">
<td><strong>Standard-fixed named presets</strong></td>
<td><code>canonical_reduce&lt;std::canonical_span_small&gt;(...)</code></td>
<td>Readable; coordinated semantics; stable across targets/versions</td>
</tr>
<tr class="even">
<td>Explicit literal</td>
<td><code>canonical_reduce_lanes&lt;128&gt;(...)</code></td>
<td>Maximum control; most verbose; requires users to pick a number</td>
</tr>
</tbody>
</table>
<p>A committee-robust approach is to provide <strong>standard-fixed
named presets</strong> for the span coordinate, and (if a default is
adopted for numeric types) define it in terms of such a preset
constant.</p>
<p><strong>Illustrative named presets (standard-fixed):</strong></p>
<div class="sourceCode" id="cb30"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">namespace</span> std <span class="op">{</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a> <span class="co">// Named semantic presets (bytes)</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a> <span class="kw">inline</span> <span class="kw">constexpr</span> <span class="dt">size_t</span> canonical_span_small <span class="op">=</span> <span class="dv">128</span><span class="op">;</span> <span class="co">// baseline coordination preset</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a> <span class="kw">inline</span> <span class="kw">constexpr</span> <span class="dt">size_t</span> canonical_span_large <span class="op">=</span> <span class="dv">1024</span><span class="op">;</span> <span class="co">// wide coordination preset</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a> <span class="co">// The golden reference span for a given V (derives L == 1)</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a> <span class="kw">template</span><span class="op">&lt;</span><span class="kw">class</span> V<span class="op">&gt;</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a> <span class="kw">inline</span> <span class="kw">constexpr</span> <span class="dt">size_t</span> canonical_span_max_portability <span class="op">=</span> <span class="kw">sizeof</span><span class="op">(</span>V<span class="op">);</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a> <span class="co">// Evolution rule: existing preset values never change.</span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a> <span class="co">// Future standards may add new presets under new names.</span></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a><span class="co">// This paper does not propose a global default topology.</span></span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a><span class="co">// A follow-on API paper may propose a default in terms of a named preset.</span></span></code></pre></div>
<p>[<em>Note:</em> Appendix J provides an indicative “straw-man” API
that uses these standard-fixed named presets (Option 3) without
committing the committee to a final spelling or placement. <em>—end
note</em>]</p>
<p>For types where <code>sizeof(V)</code> is not stable across the
intended reproducibility domain, users requiring cross-platform topology
identity should select topology by lane count L (or enforce
representation as part of the contract).</p>
<h4 id="practical-topology-selection-guidance-informative">Practical
topology selection guidance (informative)</h4>
<p>The semantic coordinate is <code>L</code>. When using the span
convenience <code>M</code>, <code>L = M / sizeof(value_type)</code>.
Performance is often improved when <code>L</code> aligns with the
target’s preferred execution granularity, but the Standard does not
prescribe hardware behavior.</p>
<ul>
<li><strong>CI/CD and cross-platform verification baseline:</strong>
choose <code>L = 1</code> (equivalently,
<code>M = sizeof(value_type)</code>), yielding a single global canonical
tree.</li>
<li><strong>Typical CPU deployment:</strong> choose <code>L</code>
matching the target SIMD lane count (e.g., <code>L = 4</code> for
AVX2/double, <code>L = 8</code> for AVX-512/double).</li>
<li><strong>GPU warp-level consistency:</strong> choose
<code>L = warp_width</code> (e.g., <code>L = 32</code> on NVIDIA
GPUs).</li>
</ul>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 22%" />
<col style="width: 22%" />
<col style="width: 22%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr class="header">
<th>Use case</th>
<th>L (double)</th>
<th>L (float)</th>
<th>M (bytes)</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Golden reference</strong></td>
<td>1</td>
<td>1</td>
<td><code>sizeof(V)</code></td>
<td>Single tree; for debugging/golden values</td>
</tr>
<tr class="even">
<td><strong>Portability baseline</strong></td>
<td>2</td>
<td>4</td>
<td>16</td>
<td>SSE/NEON width</td>
</tr>
<tr class="odd">
<td><strong>AVX / AVX2</strong></td>
<td>4</td>
<td>8</td>
<td>32</td>
<td>Desktop/server AVX2</td>
</tr>
<tr class="even">
<td><strong>AVX-512</strong></td>
<td>8</td>
<td>16</td>
<td>64</td>
<td>AVX-512 servers</td>
</tr>
<tr class="odd">
<td><strong>CUDA warp (double)</strong></td>
<td>32</td>
<td>—</td>
<td>256</td>
<td>32-thread warp</td>
</tr>
</tbody>
</table>
<h2 id="what-lewg-is-being-asked-to-agree-to-this-paper">What LEWG is
being asked to agree to (this paper)</h2>
<p>We seek direction on the semantic contract and tree shape; API
surface is explicitly deferred.</p>
<ul>
<li>We are <strong>not</strong> approving an API.</li>
<li>Authors recommend selecting <strong>iterative pairwise</strong> in
Poll 2A to align the canonical shape with common SIMD/GPU reduction
practice.</li>
<li>We are approving the <strong>fixed expression contract in
§4</strong> (canonical compute sequence) as a semantic building block:
implementations may schedule using threads/SIMD/work-stealing/GPU
kernels, but must return the value <strong>as-if evaluating the
standard-fixed expression</strong>.</li>
<li>We intend the canonical expression to have <strong>O(N)</strong>
applications of <code>binary_op</code> (accounting for carry/propagation
rules) and <strong>O(log N)</strong> depth for the canonical shape; the
semantic definition imposes <strong>no workspace requirement</strong>
(implementations may use workspace).</li>
<li>A future revision will return with an <strong>API surface</strong>
and <strong>constraints/range categories</strong> aligned with
<code>std::reduce</code> and execution-policy requirements.</li>
</ul>
<h2 id="polls-for-lewg-direction-this-paper">Polls for LEWG Direction
(this paper)</h2>
<p><em>Vote categories for all Favor/Against polls:</em>
<strong>Strongly Favor / Weakly Favor / Neutral / Weakly Against /
Strongly Against</strong></p>
<h3 id="poll-1-semantics-first-scope">Poll 1 — Semantics-first
scope</h3>
<p><strong>Question:</strong> We agree this paper proposes semantics
only, and we want LEWG to validate the fixed expression structure before
committing to API design.</p>
<p><strong>Vote:</strong> Strongly Favor / Weakly Favor / Neutral /
Weakly Against / Strongly Against</p>
<h3 id="poll-2-core-semantic-contract">Poll 2 — Core semantic
contract</h3>
<p><strong>Question:</strong> We agree the canonical expression defined
in §4 is a suitable semantic contract: implementations may execute using
threads/SIMD/work-stealing/GPU kernels, but must produce results as-if
evaluating the canonical compute sequence defined in §4. (Approval of
this poll validates the semantic contract only and does not approve an
API.)</p>
<p><strong>Vote:</strong> Strongly Favor / Weakly Favor / Neutral /
Weakly Against / Strongly Against</p>
<h3 id="poll-2a-canonical-tree-shape">Poll 2A — Canonical tree
shape</h3>
<p><strong>Question:</strong> We agree that iterative pairwise reduction
with carry (§4.2.3) is an acceptable canonical tree shape for this
proposal.</p>
<p><strong>Vote:</strong> Strongly Favor / Weakly Favor / Neutral /
Weakly Against / Strongly Against</p>
<p>[<em>Note:</em> Recursive bisection (“balanced”) was considered as an
alternative tree construction. It is documented in Appendix X
(informative) for comparison and historical record, but is not proposed
as an alternative in this paper. <em>—end note</em>]</p>
<p>If consensus is not reached on tree shape, authors will return with a
follow‑up comparing alternatives; lack of consensus on this poll does
not block agreement on the core semantic contract (Poll 2).</p>
<h3 id="poll-3-proceed-to-api-surface-clarified">Poll 3 — Proceed to API
surface (clarified)</h3>
<p><strong>Question:</strong> We agree the authors may return with one
or more API surface proposals (new algorithm and/or execution policy),
including proposed naming and header placement, an initial overload set
(iterators/ranges, with/without execution policy as applicable),
constraints/mandates, and a proposal for topology selection (including
any named topology presets and defaulting rules), contingent on
favorable outcomes for Polls 1–2, without committing LEWG to adopt a
specific surface.</p>
<p><strong>Vote:</strong> Strongly Favor / Weakly Favor / Neutral /
Weakly Against / Strongly Against</p>
<h3 id="poll-4a-straw-poll-api-approach-new-algorithm">Poll 4A — Straw
poll: API approach (new algorithm)</h3>
<p>[<em>Note:</em> Polls 4A and 4B are straw polls to guide the
direction of a future API revision, contingent on favorable outcomes for
Polls 1–3. They do not approve an API in this paper. <em>—end
note</em>]</p>
<p><strong>Question:</strong> For the API surface, we support pursuing a
<strong>new algorithm</strong> approach (topology parameter is visible
in the algorithm contract;
e.g. <code>canonical_reduce_lanes&lt;L&gt;(...)</code> (primary,
semantic coordinate) with an optional span spelling
<code>canonical_reduce&lt;M&gt;(...)</code> as convenience for numeric
domains).</p>
<p><strong>Vote:</strong> Strongly Favor / Weakly Favor / Neutral /
Weakly Against / Strongly Against</p>
<h3 id="poll-4b-straw-poll-api-approach-execution-policy">Poll 4B —
Straw poll: API approach (execution policy)</h3>
<p><strong>Question:</strong> For the API surface, we support pursuing
an <strong>execution policy</strong> approach (topology is encoded in a
policy type and composes with existing parallel algorithm forms).</p>
<p><strong>Vote:</strong> Strongly Favor / Weakly Favor / Neutral /
Weakly Against / Strongly Against</p>
<p><em>Interpretation (non-normative):</em> If both Poll 4A and Poll 4B
are favored, return with both surfaces (or an algorithm-first surface
plus a policy mapping) and ask LEWG to converge on a primary surface
after reviewing wording/teachability/composition.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>The author thanks Bryce Adelstein Lelbach for a helpful discussion on
the algorithm-vs-execution-policy design trade-off.</p>
<h2 id="revision-history">Revision History</h2>
<p><strong>DxxxxR0:</strong> Initial draft.</p>
<h2 id="references">References</h2>
<h3 id="c-standard-references">C++ Standard References</h3>
<ul>
<li><p><strong>[algorithms.general]</strong> — ISO/IEC 14882, General
requirements for algorithms. Specifies exception behavior for sequential
algorithm overloads.</p></li>
<li><p><strong>[algorithms.parallel.exceptions]</strong> — ISO/IEC
14882, Exception handling in parallel algorithms. Specifies that
<code>std::terminate</code> is called when an exception escapes during
parallel execution under standard execution policies.</p></li>
<li><p><strong>[algorithms.parallel.exec]</strong> — ISO/IEC 14882,
Execution policies. Specifies requirements for element access functions
and parallel execution semantics.</p></li>
<li><p><strong>[numeric.ops.accumulate]</strong> — ISO/IEC 14882,
Accumulate. Specifies the left-fold semantics of
<code>std::accumulate</code>.</p></li>
<li><p><strong>[numeric.ops.reduce]</strong> — ISO/IEC 14882, Reduce.
Specifies the generalized sum semantics of <code>std::reduce</code>,
which permits reassociation for parallel execution.</p></li>
</ul>
<h3 id="wg21-papers">WG21 Papers</h3>
<ul>
<li><p><strong>[P1928]</strong> — Matthias Kretz. “std::simd — merge
data-parallel types from the Parallelism TS 2.” WG21 paper P1928.
Available at:
https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2023/p1928r8.pdf</p></li>
<li><p><strong>[P2300]</strong> — Michał Dominiak, Georgy Evtushenko,
Lewis Baker, Lucian Radu Teodorescu, Lee Howes, Kirk Shoop, Michael
Garland, Eric Niebler, Bryce Adelstein Lelbach.
“<code>std::execution</code>.” WG21 paper P2300R10, adopted for C++26 at
the St. Louis plenary, June 2024. Available at:
https://wg21.link/P2300R10</p></li>
<li><p><strong>[P3375R2]</strong> — Guy Davidson. “Reproducible
floating-point results.” WG21 paper P3375R2, 2025. Proposes a
<code>strict_float</code> type specifying sufficient conformance with
ISO/IEC 60559:2020 to guarantee reproducible floating-point arithmetic
across implementations. Available at:
https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3375r2.html</p></li>
</ul>
<h3 id="industry-references">Industry References</h3>
<ul>
<li><p><strong>[IntelCNR]</strong> — Intel Corporation. “Introduction to
Conditional Numerical Reproducibility (CNR).” <em>Intel oneAPI Math
Kernel Library Developer Guide</em>. Available at:
https://www.intel.com/content/www/us/en/docs/onemkl/developer-guide/current/conditional-numerical-reproducibility.html</p></li>
<li><p><strong>[NvidiaCUB]</strong> — NVIDIA Corporation. “CUB: CUDA
UnBound.” <em>NVIDIA CUB Documentation</em>. Describes deterministic
reduction variants with fixed-order tree reduction. Available at:
https://nvlabs.github.io/cub/</p></li>
<li><p><strong>[KokkosReduce]</strong> — Sandia National Laboratories.
“Custom Reductions: Determinism.” <em>Kokkos Documentation</em>.
Available at:
https://kokkos.github.io/kokkos-core-wiki/API/core/parallel-dispatch/parallel_reduce.html</p></li>
<li><p><strong>[IntelTBB]</strong> — Intel Corporation. “Specifying a
Partitioner.” <em>Intel oneAPI Threading Building Blocks Developer
Guide</em>. Available at:
https://www.intel.com/content/www/us/en/docs/onetbb/developer-guide-reference/current/partitioner.html</p></li>
</ul>
<h3 id="academic-references">Academic References</h3>
<ul>
<li><p><strong>[Dalton2014]</strong> — B. Dalton, E. Wang, and R.
Blainey. “SIMDizing pairwise sums: a summation algorithm balancing
accuracy with throughput.” <em>Proceedings of the Workshop on
Programming Models for SIMD/Vector Processing (WPMVP)</em>, 2014. DOI:
https://doi.org/10.1145/2568058.2568070</p></li>
<li><p><strong>[Demmel2013]</strong> — James Demmel and Hong Diep
Nguyen. “Fast Reproducible Floating-Point Summation.” <em>Proceedings of
the 21st IEEE Symposium on Computer Arithmetic (ARITH)</em>, April 2013,
pp. 163–172. DOI: https://doi.org/10.1109/ARITH.2013.9</p></li>
<li><p><strong>[Higham2002]</strong> — Nicholas J. Higham. <em>Accuracy
and Stability of Numerical Algorithms</em>, Second Edition. Society for
Industrial and Applied Mathematics (SIAM), 2002. ISBN:
978-0-89871-521-7. Chapter 4 discusses pairwise summation and error
analysis of floating-point summation algorithms.</p></li>
</ul>
<h2 id="appendix-a-illustrative-wording-informative">Appendix A:
Illustrative Wording (Informative)</h2>
<p>This appendix is illustrative; no API is proposed in this paper (§2).
It shows one way the semantic definition could be expressed for a future
Standard Library facility. Names, headers, and final API shape are
intentionally provisional.</p>
<h3 id="a.1-example-algorithm-specification">A.1 Example Algorithm
Specification</h3>
<p>The following shows how the semantic definition could be expressed
for a hypothetical algorithm that exposes both topology coordinates:</p>
<ul>
<li><strong>L (lane count):</strong> the semantic topology coordinate
(§4.3).</li>
<li><strong>M (interleave span, bytes):</strong> a derived numerics
convenience coordinate that maps to <code>L = M / sizeof(V)</code> when
well-formed.</li>
</ul>
<div class="sourceCode" id="cb31"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Lane-based topology (portable across ABIs for a fixed L)</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span><span class="op">&lt;</span><span class="dt">size_t</span> L<span class="op">,</span> <span class="kw">class</span> InputIterator<span class="op">,</span> <span class="kw">class</span> T<span class="op">,</span> <span class="kw">class</span> BinaryOperation<span class="op">&gt;</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="kw">constexpr</span> T canonical_reduce_lanes<span class="op">(</span>InputIterator first<span class="op">,</span> InputIterator last<span class="op">,</span> T init<span class="op">,</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>                  BinaryOperation binary_op<span class="op">);</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span><span class="op">&lt;</span><span class="dt">size_t</span> L<span class="op">,</span> <span class="kw">class</span> ExecutionPolicy<span class="op">,</span> <span class="kw">class</span> ForwardIterator<span class="op">,</span> <span class="kw">class</span> T<span class="op">,</span> <span class="kw">class</span> BinaryOperation<span class="op">&gt;</span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>T canonical_reduce_lanes<span class="op">(</span>ExecutionPolicy<span class="op">&amp;&amp;</span> policy<span class="op">,</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>            ForwardIterator first<span class="op">,</span> ForwardIterator last<span class="op">,</span> T init<span class="op">,</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>            BinaryOperation binary_op<span class="op">);</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="co">// Span-based topology (numerics convenience; derives L from sizeof(V))</span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span><span class="op">&lt;</span><span class="dt">size_t</span> M<span class="op">,</span> <span class="kw">class</span> InputIterator<span class="op">,</span> <span class="kw">class</span> T<span class="op">,</span> <span class="kw">class</span> BinaryOperation<span class="op">&gt;</span></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a><span class="kw">constexpr</span> T canonical_reduce<span class="op">(</span>InputIterator first<span class="op">,</span> InputIterator last<span class="op">,</span> T init<span class="op">,</span></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>              BinaryOperation binary_op<span class="op">);</span></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span><span class="op">&lt;</span><span class="dt">size_t</span> M<span class="op">,</span> <span class="kw">class</span> ExecutionPolicy<span class="op">,</span> <span class="kw">class</span> ForwardIterator<span class="op">,</span> <span class="kw">class</span> T<span class="op">,</span> <span class="kw">class</span> BinaryOperation<span class="op">&gt;</span></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>T canonical_reduce<span class="op">(</span>ExecutionPolicy<span class="op">&amp;&amp;</span> policy<span class="op">,</span></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>         ForwardIterator first<span class="op">,</span> ForwardIterator last<span class="op">,</span> T init<span class="op">,</span></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>         BinaryOperation binary_op<span class="op">);</span></span></code></pre></div>
<p><strong>Constraints:</strong> - For overloads with an
ExecutionPolicy, ForwardIterator meets the Cpp17ForwardIterator
requirements.</p>
<p><strong>Mandates (lane form):</strong> - <code>L &gt;= 1</code>.</p>
<p><strong>Mandates (span form):</strong> - Let V =
<code>iter_value_t&lt;Iter&gt;</code> for the relevant iterator type. -
<code>M &gt;= sizeof(V)</code>. - <code>M % sizeof(V) == 0</code>. - The
span form is equivalent to the lane form with
<code>L = M / sizeof(V)</code>.</p>
<h3 id="a.2-semantics-as-if">A.2 Semantics (as-if)</h3>
<p>For the span form, let <code>L = M / sizeof(V)</code>.</p>
<p>A conforming implementation returns a value that is as-if evaluation
of the canonical abstract expression defined in §4, which may be
constructed conceptually as follows:</p>
<ol type="1">
<li><strong>Materialize terms.</strong> Form the conceptual sequence of
terms <code>X[0..N)</code> by converting each input element to the
reduction state type <code>A</code> as specified in §4.6.</li>
<li><strong>Partition into lanes.</strong> Choose a lane count
<code>L</code> (directly, or via <code>M→L</code>), and partition terms
into <code>L</code> logical lanes by index modulo as specified in
§4.3.1.</li>
<li><strong>Form fixed-length lane leaves with absence.</strong> Let
<code>K = ceil(N / L)</code> (for <code>N &gt; 0</code>). For each lane
index <code>j</code> in <code>[0, L)</code>, form a fixed-length leaf
sequence <code>Y_j[0..K)</code> of <code>maybe&lt;A&gt;</code> where
<code>Y_j[t]</code> is <code>present(X[j + t*L])</code> when
<code>j + t*L &lt; N</code>, and <code>∅</code> otherwise (§4.3.2).</li>
</ol>
<p><em>Informative:</em> When <code>N &lt; L</code>, some lane indices
have no corresponding input positions; such lanes are best understood as
“no lane data exists”. In the canonical expression this is represented
by <code>Y_j[t] == ∅</code> for all <code>t</code>, yielding
<code>R_j == ∅</code>.</p>
<ol start="4" type="1">
<li><strong>Per-lane canonical reduction.</strong> For each lane index
<code>j</code>, compute
<code>R_j = CANONICAL_TREE_EVAL(binary_op, Y_j)</code> using the
canonical balanced tree shape and the lifted <code>COMBINE</code> rules
for <code>∅</code> (§4.2–§4.4.1).</li>
<li><strong>Cross-lane canonical reduction.</strong> Form
<code>Z[0..L)</code> where <code>Z[j] = R_j</code> and compute
<code>R_all = CANONICAL_TREE_EVAL(binary_op, Z)</code> (§4.4.2).</li>
<li><strong>Integrate <code>init</code> (if provided).</strong> If an
initial value is provided, the result is <code>init</code> when
<code>N == 0</code>, otherwise
<code>binary_op(init, value(R_all))</code>, as specified in §4.5.</li>
</ol>
<p>[<em>Note:</em> <code>init</code> is combined once with the total
result and is not treated as an additional input element; treating it as
an extra element would shift lane assignments and change the selected
canonical expression topology. <em>—end note</em>]</p>
<p><em>Informative:</em> An implementation may skip forming absent
leaves and lanes that contain no elements, provided the returned value
is as-if evaluation of the canonical expression (since <code>∅</code>
does not induce applications of <code>binary_op</code>).</p>
<h2 id="appendix-b-implementation-guidance-informative">Appendix B:
Implementation Guidance (Informative)</h2>
<p>This appendix provides detailed guidance for implementers. It is
informative only; the normative specification is the canonical compute
sequence defined in §4.</p>
<p><strong>General principle:</strong> Implementations need not
instantiate lanes for empty subsequences; only the logical reduction
order must be preserved. For example, if L = 1000 but N = 5, the
implementation need not allocate 1000 accumulators — only the 5
non-empty subsequences participate in the final reduction.</p>
<h3 id="b.1-two-degrees-of-parallelism">B.1 Two Degrees of
Parallelism</h3>
<p>Real implementations exploit two orthogonal forms of parallelism:</p>
<table>
<thead>
<tr class="header">
<th>Parallelism</th>
<th>Mechanism</th>
<th>Typical Scale</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>SIMD</td>
<td>Vector registers, GPU warps</td>
<td>4–64 lanes</td>
</tr>
<tr class="even">
<td>Threads</td>
<td>CPU cores, GPU blocks</td>
<td>4–thousands</td>
</tr>
</tbody>
</table>
<p>This leads to two levels of reduction:</p>
<pre><code>                 FINAL RESULT
                      |
          +-----------+-----------+
          |   THREAD REDUCTION    | &lt;-- Combine thread results
          +-----------+-----------+
                      |
      +---------------+---------------+
      |               |               |
+-----+-----+   +-----+-----+   +-----+-----+
|   SIMD    |   |   SIMD    |   |   SIMD    |
| REDUCTION |   | REDUCTION |   | REDUCTION |
+-----------+   +-----------+   +-----------+</code></pre>
<p>Both levels must produce results matching the canonical
expression.</p>
<h3 id="b.2-the-efficient-simd-pattern">B.2 The Efficient SIMD
Pattern</h3>
<p>The canonical expression supports efficient vertical addition:</p>
<p>For L = 4, N = 16:</p>
<pre><code>Load V0 = {E[0], E[1], E[2], E[3]} → Acc = V0
Load V1 = {E[4], E[5], E[6], E[7]} → Acc = Acc + V1
Load V2 = {E[8], E[9], E[10], E[11]} → Acc = Acc + V2
Load V3 = {E[12], E[13], E[14], E[15]} → Acc = Acc + V3

Acc now holds {R_0, R_1, R_2, R_3}
Final horizontal reduction: op(op(R_0, R_1), op(R_2, R_3))</code></pre>
<p>This is contiguous loads plus vertical adds — optimal for SIMD.</p>
<h3 id="b.3-thread-level-partitioning">B.3 Thread-Level
Partitioning</h3>
<p>With multiple threads, each processes a contiguous chunk:</p>
<pre><code>Thread 0: E[0..256) → {R_0^T0, R_1^T0, R_2^T0, R_3^T0}
Thread 1: E[256..512) → {R_0^T1, R_1^T1, R_2^T1, R_3^T1}
...

Thread partial results are combined per lane, then across lanes:
Global R_0 = CANONICAL_TREE_EVAL(op, {R_0^T0, R_0^T1, ...})
Global R_1 = CANONICAL_TREE_EVAL(op, {R_1^T0, R_1^T1, ...})
...
Final = CANONICAL_TREE_EVAL(op, {R_0, R_1, R_2, R_3})</code></pre>
<h3 id="b.4-gpu-implementation">B.4 GPU Implementation</h3>
<p>The sequence maps to GPU architectures:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Vertical addition within block</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">size_t</span> base <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> base <span class="op">&lt;</span> n<span class="op">;</span> base <span class="op">+=</span> L<span class="op">)</span> <span class="op">{</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">size_t</span> idx <span class="op">=</span> base <span class="op">+</span> lane<span class="op">;</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span>idx <span class="op">&lt;</span> n<span class="op">)</span> acc <span class="op">=</span> op<span class="op">(</span>acc<span class="op">,</span> input<span class="op">[</span>idx<span class="op">]);</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="co">// Horizontal reduction using canonical tree</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">size_t</span> stride <span class="op">=</span> L<span class="op">/</span><span class="dv">2</span><span class="op">;</span> stride <span class="op">&gt;</span> <span class="dv">0</span><span class="op">;</span> stride <span class="op">/=</span> <span class="dv">2</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span>lane <span class="op">&lt;</span> stride<span class="op">)</span> shared<span class="op">[</span>lane<span class="op">]</span> <span class="op">=</span> op<span class="op">(</span>shared<span class="op">[</span>lane<span class="op">],</span> shared<span class="op">[</span>lane <span class="op">+</span> stride<span class="op">]);</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>  __syncthreads<span class="op">();</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a><span class="co">// Cross-block: fixed slots, not atomics</span></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>block_results<span class="op">[</span>blockIdx<span class="op">.</span>x<span class="op">]</span> <span class="op">=</span> shared<span class="op">[</span><span class="dv">0</span><span class="op">];</span> <span class="co">// Deterministic position</span></span></code></pre></div>
<p><strong>Key constraints:</strong> - Static work assignment (no work
stealing) - Fixed block result slots (no atomicAdd) - Canonical tree for
horizontal reduction</p>
<p>[<em>Note:</em> This GPU sketch is illustrative and non-normative; it
demonstrates one possible mapping of the canonical expression to a GPU
kernel and is not intended to constrain implementation strategy.
<em>—end note</em>]</p>
<h3 id="b.5-cross-platform-consistency">B.5 Cross-Platform
Consistency</h3>
<p>The proposal defines the expression structure. All conforming
implementations evaluate as-if by the same canonical parenthesization
and operand ordering for a given topology coordinate <code>L</code>.</p>
<p><strong>Within a single, controlled environment</strong> (same
ISA/backend, same compiler and flags, and equivalent floating-point
evaluation models), this removes the run-to-run variability of
<code>std::reduce</code> by fixing the reduction tree.</p>
<p><strong>Across architectures/backends</strong> (CPU ↔︎ CPU, CPU ↔︎
GPU): the facility guarantees expression/topology equivalence — i.e.,
the same abstract tree is specified. Achieving bitwise identity
additionally requires aligning the floating-point evaluation environment
(§2.5, §6).</p>
<h4 id="b.5.1-requirements-for-cross-platform-bitwise-reproducibility">B.5.1
Requirements for Cross-Platform Bitwise Reproducibility</h4>
<p>This proposal standardizes the abstract expression structure
(parenthesization and operand order). Bitwise-identical results across
different platforms, compilers, or architectures generally require that
fixed expression structure and an equivalent floating-point evaluation
environment.</p>
<p>[<em>Note:</em> For fundamental floating-point types with
non-associative operations (e.g., float, double with
<code>std::plus</code>), factors that commonly affect bitwise results
include (non-exhaustive):</p>
<ol type="1">
<li><strong>Floating-point format and evaluation rules:</strong> whether
the type and operations follow ISO/IEC/IEEE 60559 (IEEE 754) and whether
intermediate precision differs (e.g., extended precision).</li>
<li><strong>Rounding mode:</strong> both executions use the same
rounding mode (typically round-to-nearest ties-to-even).</li>
<li><strong>Contraction / FMA:</strong> whether multiply-add sequences
are fused/contracted, and whether contraction behavior matches across
backends.</li>
<li><strong>Subnormal handling:</strong> whether subnormals are
preserved or flushed to zero (FTZ/DAZ).</li>
<li><strong>Transforming optimizations:</strong> “fast-math” style
options that permit reassociation or relax IEEE behavior.</li>
<li><strong>Library math and user operations:</strong> if
<code>binary_op</code> (or upstream transformations such as
<code>views::transform</code>) call implementation-defined math
libraries, results may differ.</li>
<li><strong>Topology selection and input order:</strong> both sides use
identical topology coordinate (<code>L</code>, or <code>M→L</code>) and
the same input order.</li>
</ol>
<p><em>—end note</em>]</p>
<p><strong>What this proposal guarantees:</strong> for fixed topology
coordinate, input order, and <code>binary_op</code>, the abstract
expression (parenthesization and operand order) is identical across all
conforming implementations.</p>
<p><strong>What remains platform-specific:</strong> the floating-point
evaluation model (items above). Aligning it may require toolchain
controls (for example, disabling contraction and avoiding fast-math;
exact spellings are toolchain-specific).</p>
<p><strong>Verification workflow:</strong> the demonstrators in Appendix
K use a fixed seed and publish expected hex outputs for representative
topology coordinates. Matching those values across platforms validates
both (a) correct expression structure (this proposal) and (b)
sufficiently aligned floating-point environments (user
responsibility).</p>
<p><strong>Illustrative CPU ↔︎ GPU check:</strong></p>
<div class="sourceCode" id="cb36"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Compile and run both sides under equivalent FP settings (toolchain-specific).</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="dt">double</span> cpu <span class="op">=</span> canonical_reduce_lanes<span class="op">&lt;</span><span class="dv">16</span><span class="op">&gt;(</span>data<span class="op">.</span>begin<span class="op">(),</span> data<span class="op">.</span>end<span class="op">(),</span> <span class="fl">0.0</span><span class="op">,</span> <span class="bu">std::</span>plus<span class="op">&lt;&gt;{});</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="dt">double</span> gpu <span class="op">=</span> cuda_canonical_reduce_lanes<span class="op">&lt;</span><span class="dv">16</span><span class="op">&gt;(</span>d_data<span class="op">,</span> N<span class="op">,</span> <span class="fl">0.0</span><span class="op">);</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="co">// Bitwise equality is achievable when the FP evaluation environments are aligned:</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="ot">assert</span><span class="op">(</span><span class="bu">std::</span>bit_cast<span class="op">&lt;</span><span class="dt">uint64_t</span><span class="op">&gt;(</span>cpu<span class="op">)</span> <span class="op">==</span> <span class="bu">std::</span>bit_cast<span class="op">&lt;</span><span class="dt">uint64_t</span><span class="op">&gt;(</span>gpu<span class="op">));</span></span></code></pre></div>
<p>This enables “golden result” workflows where a reference evaluation
can act as a baseline for accelerator correctness verification.</p>
<h3 id="b.6-when-physical-width-logical-width">B.6 When Physical Width ≠
Logical Width</h3>
<p><strong>Physical &gt; Logical</strong> (e.g., 256 GPU threads, L =
8): Multiple threads cooperate on each logical lane. Partial results
combine using the canonical tree.</p>
<p><strong>Physical &lt; Logical</strong> (e.g., 4 physical SIMD lanes,
L = 16 logical lanes): Process logical lanes in chunks across multiple
physical iterations. The logical sequence is unchanged; only the
physical execution differs.</p>
<h3 id="b.7-performance-characteristics">B.7 Performance
Characteristics</h3>
<p>Representative measurements appear in Appendix N.12. With proper SIMD
optimization (8-block unrolling), the reference implementation indicates
that the canonical expression structure can be evaluated at throughput
comparable to unconstrained reduction, suggesting that conforming
implementations need not incur prohibitive overhead. Observed overhead
is workload- and configuration-dependent; see also the demonstrators in
Appendix K for platform observations.</p>
<h2 id="appendix-c-prior-art-and-industry-practice-informative">Appendix
C: Prior Art and Industry Practice (Informative)</h2>
<p>The tension between parallel performance and numerical
reproducibility is a well-documented challenge in high-performance
computing (HPC) and distributed systems. This appendix summarizes how
existing frameworks address the “Grouping Gap.”</p>
<h3 id="c.1-kokkos-ecosystem-hpc-portability">C.1 Kokkos Ecosystem (HPC
Portability)</h3>
<p>Kokkos is a widely used C++ programming model for exascale computing.
Its documentation explicitly warns users about the lack of run-to-run
stability (for FP) in reductions:</p>
<blockquote>
<p>“The result of a parallel_reduce is not guaranteed to be bitwise
identical across different runs on the same hardware, nor across
different numbers of threads, unless the joiner operation is associative
and commutative.” [KokkosReduce]</p>
</blockquote>
<p>In practice, HPC users requiring reproducibility in Kokkos must often
implement “two-pass” reductions or use fixed-point arithmetic, which
imposes a significant developer burden. The interleaved topology
proposed in §4.3 of this paper mirrors the “vector-lane” optimization
patterns used in Kokkos’s backend, but elevates it to a deterministic
semantic contract.</p>
<h3 id="c.2-intel-oneapi-threading-building-blocks-onetbb">C.2 Intel
oneAPI Threading Building Blocks (oneTBB)</h3>
<p>Intel TBB is the industry standard for task-parallelism on CPUs. Its
<code>parallel_reduce</code> implementation uses a recursive splitting
strategy that relies on a “Join” pattern:</p>
<ol type="1">
<li><strong>Splitting:</strong> The range is split into sub-ranges until
they reach a “grain size.”</li>
<li><strong>Reduction:</strong> Each sub-range is reduced locally.</li>
<li><strong>Joining:</strong> Sub-results are combined using the
user-provided <code>join()</code> method.</li>
</ol>
<p>Because TBB uses a work-stealing scheduler, the order in which these
<code>join()</code> operations occur is non-deterministic — it depends
on which thread becomes free first. While TBB offers a
<code>static_partitioner</code> to minimize this, it does not guarantee
a canonical tree topology, making bitwise identity fragile across
different thread counts [IntelTBB].</p>
<h3 id="c.3-nvidia-thrust-and-cub">C.3 NVIDIA Thrust and CUB</h3>
<p>For GPU architectures, NVIDIA provides the Thrust and CUB
libraries.</p>
<p><strong>Thrust:</strong> Generally favors throughput and uses atomic
operations in many reduction paths. These atomics are processed in an
order determined by hardware thread scheduling, leading to
non-deterministic floating-point results.</p>
<p><strong>CUB:</strong> Provides more granular control through
“Block-level” and “Warp-level” primitives. The interleaved subsequences
(S_j) defined in §4.3 are mathematically equivalent to the
“blocked-arrangement” and “striped-arrangement” memory patterns used in
CUB to achieve peak bandwidth on SIMD-intensive hardware
[NvidiaCUB].</p>
<h3 id="c.4-academic-and-research-solutions">C.4 Academic and Research
Solutions</h3>
<p>Significant research has been conducted into “Reproducible Summation”
(e.g., [Demmel2013]). These solutions typically fall into two
categories:</p>
<ol type="1">
<li><strong>Exact Summation:</strong> Using very wide fixed-precision
accumulators (e.g., 400+ bits) to ensure associativity. These are
bit-accurate but carry a 2x–10x performance penalty.</li>
<li><strong>Fixed-Topology Summation:</strong> Enforcing a specific
reduction tree.</li>
</ol>
<p>This proposal follows the Fixed-Topology school of thought. It
recognizes that while we cannot easily standardize “Exact Summation” due
to its cost, we can standardize the topology, which provides
reproducibility for a given platform at a much lower performance cost
(10–15% overhead relative to <code>std::reduce</code> in representative
configurations; see Appendix H).</p>
<p>See References for full citations of [KokkosReduce], [IntelTBB],
[NvidiaCUB], and [Demmel2013].</p>
<h2 id="appendix-d-standard-wording-for-sequential-evaluation-order-informative">Appendix
D: Standard Wording for Sequential Evaluation Order (Informative)</h2>
<p>In the C++ Standard, sequential algorithms like
<code>std::accumulate</code> and <code>std::ranges::fold_left</code>
have a mandated evaluation order — the grouping of operations is fully
specified as a left-fold.</p>
<p><strong>Important distinction:</strong> The Standard specifies
evaluation order, not bitwise reproducibility. Even with identical
evaluation order, results may differ across compilers or platforms due
to floating-point evaluation model factors (see §D.5). However, a fixed
evaluation order is a necessary precondition for reproducibility —
without it, reproducibility is impossible even under identical
floating-point evaluation models.</p>
<h3 id="d.1-stdaccumulate">D.1 std::accumulate</h3>
<p>The evaluation order guarantee for <code>std::accumulate</code> is
found in the Numerics library section of the Standard.</p>
<p><strong>Section:</strong> [numeric.ops.accumulate]/2 (In N4950/C++23:
§27.10.3)</p>
<p><strong>The Text:</strong> The Standard defines the behavior of
<code>accumulate(first, last, init, binary_op)</code> as:</p>
<blockquote>
<p>“Computes its result by initializing the accumulator acc with the
initial value init and then modifies it with
<code>acc = std::move(acc) + *i</code> or
<code>acc = binary_op(std::move(acc), *i)</code> for every iterator i in
the range [first, last) in order.”</p>
</blockquote>
<p><strong>What this guarantees:</strong> The expression structure is
fixed as <code>(((init + a) + b) + c)...</code>. The grouping is fully
specified — there is no implementation freedom in how operations are
combined.</p>
<p><strong>What this does not guarantee:</strong> Bitwise identical
results across compilers or platforms. The same grouping may produce
different bits due to floating-point evaluation model differences.</p>
<p><strong>Contrast with std::reduce:</strong> <code>std::reduce</code>
uses a generalized sum ([numerics.defns]/1–2, [numeric.ops.reduce]/7),
allowing the implementation to group elements as
<code>((a + b) + (c + d))</code> or any other valid tree. This makes
even the grouping non-deterministic.</p>
<h3 id="d.2-stdrangesfold_left-c23">D.2 std::ranges::fold_left
(C++23)</h3>
<p>For the newer Ranges-based algorithms, the specification is even more
explicit about its algebraic structure.</p>
<p><strong>Section:</strong> [alg.fold] (In N4950/C++23: §27.6.18)</p>
<p><strong>The Text:</strong></p>
<blockquote>
<p>“The range fold algorithms are sequential operations that perform a
left-fold… <code>ranges::fold_left(R, init, f)</code> is equivalent to:
<code>cpp auto acc = init; for (auto&amp;&amp; e : R) acc = f(std::move(acc), e); return acc;</code>”</p>
</blockquote>
<p><strong>What this guarantees:</strong> By defining the algorithm via
an explicit for loop, the Standard fully specifies the evaluation order.
The state of the accumulator at step N depends exactly and only on the
state at step N-1 and the Nth element.</p>
<h3 id="d.3-sequential-vs.-parallel-contrasting-guarantees">D.3
Sequential vs. Parallel: Contrasting Guarantees</h3>
<table>
<colgroup>
<col style="width: 15%" />
<col style="width: 54%" />
<col style="width: 29%" />
</colgroup>
<thead>
<tr class="header">
<th>Property</th>
<th>Sequential (accumulate/fold_left)</th>
<th>Parallel (reduce)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Standard Section</td>
<td>[numeric.ops.accumulate] / [alg.fold]</td>
<td>[numeric.ops.reduce]</td>
</tr>
<tr class="even">
<td>Grouping</td>
<td>Mandated: Left-to-right</td>
<td>Generalized Sum (Unspecified)</td>
</tr>
<tr class="odd">
<td>Complexity</td>
<td>O(N) operations</td>
<td>O(N) operations</td>
</tr>
<tr class="even">
<td>Evaluation Order</td>
<td>Fully specified</td>
<td>Not specified</td>
</tr>
</tbody>
</table>
<p><strong>Key insight:</strong> <code>std::accumulate</code> and
<code>std::reduce</code> differ in whether the grouping is specified,
not in whether they guarantee “bitwise reproducibility” (neither does,
strictly speaking).</p>
<h3 id="d.4-init-placement-comparison-with-stdaccumulate">D.4 Init
Placement: Comparison with std::accumulate</h3>
<p>This proposal specifies init placement as <code>op(I, R)</code>
(where <code>I</code> is the initial value materialized as type
<code>A</code> per §4.6) — the initial value is combined once at the end
with the tree result. The table below compares this to
<code>std::accumulate</code>:</p>
<table>
<thead>
<tr class="header">
<th>Aspect</th>
<th>std::accumulate</th>
<th>This Proposal (<code>op(I, R)</code>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Init handling</td>
<td>Folded at every step</td>
<td>Combined once at end</td>
</tr>
<tr class="even">
<td>Structure</td>
<td><code>(((init ⊕ a) ⊕ b) ⊕ c)</code></td>
<td><code>init ⊕ tree_reduce(a,b,c,d)</code></td>
</tr>
<tr class="odd">
<td>Init participates in</td>
<td>N operations</td>
<td>1 operation</td>
</tr>
</tbody>
</table>
<p><strong>Implication for non-associative operations:</strong> For
associative operations, these approaches produce equivalent results. For
non-associative operations (e.g., floating-point addition), the results
may differ. Users migrating from <code>std::accumulate</code> should be
aware of this distinction.</p>
<p>See Appendix E for rationale behind the <code>op(I, R)</code> design
choice.</p>
<h3 id="d.5-important-caveat-evaluation-order-bitwise-identity">D.5
Important Caveat: Evaluation Order ≠ Bitwise Identity</h3>
<p>The Standard specifies evaluation order, not bitwise results. Even
when the grouping of operations is fully specified (as in
<code>std::accumulate</code>), bitwise identity across different
compilers, platforms, or even different runs is not guaranteed due
to:</p>
<ol type="1">
<li><strong>Intermediate precision:</strong> The Standard permits
implementations to use higher precision for intermediate results (e.g.,
x87 80-bit registers vs SSE 64-bit). See [cfenv.syn] and
implementation-defined floating-point behavior.</li>
<li><strong>FMA Contraction:</strong> A compiler might contract
<code>a * b + c</code> into a single fused multiply-add instruction on
one platform but not another, changing the result bits. This is
controlled by <code>#pragma STDC FP_CONTRACT</code> and compiler flags
like <code>-ffp-contract</code>.</li>
<li><strong>Rounding mode:</strong> Different default rounding modes
across implementations can affect results.</li>
</ol>
<p><strong>What this proposal provides:</strong> A fully specified
expression structure (grouping and operand order). Combined with user
control of the floating-point evaluation model, this enables
reproducibility.</p>
<p><strong>What this proposal does not provide:</strong> Automatic
cross-platform bitwise identity. Users must also control their
floating-point evaluation model (see §6).</p>
<h3 id="d.6-why-compilers-cannot-reassociate-mandated-evaluation-order">D.6
Why Compilers Cannot Reassociate Mandated Evaluation Order</h3>
<p>A potential concern is whether compilers might reassociate the
reduction operations defined by this proposal, defeating the run-to-run
stability guarantee. This section explains why such reassociation would
be non-conforming.</p>
<h4 id="d.6.1-the-as-if-rule">D.6.1 The As-If Rule</h4>
<p>The C++ Standard permits compilers to perform any transformation that
does not change the observable behavior of a conforming program
([intro.abstract]/1). This is commonly called the “as-if rule.”</p>
<p>For floating-point arithmetic, reassociation (changing
<code>(a + b) + c</code> to <code>a + (b + c)</code>) generally changes
the result due to rounding. Therefore, a compiler cannot reassociate
floating-point operations under the as-if rule unless:</p>
<ol type="1">
<li>It can prove the result is unchanged (generally impossible for FP),
or</li>
<li>The user has explicitly opted into non-standard semantics via
compiler flags</li>
</ol>
<h4 id="d.6.2-existing-precedent-stdaccumulate">D.6.2 Existing
Precedent: std::accumulate</h4>
<p><code>std::accumulate</code> mandates a specific evaluation order
([numeric.ops.accumulate]/2):</p>
<blockquote>
<p>“Computes its result by initializing the accumulator acc with the
initial value init and then modifies it with
<code>acc = std::move(acc) + *i</code> or
<code>acc = binary_op(std::move(acc), *i)</code> for every iterator i in
the range [first, last) in order.”</p>
</blockquote>
<p>This wording constrains implementations to a left-fold:
<code>(((init ⊕ a) ⊕ b) ⊕ c) ...</code></p>
<p>Compilers respect this constraint. A compiler that reassociated
<code>std::accumulate</code> into a tree reduction would be
non-conforming, because the observable result would differ for
non-associative operations (including floating-point addition).</p>
<h4 id="d.6.3-this-proposal-follows-the-same-model">D.6.3 This Proposal
Follows the Same Model</h4>
<p>This proposal mandates a specific expression structure with the same
normative force. The “Generalized Sum” semantics of
<code>std::reduce</code> explicitly grant permission to reassociate;
this proposal removes that permission — exactly as
<code>std::accumulate</code> mandates a specific left-fold
expression.</p>
<p>[<em>Note:</em> For <code>std::accumulate</code>, “expression
structure” and “evaluation order” coincide because the left-fold is
inherently sequential. For this proposal, the expression structure
(parenthesization) is fixed, but independent subexpressions may be
evaluated concurrently. <em>—end note</em>]</p>
<p>The only difference from <code>std::accumulate</code> is the shape of
the mandated expression:</p>
<table>
<thead>
<tr class="header">
<th>Algorithm</th>
<th>Mandated Expression Shape</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>std::accumulate</td>
<td><code>(((init ⊕ a) ⊕ b) ⊕ c)</code> Linear fold</td>
</tr>
<tr class="even">
<td>This proposal</td>
<td><code>init ⊕ ((a ⊕ b) ⊕ (c ⊕ d))</code> Balanced binary tree</td>
</tr>
</tbody>
</table>
<p>Both specifications constrain the parenthesization. Both are subject
to the same as-if rule. A compiler that reassociates one would equally
violate conformance by reassociating the other.</p>
<h4 id="d.6.4-what-about--ffast-math">D.6.4 What About -ffast-math?</h4>
<p>Compiler flags like <code>-ffast-math</code>,
<code>-fassociative-math</code>, or <code>/fp:fast</code> explicitly opt
out of IEEE 754 compliance. Under these flags:</p>
<ul>
<li>The compiler may reassociate floating-point operations</li>
<li><code>std::accumulate</code> may not produce left-fold results</li>
<li><code>std::reduce</code> may not match any particular grouping</li>
<li>This proposal’s guarantee would also not apply</li>
</ul>
<p>This is not a defect in the proposal — it is the documented behavior
of these flags. Users who enable <code>-ffast-math</code> have
explicitly traded determinism for performance. The same trade-off
applies to all floating-point code, not just reductions.</p>
<p><strong>Recommendation for users requiring run-to-run
stability:</strong> Compile with <code>-ffp-contract=off</code> (or
equivalent) and avoid <code>-ffast-math</code>. This applies equally to
<code>std::accumulate</code> and to this proposal.</p>
<h4 id="d.6.5-summary">D.6.5 Summary</h4>
<table>
<colgroup>
<col style="width: 45%" />
<col style="width: 55%" />
</colgroup>
<thead>
<tr class="header">
<th>Concern</th>
<th>Resolution</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>“Compilers might reassociate the tree”</td>
<td>Violates as-if rule, same as for std::accumulate</td>
</tr>
<tr class="even">
<td>“What about -ffast-math?”</td>
<td>User has opted out of IEEE 754; all FP guarantees void</td>
</tr>
<tr class="odd">
<td>“Is this proposal different from existing algorithms?”</td>
<td>No — same conformance model as std::accumulate</td>
</tr>
</tbody>
</table>
<p>The expression-structure guarantee in this proposal has the same
normative force (with respect to parenthesization and operand order) as
the mandated left-fold structure in <code>std::accumulate</code>.
Compilers that respect the latter will respect the former.</p>
<h2 id="appendix-e-init-placement-rationale-informative">Appendix E:
Init Placement Rationale (Informative)</h2>
<p>This appendix provides rationale for the init placement design
choice. This proposal specifies <strong>Option A: op(I, R)</strong> —
the initial value (materialized as a value <code>I</code> of type
<code>A</code> per §4.6) is applied as the left operand to the result of
the canonical tree reduction. This appendix explains why this option was
chosen over alternatives.</p>
<p>[<em>Note:</em> <code>init</code> is combined once with the total
result and is not treated as an additional input element; treating it as
an extra element would shift lane assignments and change the selected
canonical expression topology. <em>—end note</em>]</p>
<h3 id="e.1-design-options-considered">E.1 Design Options
Considered</h3>
<p><strong>Option A: Post-reduction (<code>op(I, R)</code>) —
CHOSEN</strong></p>
<pre><code>canonical_reduce&lt;M&gt;(E[0..N), init, op):
  if N == 0:
    return init
  let R = interleaved_reduce&lt;M&gt;(E[0..N))
  let I = A(init)  // materialize init as the reduction state type
  return op(I, R)</code></pre>
<p><strong>Pros:</strong> - init is not part of the canonical reduction
tree — clear separation of concerns - Simplifies parallel implementation
(tree can complete before init is available) - Matches
<code>std::reduce</code>’s treatment of init ([numeric.ops.reduce]) -
Empty range handling is trivial: return init</p>
<p><strong>Cons:</strong> - Differs from <code>std::accumulate</code>’s
left-fold semantics - For non-associative operations, results differ
from left-fold expectations</p>
<p><strong>Option B: Post-reduction (op(R, I))</strong></p>
<p>Same as Option A but with init as the right operand.</p>
<p><strong>Pros:</strong> - Same implementation simplicity as Option
A</p>
<p><strong>Cons:</strong> - Less intuitive for users expecting init to
be “first” - Still differs from <code>std::accumulate</code></p>
<p><strong>Option C: Treat init as element 0 (prepend to
sequence)</strong></p>
<pre><code>canonical_reduce&lt;M&gt;(E[0..N), init, op):
  let E&#39; = {init, E[0], E[1], ..., E[N-1]}
  return interleaved_reduce&lt;M&gt;(E&#39;[0..N+1))</code></pre>
<p><strong>Pros:</strong> - init participates in the canonical tree with
a known position - More predictable for non-associative operations</p>
<p><strong>Cons:</strong> - Shifts all element indices by 1 - init
assigned to lane 0, changing topology when L &gt; 1 - Complicates the
interleaving definition</p>
<p><strong>Option D: Leave implementation-defined</strong></p>
<p>The standard specifies that init participates in exactly one
<code>binary_op</code> application with the tree result, but does not
specify the order.</p>
<p><strong>Pros:</strong> - Maximum implementation flexibility - Avoids
contentious design decision - For associative operations (the common
case), result is unaffected</p>
<p><strong>Cons:</strong> - Non-deterministic for non-associative
operations - Users cannot rely on specific init behavior</p>
<h3 id="e.2-analysis">E.2 Analysis</h3>
<p>For <strong>associative operations</strong> (the vast majority of use
cases), all options produce equivalent results. The choice only matters
for non-associative operations.</p>
<p>For non-associative operations, users already face the fact that the
tree reduction differs from left-fold. The init placement is one
additional degree of freedom that must be specified for full run-to-run
stability.</p>
<h3 id="e.3-why-option-a-was-chosen">E.3 Why Option A Was Chosen</h3>
<p>This proposal specifies Option A (<code>op(I, R)</code>) for the
following reasons:</p>
<ol type="1">
<li><strong>SIMD purity:</strong> Folding init into lanes would require
broadcast and can amplify init influence in non-associative
operations.</li>
<li><strong>Task independence:</strong> A “pure” tree over the range can
be computed before init is available in async/distributed contexts.</li>
<li><strong>Algebraic clarity:</strong> A balanced tree has no
distinguished “first” element; init is most naturally a post-reduction
adjustment.</li>
<li><strong>Precedent:</strong> This matches <code>std::reduce</code>’s
treatment of init ([numeric.ops.reduce]) (as part of the generalized
sum, not folded sequentially).</li>
<li><strong>Full determinism:</strong> Specifying the placement ensures
that the complete evaluation order — including init — is fully
determined, consistent with the paper’s run-to-run stability
guarantee.</li>
</ol>
<h3 id="e.4-why-not-treat-init-as-element-0">E.4 Why Not Treat init as
Element 0?</h3>
<p>Treating init as “element 0” (Option C) introduces complications:</p>
<ul>
<li>With L &gt; 1, init would be assigned to lane 0, but all other
element indices would shift</li>
<li>The meaning of init would depend on lane topology in ways that are
harder to explain and reason about</li>
<li>It conflates two distinct roles: “initial state” vs “operand in the
tree”</li>
</ul>
<p>The post-reduction design keeps these roles separate.</p>
<h3 id="e.5-migration-path-for-users-expecting-accumulate-style-semantics">E.5
Migration Path for Users Expecting accumulate-style Semantics</h3>
<p>Users who require init to participate as a leaf within the tree
(rather than post-reduction) can achieve this through composition:</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co">// To get init as element 0 in the tree:</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> extended <span class="op">=</span> concat_view<span class="op">(</span>single_view<span class="op">(</span>init<span class="op">),</span> data<span class="op">);</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> result <span class="op">=</span> canonical_reduce<span class="op">&lt;</span>M<span class="op">&gt;(</span>extended<span class="op">.</span>begin<span class="op">(),</span> extended<span class="op">.</span>end<span class="op">(),</span></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>                 identity_element<span class="op">,</span> op<span class="op">);</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a><span class="co">// Or manually:</span></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> tree_result <span class="op">=</span> canonical_reduce<span class="op">&lt;</span>M<span class="op">&gt;(</span>data<span class="op">.</span>begin<span class="op">(),</span> data<span class="op">.</span>end<span class="op">(),</span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>                    identity_element<span class="op">,</span> op<span class="op">);</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> result <span class="op">=</span> op<span class="op">(</span>init<span class="op">,</span> tree_result<span class="op">);</span> <span class="co">// explicit post-reduction (matches this proposal)</span></span></code></pre></div>
<p>This flexibility allows users to achieve alternative semantics when
needed while the standard provides a single, well-defined default.</p>
<h2 id="appendix-f-design-evolution-informative">Appendix F: Design
Evolution (Informative)</h2>
<p>This proposal underwent significant internal development before
committee submission. The design space was explored systematically, with
key decisions documented in §3 (Design Space) and the appendices. This
section summarizes the major evolution points for reviewers interested
in the design rationale.</p>
<h3 id="f.1-core-design-decisions">F.1 Core Design Decisions</h3>
<table>
<colgroup>
<col style="width: 16%" />
<col style="width: 38%" />
<col style="width: 27%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr class="header">
<th>Decision</th>
<th>Alternatives Considered</th>
<th>Chosen Approach</th>
<th>Rationale</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Topology</td>
<td>Left-fold, blocked, N-ary tree</td>
<td>Interleaved balanced binary</td>
<td>O(log N) depth, SIMD-friendly</td>
</tr>
<tr class="even">
<td>Parameterization</td>
<td>Fixed constant, lane count, implementation-defined</td>
<td>User-specified M (bytes)</td>
<td>Type-independent, future-proof</td>
</tr>
<tr class="odd">
<td>Init placement</td>
<td>As leaf, implementation-defined</td>
<td>Post-reduction <code>op(I, R)</code></td>
<td>Algebraic clarity, matches std::reduce ([numeric.ops.reduce])</td>
</tr>
<tr class="even">
<td>Split rule</td>
<td>⌈k/2⌉, variable</td>
<td>⌊k/2⌋ (normative)</td>
<td>Unique grouping specification</td>
</tr>
</tbody>
</table>
<h3 id="f.2-key-design-iterations">F.2 Key Design Iterations</h3>
<p><strong>Why bytes, not lanes:</strong> Initial designs parameterized
by lane count L. <code>L</code> is the semantic topology coordinate;
<code>M</code> is a convenience spelling for numeric domains.
<code>M</code> is convenient for layout-stable arithmetic types because
it scales across element sizes (e.g., <code>float</code> vs
<code>double</code>), but it does not change the semantic definition of
the canonical expression.</p>
<p><strong>Why interleaved, not blocked:</strong> Blocked decomposition
creates topology that varies with thread count and alignment.
Interleaved assignment (index mod L) produces stable topology regardless
of execution strategy. See §3.2.</p>
<p><strong>Why user-specified M:</strong> Fixed M ages poorly as
hardware evolves; implementation-defined M defeats determinism. User
specification places control where it belongs. See §3.3.</p>
<p><strong>Init placement:</strong> Treating init as “element 0” would
shift all indices and change lane assignment. Post-reduction application
keeps the tree “pure” and matches <code>std::reduce</code> semantics
([numeric.ops.reduce]). See §4.5 and Appendix E.</p>
<h3 id="f.3-industry-context">F.3 Industry Context</h3>
<p>The core design (fixed topology, user-controlled width) draws on
approaches seen in production libraries: - Intel oneMKL CNR (Conditional
Numerical Reproducibility) - NVIDIA CUB deterministic overloads -
PyTorch/TensorFlow deterministic modes</p>
<p>These libraries address similar problems through different
mechanisms. Their existence suggests the design space is viable and
addresses real needs. See §3.6 for references.</p>
<h2 id="appendix-g-detailed-design-rationale-informative">Appendix G:
Detailed Design Rationale (Informative)</h2>
<p>This appendix contains detailed rationale for design decisions that
were summarized in Section 4. It is provided for reviewers seeking
deeper understanding of the trade-offs.</p>
<h3 id="g.1-why-the-convenience-spelling-uses-bytes">G.1 Why the
Convenience Spelling Uses Bytes</h3>
<p>M is expressed in bytes rather than lanes for type independence: lane
count is derived from <code>sizeof(V)</code>, so a single M works across
element types.</p>
<table>
<thead>
<tr class="header">
<th>M (bytes)</th>
<th>double (8B)</th>
<th>float (4B)</th>
<th>int32_t (4B)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>16</td>
<td>2 lanes</td>
<td>4 lanes</td>
<td>4 lanes</td>
</tr>
<tr class="even">
<td>32</td>
<td>4 lanes</td>
<td>8 lanes</td>
<td>8 lanes</td>
</tr>
<tr class="odd">
<td>64</td>
<td>8 lanes</td>
<td>16 lanes</td>
<td>16 lanes</td>
</tr>
</tbody>
</table>
<p>M corresponds directly to SIMD register width:</p>
<table>
<thead>
<tr class="header">
<th>Target</th>
<th>Register Width</th>
<th>M</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>SSE / NEON</td>
<td>16 bytes</td>
<td>16</td>
</tr>
<tr class="even">
<td>AVX / AVX2</td>
<td>32 bytes</td>
<td>32</td>
</tr>
<tr class="odd">
<td>AVX-512</td>
<td>64 bytes</td>
<td>64</td>
</tr>
<tr class="even">
<td>Future (1024-bit)</td>
<td>128 bytes</td>
<td>128</td>
</tr>
</tbody>
</table>
<p>Implementations with narrower physical registers execute the
canonical expression through multiple iterations. The logical topology —
which operations combine with which — is unchanged.</p>
<h3 id="g.2-why-interleaved-topology-supports-efficient-simd">G.2 Why
Interleaved Topology Supports Efficient SIMD</h3>
<p>The interleaved topology supports the simplest and most efficient
SIMD implementation pattern:</p>
<pre><code>Memory: E[0] E[1] E[2] E[3] | E[4] E[5] E[6] E[7] | E[8] ...
    └─── Vector 0 ────┘  └─── Vector 1 ────┘

Iteration 1: Load V0 = {E[0], E[1], E[2], E[3]} → Acc = V0
Iteration 2: Load V1 = {E[4], E[5], E[6], E[7]} → Acc = Acc + V1
...
Final: Acc = {R_0, R_1, R_2, R_3}</code></pre>
<p>This pattern achieves: - One contiguous vector load per iteration
(optimal memory access) - One vector add per iteration (single
instruction) - Streaming sequential access (spatially local) - No
shuffles or gathers until final horizontal reduction</p>
<p>A blocked topology would require gather operations or sequential
per-lane processing, losing the SIMD benefit.</p>
<h3 id="g.3-information-density-and-spatial-locality">G.3 Information
Density and Spatial Locality</h3>
<p>The divisibility constraint and formula
<code>L = M / sizeof(V)</code> act as a self-regulating mechanism for
memory efficiency. For M = 64 and double (8 bytes), L = 8, maintaining
exactly 64 bytes of independent partial-accumulator state per logical
stride.</p>
<p>For large types where <code>sizeof(V) &gt; M</code>, the user
specifies <code>M = sizeof(V)</code>, giving L = 1. This degenerates to
a single-lane balanced binary tree with perfect spatial locality.</p>
<h3 id="g.4-cross-architecture-expression-parity">G.4 Cross-Architecture
Expression-Parity</h3>
<p>GPU architectures achieve peak efficiency when reduction trees align
with warp width (typically 32 threads). For double (8 bytes), a
warp-level reduction operates on 32 × 8 = 256 bytes.</p>
<p>By specifying L=32 (equivalently M=256 for double), users define a
canonical expression that maps to warp-level operations on GPU while CPU
evaluates the same mathematical expression by iterating over narrower
registers.</p>
<p><strong>What expression-parity guarantees:</strong> All platforms
evaluate the same mathematical expression — same parenthesization, same
operand ordering, same reduction tree topology.</p>
<p><strong>What it does not guarantee:</strong> Bitwise reproducibility
requires equivalent floating-point semantics across architectures, which
is difficult due to differences in FTZ/DAZ modes, FMA contraction, and
rounding behavior.</p>
<h3 id="g.5-the-golden-reference-l-1">G.5 The Golden Reference (L =
1)</h3>
<p>Specifying <code>L = 1</code> (equivalently
<code>M = sizeof(V)</code>) collapses the algorithm to a single global
balanced binary tree. This serves as a hardware-agnostic baseline for
CI/CD testing and debugging numerical discrepancies.</p>
<h3 id="g.6-divergence-from-stdaccumulate">G.6 Divergence from
std::accumulate</h3>
<p><code>std::accumulate</code> specifies a strict linear left-fold with
depth O(N). <code>canonical_reduce</code> specifies a balanced binary
tree with depth O(log N). For non-associative operations, these
represent fundamentally different algebraic expressions.</p>
<p>For floating-point summation, the tree structure is often numerically
advantageous: error growth is O(log N · ε) versus O(N · ε) for
left-fold. This is well-established as “pairwise summation” in numerical
analysis [Higham2002].</p>
<h3 id="g.7-init-placement-determinism">G.7 Init Placement
Determinism</h3>
<p>If init placement were implementation-defined, two conforming
implementations could produce different results for the same inputs. For
non-commutative operations:</p>
<pre><code>op(I, R) = 5.0 * 2 + 10.0 = 20.0
op(R, I) = 10.0 * 2 + 5.0 = 25.0</code></pre>
<p>Therefore, this proposal normatively specifies <code>op(I, R)</code>
— init as left operand of the final combination.</p>
<h2 id="appendix-h-performance-feasibility-informative">Appendix H:
Performance Feasibility (Informative)</h2>
<p>This appendix provides representative prototype measurements to
support the claim that enforcing a fixed expression structure is
practical. These measurements are not a performance guarantee.</p>
<h3 id="h.1-prototype-test-conditions">H.1 Prototype test
conditions</h3>
<ul>
<li><strong>Floating-point evaluation model controls:</strong>
<code>-ffp-contract=off</code>, <code>-fno-fast-math</code> (GCC/Clang);
<code>/fp:precise</code> (MSVC)</li>
<li><strong>FMA explicitly disabled</strong> via compiler flags where
applicable</li>
<li><strong>Input sizes:</strong> 10K to 10M elements, uniformly
distributed random values</li>
</ul>
<h3 id="h.2-representative-observations">H.2 Representative
observations</h3>
<ul>
<li>Observed overhead of approximately 10-15% compared to
<code>std::reduce</code> for typical inputs in the tested
configurations.</li>
<li>Overhead is primarily attributable to enforcing a fixed
parenthesization (removing reassociation freedom) and maintaining
per-lane state.</li>
<li>Results are configuration-dependent; different compilers, CPUs/ISAs,
and choices of M can shift the trade-off materially.</li>
</ul>
<h3 id="h.3-interpretation">H.3 Interpretation</h3>
<p>Users opt into a canonical reduction when they value reproducibility
and auditability over peak throughput. Users requiring maximum
throughput can continue to use <code>std::reduce</code> (or
domain-specific facilities) where unspecified reassociation is
acceptable.</p>
<h2 id="appendix-i-rationale-for-semantic-span-presets-informative">Appendix
I: Rationale for Semantic Span Presets (Informative)</h2>
<p>This appendix records rationale for providing a small set of
<strong>standard-fixed span preset constants</strong> (in bytes) as
coordination points for topology selection (§9.5). The goal is to
provide readable, stable choices that do not vary with platform
properties and therefore avoid “silent semantic drift” in returned
values for non-associative operations.</p>
<p>The span coordinate is a numerics convenience: for layout-stable
numeric types, it derives the semantic lane count
<code>L = M / sizeof(V)</code>.</p>
<h3 id="i.1-rationale-for-128-bytes-small-span">I.1 Rationale for 128
Bytes (Small Span)</h3>
<p>A “small” preset should provide useful lane counts for the main
scalar sizes while remaining narrow enough that overhead does not
dominate for moderate N.</p>
<p>For common scalar sizes this yields: - <strong>float (4
bytes):</strong> L = 128 / 4 = 32 - <strong>double (8 bytes):</strong> L
= 128 / 8 = 16 - <strong>64-bit integers (8 bytes):</strong> L = 16</p>
<p>This width is a practical coordination point for implementations that
exploit instruction-level parallelism through vectorization and batching
while preserving the canonical expression structure.</p>
<h3 id="i.2-rationale-for-1024-bytes-large-span">I.2 Rationale for 1024
Bytes (Large Span)</h3>
<p>A “large” preset is an explicit opt-in for throughput-oriented
structures and heterogeneous verification workflows.</p>
<p>For common scalar sizes this yields: - <strong>float (4
bytes):</strong> L = 1024 / 4 = 256 - <strong>double (8 bytes):</strong>
L = 1024 / 8 = 128</p>
<p>Such widths provide substantial independent work per lane and can
support aggressive batching and parallel evaluation of independent
reduction nodes while preserving the same abstract expression.</p>
<h3 id="i.3-cross-domain-verification">I.3 Cross-Domain
Verification</h3>
<p>Because these presets are standard-fixed, a user can select the same
span constant when executing on different hardware or in different
deployment environments. When the floating-point evaluation model is
aligned (and the same <code>binary_op</code> and input order are used),
the abstract expression structure is identical; any remaining divergence
is attributable to differences in the underlying arithmetic environment
rather than to re-association or topology choice.</p>
<p>[<em>Note:</em> This appendix is informative. These values are
coordination points for a stable abstract expression; they do not impose
any particular scheduling, threading, or vectorization strategy.
<em>—end note</em>]</p>
<h2 id="appendix-j-indicative-api-straw-man-informative">Appendix J:
Indicative API Straw Man (Informative)</h2>
<p>This appendix is illustrative; no API is proposed in this paper (§2).
It records one indicative way an eventual Standard Library facility
could expose the semantics defined in §4, while adopting the
standard-fixed named preset approach (Option 3 in §9.5). The intent is
to give LEWG something concrete to react to, while keeping spelling and
header placement explicitly provisional.</p>
<p><strong>Presentation note (informative):</strong> - The
<strong>semantic topology coordinate</strong> is the lane count
<code>L</code>. - A byte span <code>M</code> is a <strong>derived
convenience</strong> that maps to <code>L</code> using
<code>sizeof(value_type)</code>. - When cross-platform expression
stability is required, prefer APIs that specify <code>L</code>
directly.</p>
<h3 id="j.1-design-goal">J.1 Design goal</h3>
<ul>
<li>Preserve the core semantic contract: for fixed topology selection,
the returned value is as-if evaluating the canonical expression.</li>
<li>Avoid “silent semantic drift” for a no-argument default: defaults
must be stable across targets/toolchains.</li>
<li>Make “coordination choices” readable: users should be able to say
“small / large span” in code reviews, not “128 / 1024”.</li>
</ul>
<h3 id="j.2-favored-approach-standard-fixed-preset-constants-option-3">J.2
Favored approach: standard-fixed preset constants (Option 3)</h3>
<p>This approach exposes preset names as standard-fixed constants, and
(optionally) defines a default in terms of one of those preset
constants.</p>
<p><strong>Illustrative (provisional) names:</strong></p>
<div class="sourceCode" id="cb42"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">namespace</span> std <span class="op">{</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Standard-fixed span presets (bytes). Values never change.</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>  <span class="kw">inline</span> <span class="kw">constexpr</span> <span class="dt">size_t</span> canonical_span_small <span class="op">=</span> <span class="dv">128</span><span class="op">;</span> <span class="co">// &quot;narrow&quot; preset in prototypes</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>  <span class="kw">inline</span> <span class="kw">constexpr</span> <span class="dt">size_t</span> canonical_span_large <span class="op">=</span> <span class="dv">1024</span><span class="op">;</span> <span class="co">// &quot;wide&quot; preset in prototypes</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>  <span class="co">// This paper does not propose a default. A follow-on API paper may propose one.</span></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p><strong>Clarification (informative):</strong> The preset span values
above are intended to be fixed, <code>constexpr</code> constants, not
implementation-tunable parameters, so that the same canonical expression
shape is selected across implementations. This does not imply
bitwise-identical results across different floating-point evaluation
models.</p>
<p>These byte-span presets are a convenience for selecting
<code>L</code> via <code>L = M / sizeof(value_type)</code>; the
canonical expression is defined by the resulting <code>L</code>. Because
<code>sizeof(value_type)</code> may vary across ABIs/platforms, users
who require the same canonical expression across platforms should prefer
specifying <code>L</code> directly.</p>
<p><strong>Interpretation (informative):</strong> for common arithmetic
types, these spans correspond to the following lane counts: -
<strong>double (8 bytes):</strong> L_small = 16, L_large = 128 -
<strong>float (4 bytes):</strong> L_small = 32, L_large = 256</p>
<p>These correspond to the “small/narrow” and “large/wide” semantics
used in the prototype demonstrations and rationalized in Appendix I.</p>
<h3 id="j.3-straw-man-algorithm-api-span-coordinate">J.3 Straw-man
algorithm API (span coordinate)</h3>
<p>The simplest surface is an algorithm family parameterized by the span
coordinate.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="kw">namespace</span> std <span class="op">{</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Primary overloads (illustrative; header placement is out of scope for this paper)</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>  <span class="kw">template</span><span class="op">&lt;</span><span class="dt">size_t</span> M<span class="op">,</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>       <span class="kw">class</span> InputIterator<span class="op">,</span> <span class="kw">class</span> T<span class="op">,</span> <span class="kw">class</span> BinaryOperation<span class="op">&gt;</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>  <span class="kw">constexpr</span> T canonical_reduce<span class="op">(</span>InputIterator first<span class="op">,</span> InputIterator last<span class="op">,</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>                 T init<span class="op">,</span> BinaryOperation binary_op<span class="op">);</span></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>  <span class="kw">template</span><span class="op">&lt;</span><span class="dt">size_t</span> M<span class="op">,</span></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>       <span class="kw">class</span> ExecutionPolicy<span class="op">,</span> <span class="kw">class</span> ForwardIterator<span class="op">,</span> <span class="kw">class</span> T<span class="op">,</span> <span class="kw">class</span> BinaryOperation<span class="op">&gt;</span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>  T canonical_reduce<span class="op">(</span>ExecutionPolicy<span class="op">&amp;&amp;</span> policy<span class="op">,</span></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>           ForwardIterator first<span class="op">,</span> ForwardIterator last<span class="op">,</span></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>           T init<span class="op">,</span> BinaryOperation binary_op<span class="op">);</span></span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p><strong>Typical call sites:</strong></p>
<div class="sourceCode" id="cb44"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Readable coordination points (small/narrow vs large/wide)</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> a <span class="op">=</span> <span class="bu">std::</span>canonical_reduce<span class="op">&lt;</span><span class="bu">std::</span>canonical_span_small<span class="op">&gt;(</span>v<span class="op">.</span>begin<span class="op">(),</span> v<span class="op">.</span>end<span class="op">(),</span> <span class="fl">0.0</span><span class="op">,</span> <span class="bu">std::</span>plus<span class="op">&lt;&gt;{});</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> b <span class="op">=</span> <span class="bu">std::</span>canonical_reduce<span class="op">&lt;</span><span class="bu">std::</span>canonical_span_large<span class="op">&gt;(</span>v<span class="op">.</span>begin<span class="op">(),</span> v<span class="op">.</span>end<span class="op">(),</span> <span class="fl">0.0</span><span class="op">,</span> <span class="bu">std::</span>plus<span class="op">&lt;&gt;{});</span></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="co">// No default topology is proposed in this paper (users spell a preset or provide literal)</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="co">// If a future default is added, it should be standard-fixed and spelled in terms of a preset</span></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> c <span class="op">=</span> <span class="bu">std::</span>canonical_reduce<span class="op">&lt;</span><span class="bu">std::</span>canonical_span_small<span class="op">&gt;(</span>v<span class="op">.</span>begin<span class="op">(),</span> v<span class="op">.</span>end<span class="op">(),</span> <span class="fl">0.0</span><span class="op">,</span> <span class="bu">std::</span>plus<span class="op">&lt;&gt;{});</span></span></code></pre></div>
<p><strong>Rationale for this shape (informative):</strong> - Keeps the
topology selection in the type system (NTTP), which is consistent with
“semantic selection”, not a performance hint. - Supports compile-time
specialization for a fixed topology coordinate (L, or derived span M→L)
without requiring new execution-policy machinery. - Makes the
coordination presets show up in diagnostics and in code review as
names.</p>
<h3 id="j.4-straw-man-algorithm-api-lane-coordinate">J.4 Straw-man
algorithm API (lane coordinate)</h3>
<p>Where <code>sizeof(V)</code> is not stable across the intended
reproducibility domain (e.g., user-defined types), the paper already
recommends selecting topology by lane count L (§3.5, §4.3.1). An
indicative spelling for that coordinate is:</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="kw">namespace</span> std <span class="op">{</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">template</span><span class="op">&lt;</span><span class="dt">size_t</span> L<span class="op">,</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>       <span class="kw">class</span> InputIterator<span class="op">,</span> <span class="kw">class</span> T<span class="op">,</span> <span class="kw">class</span> BinaryOperation<span class="op">&gt;</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>  <span class="kw">constexpr</span> T canonical_reduce_lanes<span class="op">(</span>InputIterator first<span class="op">,</span> InputIterator last<span class="op">,</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>                    T init<span class="op">,</span> BinaryOperation binary_op<span class="op">);</span></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>  <span class="kw">template</span><span class="op">&lt;</span><span class="dt">size_t</span> L<span class="op">,</span></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>       <span class="kw">class</span> ExecutionPolicy<span class="op">,</span> <span class="kw">class</span> ForwardIterator<span class="op">,</span> <span class="kw">class</span> T<span class="op">,</span> <span class="kw">class</span> BinaryOperation<span class="op">&gt;</span></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>  T canonical_reduce_lanes<span class="op">(</span>ExecutionPolicy<span class="op">&amp;&amp;</span> policy<span class="op">,</span></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>              ForwardIterator first<span class="op">,</span> ForwardIterator last<span class="op">,</span></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>              T init<span class="op">,</span> BinaryOperation binary_op<span class="op">);</span></span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>This keeps the semantic contract ABI-independent: for fixed L, the
canonical tree is identical even if <code>sizeof(V)</code> differs
across targets.</p>
<h3 id="j.5-notes-on-naming-and-evolution">J.5 Notes on naming and
evolution</h3>
<ul>
<li>The values of existing preset constants must never change. Future
standards may add new presets under new names.</li>
<li>Whether the facility lives as a new algorithm (e.g.,
<code>canonical_reduce</code>) or as a <code>std::reduce</code>-adjacent
customization is an LEWG design choice; this appendix only shows one
straightforward spelling.</li>
<li>If LEWG prefers to mirror [numerics.defns] (“GENERALIZED_SUM”-style
definitional functions), the same preset constants can still be used as
stable topology selectors.</li>
</ul>
<p>[<em>Note:</em> This appendix is illustrative. It is intended to
reduce “API haze” during discussion while keeping the semantic core of
the paper independent of any particular surface spelling. <em>—end
note</em>]</p>
<h3 id="j.6-range-overloads-straw-man">J.6 Range overloads
(straw-man)</h3>
<p>Range overloads are deferred per §2. The following sketches are
included only to reduce “API haze” and to indicate one illustrative
direction consistent with the semantic requirements of §4.</p>
<p><strong>Key observation:</strong> the canonical expression in §4 is
defined over N elements, so a range surface generally needs N prior to
evaluation. A surface can obtain N without allocation (e.g.,
<code>sized_range</code> or a counting pass over a multipass range), or
it can require explicit materialization for single-pass sources.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="kw">namespace</span> <span class="bu">std::</span>ranges <span class="op">{</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Span coordinate — sequential/reference mode</span></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Non-sized forward ranges can be supported via a counting pass to determine N.</span></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>  <span class="kw">template</span><span class="op">&lt;</span><span class="dt">size_t</span> M<span class="op">,</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>       forward_range R<span class="op">,</span> <span class="kw">class</span> T<span class="op">,</span> <span class="kw">class</span> BinaryOperation<span class="op">&gt;</span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>  <span class="kw">requires</span> <span class="op">(</span>M <span class="op">&gt;=</span> <span class="kw">sizeof</span><span class="op">(</span><span class="dt">range_value_t</span><span class="op">&lt;</span>R<span class="op">&gt;))</span> <span class="op">&amp;&amp;</span></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>       <span class="op">(</span>M <span class="op">%</span> <span class="kw">sizeof</span><span class="op">(</span><span class="dt">range_value_t</span><span class="op">&lt;</span>R<span class="op">&gt;)</span> <span class="op">==</span> <span class="dv">0</span><span class="op">)</span></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>  <span class="kw">constexpr</span> T canonical_reduce<span class="op">(</span>R<span class="op">&amp;&amp;</span> r<span class="op">,</span> T init<span class="op">,</span> BinaryOperation op<span class="op">);</span></span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Span coordinate — execution policy overload (illustrative, conservative constraints)</span></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>  <span class="co">// A subsequent API revision could choose conservative constraints (e.g., random_access_range)</span></span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>  <span class="kw">template</span><span class="op">&lt;</span><span class="dt">size_t</span> M<span class="op">,</span></span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>       <span class="kw">class</span> ExecutionPolicy<span class="op">,</span></span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a>       random_access_range R<span class="op">,</span> <span class="kw">class</span> T<span class="op">,</span> <span class="kw">class</span> BinaryOperation<span class="op">&gt;</span></span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a>  <span class="kw">requires</span> sized_range<span class="op">&lt;</span>R<span class="op">&gt;</span> <span class="op">&amp;&amp;</span></span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a>       is_execution_policy_v<span class="op">&lt;</span><span class="dt">remove_cvref_t</span><span class="op">&lt;</span>ExecutionPolicy<span class="op">&gt;&gt;</span> <span class="op">&amp;&amp;</span></span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>       <span class="op">(</span>M <span class="op">&gt;=</span> <span class="kw">sizeof</span><span class="op">(</span><span class="dt">range_value_t</span><span class="op">&lt;</span>R<span class="op">&gt;))</span> <span class="op">&amp;&amp;</span></span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a>       <span class="op">(</span>M <span class="op">%</span> <span class="kw">sizeof</span><span class="op">(</span><span class="dt">range_value_t</span><span class="op">&lt;</span>R<span class="op">&gt;)</span> <span class="op">==</span> <span class="dv">0</span><span class="op">)</span></span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a>  T canonical_reduce<span class="op">(</span>ExecutionPolicy<span class="op">&amp;&amp;</span> policy<span class="op">,</span> R<span class="op">&amp;&amp;</span> r<span class="op">,</span> T init<span class="op">,</span> BinaryOperation op<span class="op">);</span></span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Lane coordinate — sequential/reference mode</span></span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a>  <span class="kw">template</span><span class="op">&lt;</span><span class="dt">size_t</span> L<span class="op">,</span></span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a>       forward_range R<span class="op">,</span> <span class="kw">class</span> T<span class="op">,</span> <span class="kw">class</span> BinaryOperation<span class="op">&gt;</span></span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a>  <span class="kw">requires</span> <span class="op">(</span>L <span class="op">&gt;=</span> <span class="dv">1</span><span class="op">)</span></span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a>  <span class="kw">constexpr</span> T canonical_reduce_lanes<span class="op">(</span>R<span class="op">&amp;&amp;</span> r<span class="op">,</span> T init<span class="op">,</span> BinaryOperation op<span class="op">);</span></span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb46-27"><a href="#cb46-27" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Lane coordinate — execution policy overload (illustrative, conservative constraints)</span></span>
<span id="cb46-28"><a href="#cb46-28" aria-hidden="true" tabindex="-1"></a>  <span class="kw">template</span><span class="op">&lt;</span><span class="dt">size_t</span> L<span class="op">,</span></span>
<span id="cb46-29"><a href="#cb46-29" aria-hidden="true" tabindex="-1"></a>       <span class="kw">class</span> ExecutionPolicy<span class="op">,</span></span>
<span id="cb46-30"><a href="#cb46-30" aria-hidden="true" tabindex="-1"></a>       random_access_range R<span class="op">,</span> <span class="kw">class</span> T<span class="op">,</span> <span class="kw">class</span> BinaryOperation<span class="op">&gt;</span></span>
<span id="cb46-31"><a href="#cb46-31" aria-hidden="true" tabindex="-1"></a>  <span class="kw">requires</span> sized_range<span class="op">&lt;</span>R<span class="op">&gt;</span> <span class="op">&amp;&amp;</span></span>
<span id="cb46-32"><a href="#cb46-32" aria-hidden="true" tabindex="-1"></a>       is_execution_policy_v<span class="op">&lt;</span><span class="dt">remove_cvref_t</span><span class="op">&lt;</span>ExecutionPolicy<span class="op">&gt;&gt;</span> <span class="op">&amp;&amp;</span></span>
<span id="cb46-33"><a href="#cb46-33" aria-hidden="true" tabindex="-1"></a>       <span class="op">(</span>L <span class="op">&gt;=</span> <span class="dv">1</span><span class="op">)</span></span>
<span id="cb46-34"><a href="#cb46-34" aria-hidden="true" tabindex="-1"></a>  T canonical_reduce_lanes<span class="op">(</span>ExecutionPolicy<span class="op">&amp;&amp;</span> policy<span class="op">,</span> R<span class="op">&amp;&amp;</span> r<span class="op">,</span> T init<span class="op">,</span> BinaryOperation op<span class="op">);</span></span>
<span id="cb46-35"><a href="#cb46-35" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>[<em>Note:</em> The final constraints for a range surface (e.g.,
whether to permit a counting pass for non-sized multipass ranges, and
whether to reject single-pass <code>input_range</code> to avoid implicit
allocation) are design questions for a subsequent API-focused revision
once LEWG has accepted the semantic contract in §4. Appendix L records
the relevant trade-offs. <em>—end note</em>]</p>
<h2 id="appendix-k-demonstrator-godbolts-informative">Appendix K:
Demonstrator Godbolts (Informative)</h2>
<p>This appendix is not part of the proposal. It records the
demonstrator Compiler Explorer (“Godbolt”) programs used to validate the
semantics described in §4 on multiple architectures, and to show the
gross performance impact of enforcing a fixed abstract expression.</p>
<p><strong>The programs referenced here are semantic witnesses</strong>,
not reference implementations. They exist to demonstrate that the
canonical expression defined in §4 can be evaluated on real hardware
(SIMD, GPU, multi-threaded). They are not normative examples and are not
intended as implementation guidance for general <code>binary_op</code>.
All demonstrators except GB-SEQ test only
<code>std::plus&lt;double&gt;</code> and may use <code>0.0</code> for
absent operand positions; conforming implementations must handle
arbitrary <code>binary_op</code> via the lifted <code>COMBINE</code>
rules of §4.2.2.</p>
<h4 id="k.0-golden-reference-values">§K.0 Golden reference values</h4>
<p>The following reference results are used throughout the demonstrators
to validate bitwise reproducibility under the specified canonical
expression and fixed PRNG seed:</p>
<ul>
<li><strong>NARROW</strong> (<code>L = 16</code>,
i.e. <code>M = 128 bytes</code> for <code>double</code>):
<code>0x40618f71f6379380</code></li>
<li><strong>WIDE</strong> (<code>L = 128</code>,
i.e. <code>M = 1024 bytes</code> for <code>double</code>):
<code>0x40618f71f6379397</code></li>
</ul>
<p>[<em>Note:</em> These values validate conformance to the canonical
expression for the specific demonstrator environment. Bitwise agreement
across platforms additionally requires aligned floating‑point evaluation
models (§2.5, §6). <em>—end note</em>]</p>
<p>The intent is pragmatic: give reviewers a “click-run-inspect”
artifact that: - validates semantic invariants (tree identity and
“golden hex” outputs) rather than trying to “win benchmarks”, - prints a
short verification block (seed, N, and the resulting hex value), -
checks run-to-run stability and translational invariance (address-offset
invariance), - and shows a coarse benchmark table comparing against
<code>std::accumulate</code> and <code>std::reduce</code> variants.</p>
<p><strong>Important caveat:</strong> Compiler Explorer run times vary
substantially with VM load, CPU model, and throttling. These tables are
illustrative only; the repository benchmarks (multi-thread and CUDA) are
the authoritative numbers.</p>
<h3 id="k.1-demonstrator-set-gross-platform-runs">K.1 Demonstrator set
(gross platform runs)</h3>
<p>The following Compiler Explorer (“Godbolt”) demonstrators are
intended to be click-run-inspect artifacts for reviewers. Each link
contains: - a determinism/verification block (seed, N, and printed
result hex), - run-to-run stability checks, - a coarse timing table
comparing against <code>std::accumulate</code> and
<code>std::reduce</code> variants under comparable FP settings.</p>
<table>
<colgroup>
<col style="width: 21%" />
<col style="width: 15%" />
<col style="width: 14%" />
<col style="width: 37%" />
<col style="width: 10%" />
</colgroup>
<thead>
<tr class="header">
<th>Demonstrator</th>
<th>Platform</th>
<th>Purpose</th>
<th>Arbitrary <code>binary_op</code>?</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>[GB-SEQ] single-thread reference</td>
<td>Portable</td>
<td>Sequential evaluation of the canonical expression (§4)</td>
<td>Yes (faithful §4 reference)</td>
<td>Debugger-friendly “golden” comparator; single-threaded only.</td>
</tr>
<tr class="even">
<td>[GB-x86-AVX2] single-file x86</td>
<td>x86-64</td>
<td>Canonical reduction on x86 with AVX2 codegen; compares against
<code>std::accumulate</code> and <code>std::reduce</code></td>
<td>No (<code>std::plus&lt;double&gt;</code> only)</td>
<td>Uses safe feature gating; suitable for “Run”.</td>
</tr>
<tr class="odd">
<td>[GB-x86-MT] multi-threaded x86</td>
<td>x86-64</td>
<td>Deterministic multi-threaded reduction using shift-reduce stack
state + deterministic merge</td>
<td>No (<code>std::plus&lt;double&gt;</code> only)</td>
<td>Demonstrates schedule-independent, thread-count-independent
results.</td>
</tr>
<tr class="even">
<td>[GB-x86-MT-PERF] multi-threaded perf</td>
<td>x86-64</td>
<td>Production-quality multi-threaded reduction with thread pool and
per-lane carry pairwise</td>
<td>No (<code>std::plus&lt;double&gt;</code> only)</td>
<td>Thread pool amortizes creation cost; shows throughput scaling.</td>
</tr>
<tr class="odd">
<td>[GB-NEON] single-file NEON</td>
<td>AArch64</td>
<td>Canonical reduction on ARM64 using NEON (exact tree preserved)</td>
<td>No (<code>std::plus&lt;double&gt;</code> only)</td>
<td>Prints build-proof macros (<code>__aarch64__</code>,
<code>__ARM_NEON</code>, <code>__ARM_NEON_FP</code>).</td>
</tr>
<tr class="even">
<td>[GB-NEON-PERF] NEON performance</td>
<td>AArch64</td>
<td>Shift-reduce with 8-block NEON pre-reduction for near-bandwidth
throughput</td>
<td>No (<code>std::plus&lt;double&gt;</code> only)</td>
<td>8-block straight-line NEON hot loop; carry cascade fires 8× less
often.</td>
</tr>
<tr class="odd">
<td>[GB-CUDA] (optional) CUDA / CUB comparison</td>
<td>NVCC</td>
<td>Illustrates canonical topology evaluation on GPU and compares
against CUB reduction</td>
<td>No (<code>std::plus&lt;double&gt;</code> only)</td>
<td>Heterogeneous “golden result” workflow demonstrator.</td>
</tr>
</tbody>
</table>
<p><strong>Godbolt links:</strong></p>
<ul>
<li><p>[GB-SEQ] = https://godbolt.org/z/8EEhEqrz6<br />
(single-threaded reference; faithful §4 implementation including
<code>COMBINE</code> rules; supports arbitrary
<code>binary_op</code>)</p></li>
<li><p>[GB-x86-AVX2] = https://godbolt.org/z/Eaa3vWYqb<br />
(tests <code>std::plus&lt;double&gt;</code> only; uses <code>0.0</code>
for absent positions; proves SIMD vertical-add matches §4 tree)</p></li>
<li><p>[GB-x86-MT] = https://godbolt.org/z/7a11r9o95<br />
(tests <code>std::plus&lt;double&gt;</code> only; uses <code>0.0</code>
for absent positions; proves thread-count and schedule
invariance)</p></li>
<li><p>[GB-x86-MT-PERF] = https://godbolt.org/z/sdxMohT48<br />
(tests <code>std::plus&lt;double&gt;</code> only; thread pool + per-lane
carry pairwise; proves throughput scaling with thread count)</p></li>
<li><p>[GB-NEON] = https://godbolt.org/z/Pxzc3YM7q<br />
(Arm v8 / AArch64; tests <code>std::plus&lt;double&gt;</code> only; uses
<code>0.0</code> for absent positions; proves NEON vector reduction
matches §4 tree)</p></li>
<li><p>[GB-NEON-PERF] = https://godbolt.org/z/sY9W78rze<br />
(Arm v8 / AArch64; tests <code>std::plus&lt;double&gt;</code> only;
shift-reduce with 8-block NEON pre-reduction; near-bandwidth
throughput)</p></li>
<li><p>[GB-CUDA] = https://godbolt.org/z/5n9EvGoeb<br />
(CUDA/NVCC; tests <code>std::plus&lt;double&gt;</code> only; uses
<code>0.0</code> for absent positions; proves warp shuffle matches §4
tree; includes L=16 and L=128)</p></li>
</ul>
<p>All demonstrators use the same dataset generation (seeded RNG) and
report the same canonical results for the two topology coordinates used
throughout this paper:</p>
<ul>
<li><strong>Small / narrow:</strong> L = 16 for double (M = 128
bytes)</li>
<li><strong>Large / wide:</strong> L = 128 for double (M = 1024
bytes)</li>
</ul>
<h3 id="k.2-what-the-demonstrators-are-intended-to-prove">K.2 What the
demonstrators are intended to prove</h3>
<p>Each demonstrator prints:</p>
<ol type="1">
<li><strong>Data verification:</strong> the first few generated elements
(as hex) match a known sequence for the fixed seed.</li>
<li><strong>Correctness:</strong> the returned value for L=16 and L=128
matches known “golden” hex outputs.</li>
<li><strong>Run-to-run stability:</strong> repeated evaluation yields
identical results.</li>
<li><strong>Translational invariance:</strong> copying the same values
to different memory offsets does not change the result.</li>
</ol>
<p>These are concrete checks that: - the canonical expression is
independent of execution schedule (single-thread vs vectorized), - the
topology is a function of the chosen topology coordinate (L, or derived
span M→L) and input order, - and the implementation does not
accidentally depend on address alignment or allocator behavior.</p>
<h3 id="k.3-expected-verification-outputs-for-the-published-demonstrators">K.3
Expected verification outputs (for the published demonstrators)</h3>
<p>For the canonical seed and N = 1,000,000 doubles, the demonstrators
are configured to print the following expected result hex values:</p>
<ul>
<li><strong>L = 16, “NARROW” (M = 128 bytes):</strong>
<code>0x40618f71f6379380</code></li>
<li><strong>L = 128, “WIDE” (M = 1024 bytes):</strong>
<code>0x40618f71f6379397</code></li>
</ul>
<p>The demonstrators treat a mismatch as a test failure.</p>
<h3 id="k.3.1-cancellation-stress-dataset-recommended">K.3.1
Cancellation stress dataset (recommended)</h3>
<p>The PRNG dataset validates implementation correctness but may not
demonstrate <em>why</em> canonical reduction matters. Uniform random
values of similar magnitude can produce nearly identical results under
different tree shapes, making the golden match appear trivially
achievable.</p>
<p>To demonstrate the facility’s core value proposition, demonstrators
should additionally include a <strong>cancellation-heavy
dataset</strong> where evaluation order visibly affects the result. The
pattern from §1.2, scaled to large N:</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Cancellation stress: [+1e16, +1.0, -1e16, +1.0, ...] repeated N/4 times</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="bu">std::</span>vector<span class="op">&lt;</span><span class="dt">double</span><span class="op">&gt;</span> data<span class="op">;</span></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">size_t</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> N<span class="op">/</span><span class="dv">4</span><span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>    data<span class="op">.</span>push_back<span class="op">(+</span><span class="fl">1e16</span><span class="op">);</span></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>    data<span class="op">.</span>push_back<span class="op">(+</span><span class="fl">1.0</span><span class="op">);</span></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>    data<span class="op">.</span>push_back<span class="op">(-</span><span class="fl">1e16</span><span class="op">);</span></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>    data<span class="op">.</span>push_back<span class="op">(+</span><span class="fl">1.0</span><span class="op">);</span></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>For this dataset, the demonstrators should show all three of:</p>
<ol type="1">
<li><strong><code>std::reduce</code> is non-deterministic:</strong>
repeated runs (or varying thread counts) produce different hex values,
because the scheduler changes the effective parenthesization.</li>
<li><strong><code>canonical_reduce</code> with fixed <code>L</code> is
deterministic:</strong> repeated runs produce identical hex values,
because the tree is fixed.</li>
<li><strong>Different <code>L</code> values produce different
results:</strong> confirming that the topology coordinate controls the
abstract expression and is not merely an implementation hint.</li>
</ol>
<p>This triple is the paper’s entire motivation in one test output. If
the PRNG dataset is the “does the implementation work?” test, the
cancellation dataset is the “does the facility matter?” test.</p>
<h3 id="k.4-recommended-compiler-explorer-settings">K.4 Recommended
Compiler Explorer settings</h3>
<h4 id="k.4.1-x86-demonstrator-gb-x86-avx2">K.4.1 x86 demonstrator
([GB-x86-AVX2])</h4>
<p><strong>Compiler:</strong> x86-64 clang (trunk) (or GCC)</p>
<p><strong>Recommended flags (for running):</strong></p>
<pre><code>-O3 -std=c++20 -ffp-contract=off -fno-fast-math</code></pre>
<h4 id="k.4.2-aarch64-demonstrator-gb-neon">K.4.2 AArch64 demonstrator
([GB-NEON])</h4>
<p><strong>Compiler:</strong> AArch64 clang (trunk) (or GCC)</p>
<p><strong>Recommended flags (for running):</strong></p>
<pre><code>-O3 -std=c++20 -march=armv8-a -ffp-contract=off -fno-fast-math</code></pre>
<p>If <code>std::execution::par</code> is unavailable or unreliable in
the CE environment, the demonstrator can be built with:</p>
<pre><code>-DNO_PAR_POLICIES=1</code></pre>
<h3 id="k.5-interpreting-the-performance-tables-gross-impacts">K.5
Interpreting the performance tables (gross impacts)</h3>
<p>The demonstrators typically report: - <code>std::accumulate</code> as
a deterministic, sequential baseline; - <code>std::reduce</code>
variants (no policy, seq, unseq, and optionally par*) as “existing
practice” comparators; - deterministic/canonical narrow and wide
presets.</p>
<p>Readers should interpret the results as: - <strong>Gross cost of
structure:</strong> overhead (or sometimes speedup) relative to
<code>std::reduce(seq)</code> under similar FP settings. -
<strong>Configuration sensitivity:</strong> narrow vs wide spans can
trade off cache behavior, vectorization, and bandwidth. -
<strong>Non-authoritative:</strong> results on CE do not replace proper
benchmarking; they are a convenience for quick inspection.</p>
<h3 id="k.6-relationship-to-repository-evidence">K.6 Relationship to
repository evidence</h3>
<p>The accompanying repository (referenced in the paper’s artifacts
section) contains: - controlled micro-benchmarks on pinned CPUs
(repeatable measurements), - multi-threaded execution model comparisons,
- and a CUDA demonstrator evaluating the same canonical expression (for
chosen topology) to support heterogeneous “golden result” workflows.</p>
<p>[<em>Note:</em> The paper’s semantic contract is independent of any
specific demonstrator; these artifacts exist to make the semantics
tangible and reviewable. <em>—end note</em>]</p>
<p>[<em>Note:</em> These demonstrators depend on specific compiler
versions and Compiler Explorer VM configurations available at the time
of writing. The normative specification is §4; demonstrator links
provide supplementary illustration only. If a link becomes unavailable
or produces results inconsistent with the published golden values due to
toolchain changes, the specification remains unaffected. <em>—end
note</em>]</p>
<h2 id="appendix-l-ranges-compatibility-informative">Appendix L: Ranges
Compatibility (Informative)</h2>
<p>This appendix is not part of the proposal. It records design
considerations for eventual C++20/23 ranges overloads that expose the
semantics of §4 in a composable way.</p>
<h3 id="l.1-a-range-surface-does-not-change-the-semantic-contract">L.1 A
range surface does not change the semantic contract</h3>
<p>A range-based surface would not change the semantic contract: for a
fixed topology selector (<code>L</code>, or span <code>M</code>
interpreted as <code>M→L</code>), input order, and
<code>binary_op</code>, the returned value is as-if evaluating the
canonical expression defined in §4.</p>
<p>This paper intentionally defers the ranges surface to keep first
discussion focused on the semantic contract (see §2). Appendix J.6
contains a straw-man sketch.</p>
<h3 id="l.2-determining-n-without-hidden-allocation">L.2 Determining N
without hidden allocation</h3>
<p>The canonical expression in §4 is defined over N elements in a fixed
input order. A range-based surface therefore needs a way to determine N
and to preserve the element order used by the expression.</p>
<p>Three implementation strategies exist:</p>
<ul>
<li><strong>sized_range:</strong> obtain N in O(1) via
<code>ranges::size(r)</code>.</li>
<li><strong>Non-sized, multipass (forward_range):</strong> determine N
via a counting pass (e.g., <code>ranges::distance(r)</code>), then
evaluate the canonical expression in a second pass. This avoids
allocation but may traverse the range twice.</li>
<li><strong>Single-pass (input_range):</strong> the count is not
available without consuming the range; this specification does not
support single-pass ranges without explicit materialization by the
caller.</li>
</ul>
<p>One conservative design point (for a subsequent API revision) is to
avoid implicit allocation:</p>
<ul>
<li><strong>Sequential/reference overloads:</strong> accept
<code>forward_range</code> and permit a counting pass when N is not
otherwise known.</li>
<li><strong>Execution-policy overloads:</strong> require a stronger
range category (e.g., <code>random_access_range</code>) and may also
require <code>sized_range</code>, keeping buffering explicit.</li>
</ul>
<h3 id="l.3-working-with-non-sized-single-pass-sources">L.3 Working with
non-sized, single-pass sources</h3>
<p>For sources that are single-pass (or otherwise cannot be traversed
twice), users requiring canonical reduction can materialize
explicitly:</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> filtered <span class="op">=</span> data</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>  <span class="op">|</span> <span class="bu">std::</span>views::filter<span class="op">(</span>pred<span class="op">)</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>  <span class="op">|</span> <span class="bu">std::</span>ranges::to<span class="op">&lt;</span><span class="bu">std::</span>vector<span class="op">&gt;();</span></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> r <span class="op">=</span> <span class="bu">std::</span>ranges::canonical_reduce<span class="op">&lt;</span><span class="bu">std::</span>canonical_span_small<span class="op">&gt;(</span></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>  filtered<span class="op">,</span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>  <span class="fl">0.0</span><span class="op">,</span></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>  <span class="bu">std::</span>plus<span class="op">&lt;&gt;{});</span></span></code></pre></div>
<p>This makes allocation explicit and keeps the cost model under user
control.</p>
<h3 id="l.4-projection-parameter">L.4 Projection parameter</h3>
<p>This paper does not attempt to design a projection parameter. Users
can express projection by composing with <code>views::transform</code>,
keeping the algorithm surface orthogonal and consistent with ranges
composition:</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> sum <span class="op">=</span> <span class="bu">std::</span>ranges::canonical_reduce<span class="op">&lt;</span><span class="bu">std::</span>canonical_span_small<span class="op">&gt;(</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>  data <span class="op">|</span> <span class="bu">std::</span>views::transform<span class="op">([](</span><span class="at">const</span> <span class="kw">auto</span><span class="op">&amp;</span> x<span class="op">)</span> <span class="op">{</span> <span class="cf">return</span> x<span class="op">.</span>value<span class="op">;</span> <span class="op">}),</span></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>  <span class="fl">0.0</span><span class="op">,</span></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>  <span class="bu">std::</span>plus<span class="op">&lt;&gt;{});</span></span></code></pre></div>
<p>[<em>Note:</em> This appendix is informative. It does not commit the
proposal to a particular ranges overload set; it documents constraints
and trade-offs to inform a subsequent API-focused revision. <em>—end
note</em>]</p>
<h2 id="appendix-m-detailed-motivation-and-use-cases-informative">Appendix
M: Detailed Motivation and Use Cases (Informative)</h2>
<p>This appendix is not part of the proposal. It collects the longer
use-case narratives (with examples) to keep the main document focused on
the semantic contract in §4.</p>
<p>The following use cases illustrate the value of run-to-run stable
parallel reduction.</p>
<p>[<em>Note:</em> Code examples use a placeholder spelling. No specific
API is proposed in this paper. <em>—end note</em>]</p>
<h3 id="m.1-ci-regression-testing">M.1 CI Regression Testing</h3>
<div class="sourceCode" id="cb53"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Test that fails intermittently with std::reduce</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="dt">double</span> baseline <span class="op">=</span> load_golden_value<span class="op">(</span><span class="st">&quot;gradient_sum.bin&quot;</span><span class="op">);</span> <span class="co">// Previously computed</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="dt">double</span> computed <span class="op">=</span> <span class="bu">std::</span>reduce<span class="op">(</span><span class="bu">std::</span>execution::par<span class="op">,</span></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>               gradients<span class="op">.</span>begin<span class="op">(),</span> gradients<span class="op">.</span>end<span class="op">(),</span> <span class="fl">0.0</span><span class="op">);</span></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>EXPECT_DOUBLE_EQ<span class="op">(</span>baseline<span class="op">,</span> computed<span class="op">);</span> <span class="co">// FAILS: result varies by thread scheduling</span></span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a><span class="co">// With canonical_reduce_lanes: no run-to-run variation</span></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a><span class="dt">double</span> computed <span class="op">=</span> canonical_reduce_lanes<span class="op">&lt;</span><span class="dv">8</span><span class="op">&gt;(</span>           <span class="co">// L = 8 (or equivalently M = 64 bytes for double)</span></span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>  gradients<span class="op">.</span>begin<span class="op">(),</span> gradients<span class="op">.</span>end<span class="op">(),</span> <span class="fl">0.0</span><span class="op">);</span></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>EXPECT_DOUBLE_EQ<span class="op">(</span>baseline<span class="op">,</span> computed<span class="op">);</span> <span class="co">// PASSES: expression structure is fixed</span></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a><span class="co">// (baseline must also have been computed with same L on same platform)</span></span></code></pre></div>
<p><strong>Value:</strong> Eliminate spurious test failures caused by
run-to-run variation from unspecified reduction order. Within a
consistent build/test environment, the returned value is stable across
invocations.</p>
<h3 id="m.2-distributed-training-checkpoints">M.2 Distributed Training
Checkpoints</h3>
<div class="sourceCode" id="cb54"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Machine learning gradient aggregation</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="co">// Expression structure fixed by the chosen lane count L, enabling reproducible checkpoints</span></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> gradient_sum <span class="op">=</span> canonical_reduce_lanes<span class="op">&lt;</span><span class="dv">8</span><span class="op">&gt;(</span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>  local_gradients<span class="op">.</span>begin<span class="op">(),</span> local_gradients<span class="op">.</span>end<span class="op">(),</span> <span class="fl">0.0</span><span class="op">);</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a><span class="co">// Later, on same or equivalent hardware:</span></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> restored_sum <span class="op">=</span> canonical_reduce_lanes<span class="op">&lt;</span><span class="dv">8</span><span class="op">&gt;(</span></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>  restored_gradients<span class="op">.</span>begin<span class="op">(),</span> restored_gradients<span class="op">.</span>end<span class="op">(),</span> <span class="fl">0.0</span><span class="op">);</span></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a><span class="co">// Reproducible: same inputs + same L + the same floating-point evaluation model → same result</span></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a><span class="co">// No longer subject to thread-count or scheduling variation</span></span></code></pre></div>
<p><strong>Value:</strong> Enable checkpoint/restore with reproducible
gradient aggregation, eliminating run-to-run variation from unspecified
reduction order. Cross-platform restore requires matching floating-point
semantics.</p>
<h3 id="m.3-regulatory-audit-trails">M.3 Regulatory Audit Trails</h3>
<div class="sourceCode" id="cb55"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Financial risk calculation that must be reproducible for auditors</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="kw">struct</span> RiskCalculation <span class="op">{</span></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">static</span> <span class="kw">constexpr</span> <span class="dt">size_t</span> L <span class="op">=</span> <span class="dv">8</span><span class="op">;</span> <span class="co">// Documented in audit trail</span></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">double</span> compute_var<span class="op">(</span><span class="at">const</span> <span class="bu">std::</span>vector<span class="op">&lt;</span><span class="dt">double</span><span class="op">&gt;&amp;</span> scenarios<span class="op">)</span> <span class="op">{</span></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> canonical_reduce_lanes<span class="op">&lt;</span>L<span class="op">&gt;(</span></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>      scenarios<span class="op">.</span>begin<span class="op">(),</span> scenarios<span class="op">.</span>end<span class="op">(),</span> <span class="fl">0.0</span><span class="op">,</span></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>      <span class="op">[](</span><span class="dt">double</span> a<span class="op">,</span> <span class="dt">double</span> b<span class="op">)</span> <span class="op">{</span> <span class="cf">return</span> a <span class="op">+</span> b <span class="op">*</span> b<span class="op">;</span> <span class="op">});</span></span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a><span class="op">};</span></span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a><span class="co">// Auditor can reproduce exact result years later with same L</span></span></code></pre></div>
<p><strong>Value:</strong> Meet regulatory requirements for reproducible
risk calculations (Basel III, Solvency II).</p>
<h3 id="m.4-scientific-reproducibility">M.4 Scientific
Reproducibility</h3>
<div class="sourceCode" id="cb56"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Climate model energy conservation check</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="co">// Paper claims: &quot;Energy drift &lt; 1e-12 per century&quot;</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="co">// Reviewers must reproduce this result</span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> total_energy <span class="op">=</span> canonical_reduce_lanes<span class="op">&lt;</span><span class="dv">8</span><span class="op">&gt;(</span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>  grid_cells<span class="op">.</span>begin<span class="op">(),</span> grid_cells<span class="op">.</span>end<span class="op">(),</span> <span class="fl">0.0</span><span class="op">,</span></span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>  <span class="op">[](</span><span class="dt">double</span> sum<span class="op">,</span> <span class="at">const</span> Cell<span class="op">&amp;</span> c<span class="op">)</span> <span class="op">{</span> <span class="cf">return</span> sum <span class="op">+</span> c<span class="op">.</span>kinetic <span class="op">+</span> c<span class="op">.</span>potential<span class="op">;</span> <span class="op">});</span></span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a><span class="co">// Published with: &quot;Results computed with L=8, compiled with -ffp-contract=off&quot;</span></span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a><span class="co">// Reviewer with the same L and evaluation-model settings gets same result</span></span></code></pre></div>
<p><strong>Value:</strong> Enable peer verification of published
computational results when floating-point evaluation model is documented
and matched.</p>
<h3 id="m.5-exascale-hpc-with-kokkos">M.5 Exascale HPC with Kokkos</h3>
<div class="sourceCode" id="cb57"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co">// DOE exascale application using Kokkos</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="co">// Current Kokkos parallel_reduce is non-deterministic</span></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="co">// Today: Results vary across runs, making debugging nearly impossible</span></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>Kokkos<span class="op">::</span>parallel_reduce<span class="op">(</span><span class="st">&quot;EnergySum&quot;</span><span class="op">,</span> N<span class="op">,</span> KOKKOS_LAMBDA<span class="op">(</span><span class="dt">int</span> i<span class="op">,</span> <span class="dt">double</span><span class="op">&amp;</span> sum<span class="op">)</span> <span class="op">{</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>  sum <span class="op">+=</span> compute_energy<span class="op">(</span>i<span class="op">);</span></span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a><span class="op">},</span> total_energy<span class="op">);</span></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a><span class="co">// With standardized canonical reduction semantics, frameworks could expose a canonical mode</span></span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a><span class="co">// (illustrative — actual Kokkos API would be determined by Kokkos maintainers)</span></span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>Kokkos<span class="op">::</span>parallel_reduce<span class="op">&lt;</span>Kokkos<span class="op">::</span>CanonicalLanes<span class="op">&lt;</span><span class="dv">8</span><span class="op">&gt;&gt;(</span><span class="st">&quot;EnergySum&quot;</span><span class="op">,</span> N<span class="op">,</span> KOKKOS_LAMBDA<span class="op">(</span><span class="dt">int</span> i<span class="op">,</span> <span class="dt">double</span><span class="op">&amp;</span> sum<span class="op">)</span> <span class="op">{</span></span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>  sum <span class="op">+=</span> compute_energy<span class="op">(</span>i<span class="op">);</span></span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a><span class="op">},</span> total_energy<span class="op">);</span></span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a><span class="co">// Reproducible across runs on same platform</span></span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a><span class="co">// Cross-platform reproducibility requires matching FP semantics</span></span></code></pre></div>
<p><strong>Value:</strong> Enable reproducible physics in production HPC
codes, at least within a consistent execution environment.</p>
<h3 id="m.6-cross-platform-development">M.6 Cross-Platform
Development</h3>
<div class="sourceCode" id="cb58"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Game physics: aim for consistency between client platforms</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="kw">constexpr</span> <span class="dt">size_t</span> PHYSICS_L <span class="op">=</span> <span class="dv">16</span><span class="op">;</span> <span class="co">// 16 lanes (M = 64 bytes for float)</span></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a><span class="dt">float</span> compute_collision_impulse<span class="op">(</span><span class="at">const</span> <span class="bu">std::</span>vector<span class="op">&lt;</span>Contact<span class="op">&gt;&amp;</span> contacts<span class="op">)</span> <span class="op">{</span></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> canonical_reduce_lanes<span class="op">&lt;</span>PHYSICS_L<span class="op">&gt;(</span></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>    contacts<span class="op">.</span>begin<span class="op">(),</span> contacts<span class="op">.</span>end<span class="op">(),</span> <span class="fl">0.0</span><span class="bu">f</span><span class="op">,</span></span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>    <span class="op">[](</span><span class="dt">float</span> sum<span class="op">,</span> <span class="at">const</span> Contact<span class="op">&amp;</span> c<span class="op">)</span> <span class="op">{</span> <span class="cf">return</span> sum <span class="op">+</span> c<span class="op">.</span>impulse<span class="op">;</span> <span class="op">});</span></span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a><span class="co">// Expression structure is fixed.</span></span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a><span class="co">// Cross-platform match requires controlling floating-point evaluation model</span></span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a><span class="co">// (e.g., disabling FMA, matching rounding modes)</span></span></code></pre></div>
<p><strong>Value:</strong> Fix expression structure as one component of
cross-platform consistency. Full cross-platform bitwise identity
additionally requires matching hardware-level FP behaviors (e.g.,
contraction, intermediate precision, and subnormal handling), which are
outside the scope of this proposal.</p>
<h3 id="m.7-reference-and-debugging-mode">M.7 Reference and Debugging
Mode</h3>
<p>A practical requirement for reproducibility is the ability to
validate and debug operator logic on a single thread while preserving
the same abstract expression as production parallel execution. For that
reason, if an eventual Standard Library API is adopted, the overload
without an ExecutionPolicy should be specified to evaluate the identical
canonical expression tree as the execution-policy overloads for the same
L and inputs.</p>
<p>This “reference mode” enables: - Reproducing “golden results” for
audit or regression testing on a single thread, - Debugging
<code>binary_op</code> with conventional tools while guaranteeing
expression-equivalence to the parallel workload, - Cross-platform
verification (e.g., CPU verification of accelerator-produced results)
when evaluation models are aligned.</p>
<p>[<em>Note:</em> This paper does not propose a specific API shape;
this subsection records a design requirement that follows from the
semantic goal. <em>—end note</em>]</p>
<h2 id="appendix-n-multi-threaded-implementation-via-ordered-state-merge-informative">Appendix
N: Multi-Threaded Implementation via Ordered State Merge
(Informative)</h2>
<p>This appendix describes a multi-threaded implementation strategy that
achieves parallel execution while preserving bitwise identity with the
single-threaded canonical compute sequence. A reference implementation
is available at <strong>[GB-x86-MT]</strong>.</p>
<h3 id="n.1-overview">N.1 Overview</h3>
<p>The single-threaded shift-reduce algorithm (§4.2) processes blocks
sequentially, maintaining a binary-counter stack that implicitly encodes
the canonical pairwise tree. To parallelize this while preserving
determinism, we observe that:</p>
<ol type="1">
<li>The stack state after processing any prefix of blocks is a complete
representation of that prefix’s reduction</li>
<li>Two stack states can be merged to produce the state that would
result from processing their blocks sequentially</li>
<li>Power-of-2 aligned partition boundaries ensure merges are
algebraically equivalent to sequential processing</li>
</ol>
<p>This leads to a three-phase algorithm: parallel local reduction,
deterministic merge, and final fold.</p>
<h3 id="n.2-stack-state-representation">N.2 Stack State
Representation</h3>
<p>An <strong>ordered reduction state</strong> summarizes a contiguous
range as an ordered sequence of fully reduced power-of-two blocks.</p>
<p>For a contiguous range <code>R</code>, the state may be viewed as a
sequence:</p>
<pre><code>[B0, B1, ..., Bm]</code></pre>
<p>where each <code>Bi</code> is the canonical reduction of a contiguous
subrange of <code>R</code>, the subranges are disjoint and appear in
strictly increasing stream order, each block size is <code>2^k</code>,
and the block sizes are strictly decreasing. The concatenation of these
subranges equals <code>R</code>.</p>
<p>The stack/bucket representation below is one concrete way to store
this ordered block sequence and to implement the canonical “coalesce
equal-sized adjacent blocks” rule used by shift-reduce.</p>
<p>A <strong>stack state</strong> compactly represents a partially
reduced sequence as a collection of buckets:</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span><span class="op">&lt;</span><span class="dt">size_t</span> L<span class="op">&gt;</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="kw">struct</span> StackState <span class="op">{</span></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">static</span> <span class="kw">constexpr</span> <span class="dt">size_t</span> MAX_DEPTH <span class="op">=</span> <span class="dv">32</span><span class="op">;</span></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span> buckets<span class="op">[</span>MAX_DEPTH<span class="op">][</span>L<span class="op">];</span>  <span class="co">// buckets[k] holds 2^k blocks worth of reduction</span></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>    <span class="dt">size_t</span> counts<span class="op">[</span>MAX_DEPTH<span class="op">];</span>      <span class="co">// lane counts (L for full, &lt;L for partial)</span></span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>    <span class="dt">uint32_t</span> mask<span class="op">;</span>                 <span class="co">// bit k set iff buckets[k] is occupied</span></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a><span class="op">};</span></span></code></pre></div>
<p><strong>Invariant:</strong> If <code>mask</code> has bits set at
positions {k₀, k₁, …, kₘ} where k₀ &lt; k₁ &lt; … &lt; kₘ, then the
state represents a prefix of length:</p>
<pre><code>num_blocks = 2^k₀ + 2^k₁ + ... + 2^kₘ</code></pre>
<p>Each <code>buckets[kᵢ]</code> holds the fully reduced L-lane vector
of a contiguous block of 2^kᵢ input blocks. <strong>Lower-indexed
buckets represent more recent (rightward) blocks; higher-indexed buckets
represent older (leftward) blocks.</strong></p>
<h3 id="n.3-the-push-operation">N.3 The Push Operation</h3>
<p>The push operation incorporates a new L-lane vector into the stack
state, implementing binary-counter carry propagation:</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> push<span class="op">(</span>StackState<span class="op">&amp;</span> S<span class="op">,</span> <span class="at">const</span> <span class="dt">double</span><span class="op">*</span> vec<span class="op">,</span> <span class="dt">size_t</span> count<span class="op">,</span> <span class="dt">size_t</span> level <span class="op">=</span> <span class="dv">0</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span> current<span class="op">[</span>L<span class="op">];</span></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>    copy<span class="op">(</span>vec<span class="op">,</span> current<span class="op">,</span> count<span class="op">);</span></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>    <span class="dt">size_t</span> current_count <span class="op">=</span> count<span class="op">;</span></span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="op">(</span>S<span class="op">.</span>mask <span class="op">&amp;</span> <span class="op">(</span><span class="dv">1</span><span class="bu">u</span> <span class="op">&lt;&lt;</span> level<span class="op">))</span> <span class="op">{</span></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Bucket exists: combine (older on left)</span></span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>        current <span class="op">=</span> combine<span class="op">(</span>S<span class="op">.</span>buckets<span class="op">[</span>level<span class="op">],</span> S<span class="op">.</span>counts<span class="op">[</span>level<span class="op">],</span> </span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a>                          current<span class="op">,</span> current_count<span class="op">);</span></span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a>        S<span class="op">.</span>mask <span class="op">&amp;=</span> <span class="op">~(</span><span class="dv">1</span><span class="bu">u</span> <span class="op">&lt;&lt;</span> level<span class="op">);</span>  <span class="co">// Clear bucket</span></span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a>        <span class="op">++</span>level<span class="op">;</span>                    <span class="co">// Carry to next level</span></span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a>    S<span class="op">.</span>buckets<span class="op">[</span>level<span class="op">]</span> <span class="op">=</span> current<span class="op">;</span></span>
<span id="cb62-15"><a href="#cb62-15" aria-hidden="true" tabindex="-1"></a>    S<span class="op">.</span>counts<span class="op">[</span>level<span class="op">]</span> <span class="op">=</span> current_count<span class="op">;</span></span>
<span id="cb62-16"><a href="#cb62-16" aria-hidden="true" tabindex="-1"></a>    S<span class="op">.</span>mask <span class="op">|=</span> <span class="op">(</span><span class="dv">1</span><span class="bu">u</span> <span class="op">&lt;&lt;</span> level<span class="op">);</span></span>
<span id="cb62-17"><a href="#cb62-17" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p><strong>Key property:</strong> Processing element i triggers carries
corresponding to the trailing 1-bits in the binary representation of i.
This exactly mirrors the canonical pairwise tree structure.</p>
<h3 id="n.4-the-fold-operation">N.4 The Fold Operation</h3>
<p>After all blocks are pushed, the stack is collapsed to a single
vector:</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="dt">double</span><span class="op">*</span> fold<span class="op">(</span><span class="at">const</span> StackState<span class="op">&amp;</span> S<span class="op">)</span> <span class="op">{</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span> acc<span class="op">[</span>L<span class="op">];</span></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>    <span class="dt">size_t</span> acc_count <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>    <span class="dt">bool</span> have <span class="op">=</span> <span class="kw">false</span><span class="op">;</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">// CRITICAL: Iterate low-to-high, bucket on LEFT</span></span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">size_t</span> k <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> k <span class="op">&lt;</span> MAX_DEPTH<span class="op">;</span> <span class="op">++</span>k<span class="op">)</span> <span class="op">{</span></span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="op">(!(</span>S<span class="op">.</span>mask <span class="op">&amp;</span> <span class="op">(</span><span class="dv">1</span><span class="bu">u</span> <span class="op">&lt;&lt;</span> k<span class="op">)))</span> <span class="cf">continue</span><span class="op">;</span></span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="op">(!</span>have<span class="op">)</span> <span class="op">{</span></span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>            acc <span class="op">=</span> S<span class="op">.</span>buckets<span class="op">[</span>k<span class="op">];</span></span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a>            acc_count <span class="op">=</span> S<span class="op">.</span>counts<span class="op">[</span>k<span class="op">];</span></span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a>            have <span class="op">=</span> <span class="kw">true</span><span class="op">;</span></span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span> <span class="cf">else</span> <span class="op">{</span></span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a>            <span class="co">// Higher bucket (older) goes on LEFT</span></span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a>            acc <span class="op">=</span> combine<span class="op">(</span>S<span class="op">.</span>buckets<span class="op">[</span>k<span class="op">],</span> S<span class="op">.</span>counts<span class="op">[</span>k<span class="op">],</span> acc<span class="op">,</span> acc_count<span class="op">);</span></span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb63-19"><a href="#cb63-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> acc<span class="op">;</span></span>
<span id="cb63-20"><a href="#cb63-20" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p><strong>Critical detail:</strong> The fold must iterate from low to
high indices, placing each bucket on the <strong>left</strong> of the
accumulator. This reconstructs the canonical tree’s final merges where
older (leftward) partial results combine with newer (rightward)
ones.</p>
<h3 id="n.5-power-of-2-aligned-partitioning">N.5 Power-of-2 Aligned
Partitioning</h3>
<p>For multi-threaded execution, we partition the block index space
among threads. <strong>The critical requirement is that partition
boundaries fall on power-of-2 aligned indices.</strong></p>
<p><strong>Definition:</strong> A block index b is <em>k-aligned</em> if
b is a multiple of 2^k.</p>
<p><strong>Observation:</strong> After processing blocks [0, b) where b
= m × 2^k, the stack state has no occupied buckets below level k.</p>
<p>The binary representation of b has zeros in positions 0 through k-1.
Since bucket[j] is occupied iff bit j is set in the count of processed
blocks, buckets 0 through k-1 are empty.</p>
<p><strong>Consequence:</strong> When thread boundaries are power-of-2
aligned, the “receiving” state A has no low-level buckets that could
collide incorrectly with the “incoming” state B’s buckets during
merge.</p>
<h3 id="n.6-partition-strategy">N.6 Partition Strategy</h3>
<p>Given B total blocks and T threads, choose chunk size C = 2^k where k
is the largest integer such that B / 2^k ≥ T:</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="dt">size_t</span> choose_chunk_size<span class="op">(</span><span class="dt">size_t</span> num_blocks<span class="op">,</span> <span class="dt">size_t</span> T<span class="op">)</span> <span class="op">{</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>num_blocks <span class="op">&lt;=</span> T<span class="op">)</span> <span class="cf">return</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>    <span class="dt">size_t</span> k <span class="op">=</span> bit_width<span class="op">(</span>num_blocks <span class="op">/</span> T<span class="op">)</span> <span class="op">-</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dt">size_t</span><span class="op">{</span><span class="dv">1</span><span class="op">}</span> <span class="op">&lt;&lt;</span> k<span class="op">;</span></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>This yields ceil(B / C) chunks, each containing C blocks (except
possibly the last). Thread t processes chunks assigned to it, producing
a local stack state for each.</p>
<p><strong>Example:</strong> For B = 1000 blocks and T = 4 threads, C =
256 (2^8):</p>
<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th>Thread</th>
<th>Chunks</th>
<th>Blocks</th>
<th>Output</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
<td>[0, 256)</td>
<td>Single bucket at level 8</td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>[256, 512)</td>
<td>Single bucket at level 8</td>
</tr>
<tr class="odd">
<td>2</td>
<td>2</td>
<td>[512, 768)</td>
<td>Single bucket at level 8</td>
</tr>
<tr class="even">
<td>3</td>
<td>3</td>
<td>[768, 1000)</td>
<td>Buckets at levels {7, 6, 5, 3} for remainder 232</td>
</tr>
</tbody>
</table>
<p>Full chunks produce exactly one bucket at level k (since 2^k blocks
reduce to one bucket). The remainder chunk (232 = 128 + 64 + 32 + 8)
naturally decomposes via shift-reduce into multiple buckets.</p>
<h3 id="n.7-the-merge-operation">N.7 The Merge Operation</h3>
<p>To combine two stack states where A represents an earlier (leftward)
portion of the sequence and B represents a later (rightward)
portion:</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> merge_into<span class="op">(</span>StackState<span class="op">&amp;</span> A<span class="op">,</span> <span class="at">const</span> StackState<span class="op">&amp;</span> B<span class="op">)</span> <span class="op">{</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Process B&#39;s buckets in increasing level order</span></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">size_t</span> k <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> k <span class="op">&lt;</span> MAX_DEPTH<span class="op">;</span> <span class="op">++</span>k<span class="op">)</span> <span class="op">{</span></span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="op">(</span>B<span class="op">.</span>mask <span class="op">&amp;</span> <span class="op">(</span><span class="dv">1</span><span class="bu">u</span> <span class="op">&lt;&lt;</span> k<span class="op">))</span> <span class="op">{</span></span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>            <span class="co">// Push B&#39;s bucket into A, starting at level k (NOT level 0!)</span></span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>            push<span class="op">(</span>A<span class="op">,</span> B<span class="op">.</span>buckets<span class="op">[</span>k<span class="op">],</span> B<span class="op">.</span>counts<span class="op">[</span>k<span class="op">],</span> k<span class="op">);</span></span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p><strong>Critical detail:</strong>
<code>push(A, B.buckets[k], B.counts[k], k)</code> starts at level k,
not level 0. This correctly reflects that B.buckets[k] represents 2^k
already-reduced blocks.</p>
<p><strong>Why low-to-high order:</strong> Within B’s stack state,
lower-indexed buckets represent more recent (rightward) elements within
B’s range. Processing them first ensures they combine before
higher-indexed (older) buckets, matching the canonical left-to-right
order.</p>
<h3 id="n.8-correctness-argument">N.8 Correctness argument</h3>
<p><strong>Claim:</strong> The multi-threaded algorithm produces a
result bitwise identical to single-threaded shift-reduce for any number
of threads T ≥ 1 and any input size N.</p>
<p><em>Argument:</em></p>
<ol type="1">
<li><p><strong>Shift-reduce correctness:</strong> Single-threaded
shift-reduce evaluates the canonical pairwise-with-carry tree. The push
operation’s carry pattern exactly mirrors binary increment,
corresponding to completing subtrees of size 2^k.</p></li>
<li><p><strong>Alignment property:</strong> For a prefix of length b = m
× 2^k blocks, the stack state has no occupied buckets below level k (see
observation above).</p></li>
<li><p><strong>Merge correctness:</strong> <code>merge_into(A, B)</code>
produces the same state as processing A’s blocks followed by B’s blocks
sequentially. By the alignment property, when B starts at an aligned
boundary, A has no buckets below the alignment level. B’s buckets, when
pushed at their native levels, trigger exactly the carries that would
occur from processing B’s blocks after A’s blocks. The low-to-high
iteration order preserves within-B ordering.</p></li>
<li><p><strong>Combining these:</strong> Thread boundaries are aligned,
so merge correctness applies to each merge. The left-to-right merge
order (thread 0, then 1, then 2, …) matches sequential block
order.</p></li>
</ol>
<h3 id="n.9-complete-algorithm">N.9 Complete Algorithm</h3>
<div class="sourceCode" id="cb66"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span><span class="op">&lt;</span><span class="dt">size_t</span> L<span class="op">&gt;</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="dt">double</span> deterministic_reduce_MT<span class="op">(</span><span class="at">const</span> <span class="dt">double</span><span class="op">*</span> input<span class="op">,</span> <span class="dt">size_t</span> N<span class="op">,</span> <span class="dt">size_t</span> T<span class="op">)</span> <span class="op">{</span></span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="dt">size_t</span> num_blocks <span class="op">=</span> <span class="op">(</span>N <span class="op">+</span> L <span class="op">-</span> <span class="dv">1</span><span class="op">)</span> <span class="op">/</span> L<span class="op">;</span></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="dt">size_t</span> C <span class="op">=</span> choose_chunk_size<span class="op">(</span>num_blocks<span class="op">,</span> T<span class="op">);</span></span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="dt">size_t</span> num_chunks <span class="op">=</span> <span class="op">(</span>num_blocks <span class="op">+</span> C <span class="op">-</span> <span class="dv">1</span><span class="op">)</span> <span class="op">/</span> C<span class="op">;</span></span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a>    vector<span class="op">&lt;</span>StackState<span class="op">&lt;</span>L<span class="op">&gt;&gt;</span> states<span class="op">(</span>num_chunks<span class="op">);</span></span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Phase 1: Parallel local reductions</span></span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a>    parallel_for<span class="op">(</span><span class="dv">0</span><span class="op">,</span> num_chunks<span class="op">,</span> <span class="op">[&amp;](</span><span class="dt">size_t</span> chunk<span class="op">)</span> <span class="op">{</span></span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a>        <span class="dt">size_t</span> b0 <span class="op">=</span> chunk <span class="op">*</span> C<span class="op">;</span></span>
<span id="cb66-12"><a href="#cb66-12" aria-hidden="true" tabindex="-1"></a>        <span class="dt">size_t</span> b1 <span class="op">=</span> min<span class="op">(</span>b0 <span class="op">+</span> C<span class="op">,</span> num_blocks<span class="op">);</span></span>
<span id="cb66-13"><a href="#cb66-13" aria-hidden="true" tabindex="-1"></a>        states<span class="op">[</span>chunk<span class="op">]</span> <span class="op">=</span> replay_range<span class="op">(</span>input<span class="op">,</span> N<span class="op">,</span> b0<span class="op">,</span> b1<span class="op">);</span></span>
<span id="cb66-14"><a href="#cb66-14" aria-hidden="true" tabindex="-1"></a>    <span class="op">});</span></span>
<span id="cb66-15"><a href="#cb66-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb66-16"><a href="#cb66-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Phase 2: Serial merge (left-to-right order)</span></span>
<span id="cb66-17"><a href="#cb66-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">size_t</span> i <span class="op">=</span> <span class="dv">1</span><span class="op">;</span> i <span class="op">&lt;</span> num_chunks<span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb66-18"><a href="#cb66-18" aria-hidden="true" tabindex="-1"></a>        merge_into<span class="op">(</span>states<span class="op">[</span><span class="dv">0</span><span class="op">],</span> states<span class="op">[</span>i<span class="op">]);</span></span>
<span id="cb66-19"><a href="#cb66-19" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb66-20"><a href="#cb66-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb66-21"><a href="#cb66-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Phase 3: Final fold + cross-lane reduction</span></span>
<span id="cb66-22"><a href="#cb66-22" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span><span class="op">*</span> lane_result <span class="op">=</span> fold<span class="op">(</span>states<span class="op">[</span><span class="dv">0</span><span class="op">]);</span></span>
<span id="cb66-23"><a href="#cb66-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cross_lane_pairwise_reduce<span class="op">(</span>lane_result<span class="op">);</span></span>
<span id="cb66-24"><a href="#cb66-24" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<h3 id="n.10-complexity-analysis">N.10 Complexity Analysis</h3>
<p>The semantics do not require parallel scalability. This section
explains that the canonical expression <em>admits</em> scalable
multi-threaded realizations.</p>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 17%" />
<col style="width: 62%" />
</colgroup>
<thead>
<tr class="header">
<th>Phase</th>
<th>Work</th>
<th>Span (critical path)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Local reduction</td>
<td><code>O(N)</code> total</td>
<td><code>O(N/T)</code></td>
</tr>
<tr class="even">
<td>Ordered merge (sequential)</td>
<td><code>O(T · log N)</code></td>
<td><code>O(T · log N)</code></td>
</tr>
<tr class="odd">
<td>Ordered merge (tree-structured)</td>
<td><code>O(T · log N)</code></td>
<td><code>O(log T · log N)</code></td>
</tr>
<tr class="even">
<td>Final fold</td>
<td><code>O(log N)</code></td>
<td><code>O(log N)</code></td>
</tr>
</tbody>
</table>
<p><strong>Total work:</strong> <code>O(N + T·log N)</code> (typically
<code>O(N)</code> when <code>T &lt;&lt; N</code>)</p>
<p><strong>Span:</strong> With a tree-structured merge,
<code>O(N/T + log T · log N)</code>.</p>
<p><strong>Space:</strong> <code>O(T · L · log N)</code> for per-thread
states (typically a few KB per thread, depending on <code>L</code>).</p>
<h3 id="n.11-simd-optimization-8-block-unrolling">N.11 SIMD
Optimization: 8-Block Unrolling</h3>
<p>A significant optimization reduces stack operation overhead by
processing 8 blocks at once:</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Instead of pushing one block at a time:</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">size_t</span> b <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> b <span class="op">&lt;</span> num_blocks<span class="op">;</span> <span class="op">++</span>b<span class="op">)</span> <span class="op">{</span></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>    push<span class="op">(</span>S<span class="op">,</span> load_block<span class="op">(</span>b<span class="op">),</span> L<span class="op">,</span> <span class="dv">0</span><span class="op">);</span>  <span class="co">// O(num_blocks) stack operations</span></span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a><span class="co">// Process 8 blocks in one SIMD reduction, push at level 3:</span></span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">size_t</span> g <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> g <span class="op">&lt;</span> num_groups_of_8<span class="op">;</span> <span class="op">++</span>g<span class="op">)</span> <span class="op">{</span></span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span> result<span class="op">[</span>L<span class="op">];</span></span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>    reduce_8_blocks_simd<span class="op">(</span>input <span class="op">+</span> g <span class="op">*</span> <span class="dv">8</span> <span class="op">*</span> L<span class="op">,</span> result<span class="op">);</span>  <span class="co">// 8 blocks → 1 vector</span></span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a>    push<span class="op">(</span>S<span class="op">,</span> result<span class="op">,</span> L<span class="op">,</span> <span class="dv">3</span><span class="op">);</span>  <span class="co">// Start at level 3 (since 8 = 2^3)</span></span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>This reduces stack operations from N/L to N/(8L), yielding
substantial performance improvements. The reference implementation
achieves throughput exceeding <code>std::reduce</code> while maintaining
full determinism.</p>
<h3 id="n.12-performance-observations">N.12 Performance
Observations</h3>
<p>The reference implementation at <strong>[GB-x86-MT]</strong>
demonstrates:</p>
<table>
<colgroup>
<col style="width: 21%" />
<col style="width: 28%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="header">
<th>Variant</th>
<th>Throughput</th>
<th>vs <code>std::accumulate</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>std::accumulate</code></td>
<td>5.4 GB/s</td>
<td>baseline</td>
</tr>
<tr class="even">
<td><code>std::reduce</code></td>
<td>21.4 GB/s</td>
<td>+297%</td>
</tr>
<tr class="odd">
<td><strong>Deterministic ST (L=16, 8-block unroll)</strong></td>
<td><strong>26.5 GB/s</strong></td>
<td><strong>+391%</strong></td>
</tr>
<tr class="even">
<td>Deterministic MT (L=16, T=2)</td>
<td>21.2 GB/s</td>
<td>+293%</td>
</tr>
</tbody>
</table>
<p><strong>Key finding:</strong> With proper SIMD optimization,
deterministic reduction is <strong>faster</strong> than
non-deterministic <code>std::reduce</code> while guaranteeing bitwise
reproducibility. The 8-block unrolling minimizes stack overhead, and the
interleaved lane structure enables efficient vectorized combines.</p>
<h3 id="n.13-implementation-notes">N.13 Implementation Notes</h3>
<p><strong>Thread pool reuse:</strong> For production implementations,
reuse a thread pool rather than spawning threads per invocation. The
reference limits thread creation for Godbolt compatibility.</p>
<p><strong>Parallel merge (optional):</strong> The merge phase can
itself be parallelized using a tree structure:</p>
<pre><code>Initial:    +---+ +---+ +---+ +---+ +---+ +---+ +---+ +---+
            |S₀ | |S₁ | |S₂ | |S₃ | |S₄ | |S₅ | |S₆ | |S₇ |
            +-+-+ +-+-+ +-+-+ +-+-+ +-+-+ +-+-+ +-+-+ +-+-+
              |     |     |     |     |     |     |     |
              +-----+     +-----+     +-----+     +-----+
                |           |           |           |
Stride 1:     +---+       +---+       +---+       +---+
              |S₀₁|       |S₂₃|       |S₄₅|       |S₆₇|  (parallel)
              +-+-+       +-+-+       +-+-+       +-+-+
                |           |           |           |
                +-----------+           +-----------+
                    |                       |
Stride 2:         +-------+               +-------+
                  |S₀₁₂₃  |               |S₄₅₆₇  |      (parallel)
                  +---+---+               +---+---+
                      |                       |
                      +-----------------------+
                              |
Stride 4:                 +-----------+
                          |S₀₁₂₃₄₅₆₇  |                    (final)
                          +-----------+</code></pre>
<p>This reduces the merge critical path from O(T) to O(log T), though
for typical T values the improvement is marginal compared to the
dominant O(N/T) local reduction phase.</p>
<p><strong>Memory efficiency:</strong> Stack states are fixed-size (O(L
× log N) per state) with no heap allocation required during the hot
path. The merge operation modifies A in-place, requiring only
register-level temporaries for the carry chain.</p>
<h3 id="n.14-summary">N.14 Summary</h3>
<p>The multi-threaded stack ordered state merge algorithm achieves:</p>
<table>
<thead>
<tr class="header">
<th>Property</th>
<th>Guarantee</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Determinism</strong></td>
<td>Bitwise identical to single-threaded for any T</td>
</tr>
<tr class="even">
<td><strong>Correctness</strong></td>
<td>Provably equivalent to canonical pairwise tree</td>
</tr>
<tr class="odd">
<td><strong>Efficiency</strong></td>
<td>O(N) work, O(N/T + T log N) span</td>
</tr>
<tr class="even">
<td><strong>Practicality</strong></td>
<td>Demonstrated faster than <code>std::reduce</code> with SIMD</td>
</tr>
</tbody>
</table>
<p>The key insights enabling this approach are:</p>
<ol type="1">
<li>Power-of-2 aligned partitioning eliminates boundary ambiguity</li>
<li>Stack states are complete representations of partial reductions</li>
<li>Pushing at native levels during merge preserves the canonical tree
structure</li>
<li>8-block SIMD unrolling amortizes stack overhead</li>
</ol>
<p>This demonstrates that deterministic parallel reduction is not merely
<em>feasible</em> but can be <em>optimal</em> — the constraint of a
fixed evaluation order, far from being a performance liability, enables
aggressive SIMD optimization that outperforms unconstrained
implementations.</p>
<h2 id="appendix-x-recursive-bisection-balanced-tree-construction-informative">Appendix
X: Recursive Bisection (“Balanced”) Tree Construction (Informative)</h2>
<p>This appendix records a previously-considered alternative canonical
tree construction based on recursive bisection. It is
<strong>not</strong> part of the normative contract in §4; this paper
specifies iterative pairwise (§4.2.3) as the sole canonical tree
definition.</p>
<h3 id="x.1-definition">X.1 Definition</h3>
<p>Define <code>CANONICAL_TREE_EVAL_RECURSIVE(op, Y[0..k))</code>, where
<code>k &gt;= 1</code> and each <code>Y[t]</code> is in
<code>maybe&lt;A&gt;</code>:</p>
<pre><code>CANONICAL_TREE_EVAL_RECURSIVE(op, Y[0..k)):
    if k == 1:
        return Y[0]
    let m = floor(k / 2)
    return COMBINE(op,
        CANONICAL_TREE_EVAL_RECURSIVE(op, Y[0..m)),
        CANONICAL_TREE_EVAL_RECURSIVE(op, Y[m..k))
    )</code></pre>
<h3 id="x.2-equivalence-on-power-of-two-sizes">X.2 Equivalence on
power-of-two sizes</h3>
<p>For <code>k = 2^n</code> (k = 2, 4, 8, …), recursive bisection and
iterative pairwise produce identical trees, and therefore define
identical abstract expressions.</p>
<h3 id="x.3-differences-on-non-power-of-two-sizes">X.3 Differences on
non-power-of-two sizes</h3>
<p>For non-power-of-two <code>k</code>, the trees differ in structure
(but maintain the same asymptotic depth bounds).</p>
<p>Example (<code>k = 5</code>), writing <code>+</code> for
<code>op</code>:</p>
<ul>
<li>Recursive bisection:
<ul>
<li><code>(e0 + e1) + (e2 + (e3 + e4))</code></li>
</ul></li>
<li>Iterative pairwise:
<ul>
<li><code>((e0 + e1) + (e2 + e3)) + e4</code></li>
</ul></li>
</ul>
<h3 id="x.4-rationale-for-selecting-iterative-pairwise">X.4 Rationale
for selecting iterative pairwise</h3>
<p>This paper selects iterative pairwise as the sole canonical tree
definition for the following reasons:</p>
<ul>
<li><strong>Industry alignment:</strong> SIMD/GPU deterministic
reductions commonly use adjacent pairwise combination patterns.</li>
<li><strong>Direct hardware mapping:</strong> iterative pairing
corresponds directly to SIMD-lane and warp-lane pairing operations.</li>
<li><strong>Adoption continuity:</strong> existing deterministic
implementations can conform without structural change.</li>
<li><strong>Specification focus:</strong> once a single canonical tree
is selected, retaining alternatives in the normative contract increases
complexity without adding semantic value.</li>
</ul>
<p>No other grouping is permitted.</p>
</body>
</html>
