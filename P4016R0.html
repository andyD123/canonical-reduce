<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang xml:lang>
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>P4016R0 — Canonical Parallel Reduction: A Fixed Expression Structure for Run-To-Run Consistency</title>
  <style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}

ul.task-list[class]{list-style: none;}
ul.task-list li input[type="checkbox"] {
font-size: inherit;
width: 0.8em;
margin: 0 0.8em 0.2em -1.6em;
vertical-align: middle;
}
.display.math{display: block; text-align: center; margin: 0.5rem auto;}

pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
  <style type="text/css">
body {
font-family: 'DejaVu Serif', 'Times New Roman', Georgia, serif;
max-width: 52em;
margin: 2em auto;
padding: 0 1.5em;
line-height: 1.55;
color: #222;
font-size: 14px;
}
h1, h2, h3, h4, h5, h6 {
font-family: 'DejaVu Sans', 'Helvetica Neue', Arial, sans-serif;
color: #005A9C;
margin-top: 1.8em;
margin-bottom: 0.5em;
line-height: 1.25;
}
h1 { font-size: 1.7em; border-bottom: 2px solid #005A9C; padding-bottom: 0.3em; }
h2 { font-size: 1.4em; border-bottom: 1px solid #ccc; padding-bottom: 0.2em; }
h3 { font-size: 1.15em; }
h4 { font-size: 1.05em; }
code, pre {
font-family: 'DejaVu Sans Mono', 'Consolas', 'Courier New', monospace;
font-size: 0.92em;
}
pre {
background: #f5f5f5;
border: 1px solid #ddd;
border-radius: 3px;
padding: 0.8em 1em;
overflow-x: auto;
line-height: 1.4;
}
code {
background: #f0f0f0;
padding: 0.15em 0.35em;
border-radius: 2px;
}
pre code {
background: none;
padding: 0;
border-radius: 0;
}
table {
border-collapse: collapse;
margin: 1em 0;
width: 100%;
font-size: 0.93em;
}
th, td {
border: 1px solid #bbb;
padding: 0.45em 0.7em;
text-align: left;
vertical-align: top;
}
th {
background: #e8e8e8;
font-family: 'DejaVu Sans', 'Helvetica Neue', Arial, sans-serif;
font-weight: bold;
}
tr:nth-child(even) { background: #f9f9f9; }
blockquote {
border-left: 3px solid #005A9C;
margin: 1em 0;
padding: 0.5em 1em;
background: #f0f4f8;
color: #333;
}
a { color: #005A9C; text-decoration: none; }
a:hover { text-decoration: underline; }
hr { border: none; border-top: 1px solid #ccc; margin: 2em 0; }

nav#TOC {
background: #f8f9fa;
border: 1px solid #ddd;
border-radius: 4px;
padding: 1em 1.5em;
margin: 1.5em 0 2em 0;
page-break-after: always;
}
nav#TOC > ul { margin: 0; padding-left: 0; list-style: none; }
nav#TOC ul { padding-left: 1.2em; }
nav#TOC li { margin: 0.2em 0; line-height: 1.4; }
nav#TOC a { text-decoration: none; color: #333; }
nav#TOC a:hover { color: #005A9C; text-decoration: underline; }

.metadata-block p {
margin: 0.15em 0;
line-height: 1.4;
}

@media print {
body { max-width: none; margin: 0; padding: 0 1cm; font-size: 11pt; }
nav#TOC { page-break-after: always; }
h2, h3 { page-break-after: avoid; }
pre { page-break-inside: avoid; font-size: 9pt; }
table { page-break-inside: avoid; font-size: 9pt; }
a { color: #005A9C; }
}
</style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Canonical Parallel Reduction: A Fixed Expression
Structure for Run-To-Run Consistency</h1>
</header>

<p><strong>Document:</strong> P4016R0<br />
<strong>Date:</strong> 2026-02-19<br />
<strong>Reply-To:</strong> Andrew Drakeford <a href="mailto:andreedrakeford@hotmail.com" class="email">andreedrakeford@hotmail.com</a><br />
<strong>Audience:</strong> SG6 (Numerics), LEWG, SG1 (Concurrency), SG14
(Low Latency/Games/Embedded/Financial Trading)</p>
<h2 id="toc-heading">Table of Contents</h2>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#abstract" id="toc-abstract">Abstract</a></li>
<li><a href="#executive-overview" id="toc-executive-overview">0.
Executive Overview</a></li>
<li><a href="#the-semantic-gap-in-c-reduction-facilities" id="toc-the-semantic-gap-in-c-reduction-facilities">1. The Semantic Gap
in C++ Reduction Facilities</a>
<ul>
<li><a href="#why-this-needs-to-be-in-the-standard-library" id="toc-why-this-needs-to-be-in-the-standard-library">1.1 Why This Needs
to Be in the Standard Library</a></li>
<li><a href="#motivating-example-informative" id="toc-motivating-example-informative">1.2 Motivating example
(informative)</a></li>
<li><a href="#determinism-via-expression-ownership-in-existing-practice" id="toc-determinism-via-expression-ownership-in-existing-practice">1.3
Determinism via Expression Ownership in Existing Practice</a></li>
</ul></li>
<li><a href="#scope-and-non-goals" id="toc-scope-and-non-goals">2. Scope
and Non-Goals</a>
<ul>
<li><a href="#opt-in-reproducibility-and-performance-trade-off-informative" id="toc-opt-in-reproducibility-and-performance-trade-off-informative">2.1
Opt-in reproducibility and performance trade-off (informative)</a></li>
<li><a href="#traversal-and-sizing-requirements-informative" id="toc-traversal-and-sizing-requirements-informative">2.2 Traversal and
sizing requirements (informative)</a></li>
<li><a href="#exception-safety" id="toc-exception-safety">2.3 Exception
safety</a></li>
<li><a href="#complexity" id="toc-complexity">2.4 Complexity</a></li>
<li><a href="#expression-identity-vs.-bitwise-identity" id="toc-expression-identity-vs.-bitwise-identity">2.5 Expression
identity vs. bitwise identity</a></li>
<li><a href="#constant-evaluation" id="toc-constant-evaluation">2.6
Constant evaluation</a></li>
<li><a href="#freestanding" id="toc-freestanding">2.7
Freestanding</a></li>
</ul></li>
<li><a href="#design-space-informative" id="toc-design-space-informative">3. Design Space (Informative)</a>
<ul>
<li><a href="#expression-algorithm-and-execution" id="toc-expression-algorithm-and-execution">3.0 Expression, Algorithm,
and Execution</a></li>
<li><a href="#why-not-left-fold" id="toc-why-not-left-fold">3.1 Why Not
Left-Fold?</a></li>
<li><a href="#why-not-blocked-decomposition" id="toc-why-not-blocked-decomposition">3.2 Why Not Blocked
Decomposition?</a></li>
<li><a href="#why-not-a-fixed-standard-constant" id="toc-why-not-a-fixed-standard-constant">3.3 Why Not a Fixed Standard
Constant?</a></li>
<li><a href="#why-not-implementation-defined" id="toc-why-not-implementation-defined">3.4 Why Not
Implementation-Defined?</a></li>
<li><a href="#the-proposed-design" id="toc-the-proposed-design">3.5 The
proposed design</a></li>
<li><a href="#industry-precedents-for-constrained-computation-structure" id="toc-industry-precedents-for-constrained-computation-structure">3.6
Industry precedents for constrained computation structure</a></li>
<li><a href="#the-structural-necessity-of-a-new-algorithm" id="toc-the-structural-necessity-of-a-new-algorithm">3.7 The structural
necessity of a new algorithm</a></li>
<li><a href="#rationale-for-tree-shape-choice" id="toc-rationale-for-tree-shape-choice">3.8 Rationale for tree shape
choice</a></li>
<li><a href="#deferral-of-api-surface" id="toc-deferral-of-api-surface">3.9 Deferral of API surface</a></li>
<li><a href="#relationship-to-executors" id="toc-relationship-to-executors">3.10 Relationship to
Executors</a></li>
</ul></li>
<li><a href="#fixed-expression-structure-canonical-reduction-expression-canonical.reduce" id="toc-fixed-expression-structure-canonical-reduction-expression-canonical.reduce">4.
Fixed Expression Structure (Canonical Reduction Expression)
[canonical.reduce]</a>
<ul>
<li><a href="#canonical-tree-shape-canonical.reduce.tree" id="toc-canonical-tree-shape-canonical.reduce.tree">4.1 Canonical tree
shape [canonical.reduce.tree]</a></li>
<li><a href="#canonical-tree-reduction-with-absent-operands-canonical.reduce.tree.absent" id="toc-canonical-tree-reduction-with-absent-operands-canonical.reduce.tree.absent">4.2
Canonical tree reduction with absent operands
[canonical.reduce.tree.absent]</a></li>
<li><a href="#interleaved-topology-canonical.reduce.interleaved" id="toc-interleaved-topology-canonical.reduce.interleaved">4.3
Interleaved topology [canonical.reduce.interleaved]</a></li>
<li><a href="#two-stage-reduction-semantics-canonical.reduce.twostage" id="toc-two-stage-reduction-semantics-canonical.reduce.twostage">4.4
Two-stage reduction semantics [canonical.reduce.twostage]</a></li>
<li><a href="#integration-of-an-initial-value-if-provided-canonical.reduce.init" id="toc-integration-of-an-initial-value-if-provided-canonical.reduce.init">4.5
Integration of an initial value (if provided)
[canonical.reduce.init]</a></li>
<li><a href="#materialization-of-conceptual-terms-and-reduction-state-introducing-v-and-a-canonical.reduce.types" id="toc-materialization-of-conceptual-terms-and-reduction-state-introducing-v-and-a-canonical.reduce.types">4.6
Materialization of conceptual terms and reduction state (introducing V
and A) [canonical.reduce.types]</a></li>
</ul></li>
<li><a href="#invariance-properties-canonical.reduce.invariance" id="toc-invariance-properties-canonical.reduce.invariance">5. Invariance
Properties [canonical.reduce.invariance]</a></li>
<li><a href="#floating-point-considerations-informative" id="toc-floating-point-considerations-informative">6. Floating-Point
Considerations (Informative)</a></li>
<li><a href="#relationship-to-existing-facilities-informative" id="toc-relationship-to-existing-facilities-informative">7. Relationship
to Existing Facilities (Informative)</a></li>
<li><a href="#motivation-and-use-cases-informative" id="toc-motivation-and-use-cases-informative">8. Motivation and Use
Cases (Informative)</a></li>
<li><a href="#api-design-space-informative" id="toc-api-design-space-informative">9. API Design Space
(Informative)</a>
<ul>
<li><a href="#new-algorithm-approach-illustrative" id="toc-new-algorithm-approach-illustrative">9.1 New Algorithm Approach
(Illustrative)</a></li>
<li><a href="#rationale-why-lane-count-l-is-the-sole-coordinate-the-portability-trap" id="toc-rationale-why-lane-count-l-is-the-sole-coordinate-the-portability-trap">9.2
Rationale: Why Lane Count (L) is the Sole Coordinate (The Portability
Trap)</a></li>
<li><a href="#execution-policy-approach-illustrative" id="toc-execution-policy-approach-illustrative">9.3 Execution Policy
Approach (Illustrative)</a></li>
<li><a href="#trade-offs-informative" id="toc-trade-offs-informative">9.4 Trade-offs (Informative)</a></li>
<li><a href="#naming-considerations-informative" id="toc-naming-considerations-informative">9.5 Naming Considerations
(Informative)</a></li>
<li><a href="#topology-defaults-and-named-presets-informative" id="toc-topology-defaults-and-named-presets-informative">9.6 Topology
Defaults and Named Presets (Informative)</a></li>
</ul></li>
<li><a href="#what-lewg-is-being-asked-to-agree-to" id="toc-what-lewg-is-being-asked-to-agree-to">10. What LEWG Is Being
Asked to Agree To</a></li>
<li><a href="#polls-for-lewg-direction" id="toc-polls-for-lewg-direction">11. Polls for LEWG Direction</a>
<ul>
<li><a href="#poll-1a-problem-validation" id="toc-poll-1a-problem-validation">Poll 1A — Problem
validation</a></li>
<li><a href="#poll-1b-semantics-first-scope" id="toc-poll-1b-semantics-first-scope">Poll 1B — Semantics-first
scope</a></li>
<li><a href="#poll-2-core-semantic-contract" id="toc-poll-2-core-semantic-contract">Poll 2 — Core semantic
contract</a></li>
<li><a href="#poll-2a-canonical-tree-shape" id="toc-poll-2a-canonical-tree-shape">Poll 2A — Canonical tree
shape</a></li>
<li><a href="#poll-3-proceed-to-api-surface" id="toc-poll-3-proceed-to-api-surface">Poll 3 — Proceed to API
surface</a></li>
<li><a href="#poll-4a-straw-poll-api-approach-new-algorithm" id="toc-poll-4a-straw-poll-api-approach-new-algorithm">Poll 4A — Straw
poll: API approach (new algorithm)</a></li>
<li><a href="#poll-4b-straw-poll-api-approach-execution-policy" id="toc-poll-4b-straw-poll-api-approach-execution-policy">Poll 4B —
Straw poll: API approach (execution policy)</a></li>
</ul></li>
<li><a href="#acknowledgements" id="toc-acknowledgements">Acknowledgements</a></li>
<li><a href="#references" id="toc-references">References</a>
<ul>
<li><a href="#c-standard-references" id="toc-c-standard-references">C++
Standard References</a></li>
<li><a href="#wg21-papers" id="toc-wg21-papers">WG21 Papers</a></li>
<li><a href="#industry-references" id="toc-industry-references">Industry
References</a></li>
<li><a href="#academic-references" id="toc-academic-references">Academic
References</a></li>
</ul></li>
<li><a href="#appendix-a-illustrative-wording-informative" id="toc-appendix-a-illustrative-wording-informative">Appendix A:
Illustrative Wording (Informative)</a>
<ul>
<li><a href="#a.1-example-algorithm-specification" id="toc-a.1-example-algorithm-specification">A.1 Example Algorithm
Specification</a></li>
<li><a href="#a.2-semantics-as-if" id="toc-a.2-semantics-as-if">A.2
Semantics (as-if)</a></li>
</ul></li>
<li><a href="#appendix-b-implementation-guidance-informative" id="toc-appendix-b-implementation-guidance-informative">Appendix B:
Implementation Guidance (Informative)</a>
<ul>
<li><a href="#b.1-two-degrees-of-parallelism" id="toc-b.1-two-degrees-of-parallelism">B.1 Two Degrees of
Parallelism</a></li>
<li><a href="#b.2-the-efficient-simd-pattern" id="toc-b.2-the-efficient-simd-pattern">B.2 The Efficient SIMD
Pattern</a></li>
<li><a href="#b.3-thread-level-partitioning" id="toc-b.3-thread-level-partitioning">B.3 Thread-Level
Partitioning</a></li>
<li><a href="#b.4-gpu-implementation" id="toc-b.4-gpu-implementation">B.4 GPU Implementation</a></li>
<li><a href="#b.5-cross-platform-consistency" id="toc-b.5-cross-platform-consistency">B.5 Cross-Platform
Consistency</a></li>
<li><a href="#b.6-when-physical-width-logical-width" id="toc-b.6-when-physical-width-logical-width">B.6 When Physical Width ≠
Logical Width</a></li>
<li><a href="#b.7-performance-characteristics" id="toc-b.7-performance-characteristics">B.7 Performance
Characteristics</a></li>
</ul></li>
<li><a href="#appendix-c-prior-art-and-industry-practice-informative" id="toc-appendix-c-prior-art-and-industry-practice-informative">Appendix
C: Prior Art and Industry Practice (Informative)</a>
<ul>
<li><a href="#c.1-kokkos-ecosystem-hpc-portability" id="toc-c.1-kokkos-ecosystem-hpc-portability">C.1 Kokkos Ecosystem (HPC
Portability)</a></li>
<li><a href="#c.2-intel-oneapi-threading-building-blocks-onetbb" id="toc-c.2-intel-oneapi-threading-building-blocks-onetbb">C.2 Intel
oneAPI Threading Building Blocks (oneTBB)</a></li>
<li><a href="#c.3-nvidia-thrust-and-cub" id="toc-c.3-nvidia-thrust-and-cub">C.3 NVIDIA Thrust and CUB</a></li>
<li><a href="#c.4-academic-and-research-solutions" id="toc-c.4-academic-and-research-solutions">C.4 Academic and Research
Solutions</a></li>
</ul></li>
<li><a href="#appendix-d-standard-wording-for-sequential-evaluation-order-informative" id="toc-appendix-d-standard-wording-for-sequential-evaluation-order-informative">Appendix
D: Standard Wording for Sequential Evaluation Order (Informative)</a>
<ul>
<li><a href="#d.1-stdaccumulate" id="toc-d.1-stdaccumulate">D.1
std::accumulate</a></li>
<li><a href="#d.2-stdrangesfold_left-c23" id="toc-d.2-stdrangesfold_left-c23">D.2 std::ranges::fold_left
(C++23)</a></li>
<li><a href="#d.3-sequential-vs.-parallel-contrasting-guarantees" id="toc-d.3-sequential-vs.-parallel-contrasting-guarantees">D.3
Sequential vs. Parallel: Contrasting Guarantees</a></li>
<li><a href="#d.4-init-placement-comparison-with-stdaccumulate" id="toc-d.4-init-placement-comparison-with-stdaccumulate">D.4 Init
Placement: Comparison with std::accumulate</a></li>
<li><a href="#d.5-important-caveat-evaluation-order-bitwise-identity" id="toc-d.5-important-caveat-evaluation-order-bitwise-identity">D.5
Important Caveat: Evaluation Order ≠ Bitwise Identity</a></li>
<li><a href="#d.6-why-compilers-cannot-reassociate-mandated-evaluation-order" id="toc-d.6-why-compilers-cannot-reassociate-mandated-evaluation-order">D.6
Why Compilers Cannot Reassociate Mandated Evaluation Order</a></li>
</ul></li>
<li><a href="#appendix-e-init-placement-rationale-informative" id="toc-appendix-e-init-placement-rationale-informative">Appendix E:
Init Placement Rationale (Informative)</a>
<ul>
<li><a href="#e.1-design-options-considered" id="toc-e.1-design-options-considered">E.1 Design Options
Considered</a></li>
<li><a href="#e.2-analysis" id="toc-e.2-analysis">E.2 Analysis</a></li>
<li><a href="#e.3-why-option-a-was-chosen" id="toc-e.3-why-option-a-was-chosen">E.3 Why Option A Was
Chosen</a></li>
<li><a href="#e.4-why-not-treat-init-as-element-0" id="toc-e.4-why-not-treat-init-as-element-0">E.4 Why Not Treat init as
Element 0?</a></li>
<li><a href="#e.5-migration-path-for-users-expecting-accumulate-style-semantics" id="toc-e.5-migration-path-for-users-expecting-accumulate-style-semantics">E.5
Migration Path for Users Expecting accumulate-style Semantics</a></li>
</ul></li>
<li><a href="#appendix-f-design-evolution-informative" id="toc-appendix-f-design-evolution-informative">Appendix F: Design
Evolution (Informative)</a>
<ul>
<li><a href="#f.1-core-design-decisions" id="toc-f.1-core-design-decisions">F.1 Core Design Decisions</a></li>
<li><a href="#f.2-key-design-iterations" id="toc-f.2-key-design-iterations">F.2 Key Design Iterations</a></li>
<li><a href="#f.3-industry-context" id="toc-f.3-industry-context">F.3
Industry Context</a></li>
</ul></li>
<li><a href="#appendix-g-detailed-design-rationale-informative" id="toc-appendix-g-detailed-design-rationale-informative">Appendix G:
Detailed Design Rationale (Informative)</a>
<ul>
<li><a href="#g.1-lane-count-and-hardware-alignment" id="toc-g.1-lane-count-and-hardware-alignment">G.1 Lane Count and
Hardware Alignment</a></li>
<li><a href="#g.2-why-interleaved-topology-supports-efficient-simd" id="toc-g.2-why-interleaved-topology-supports-efficient-simd">G.2 Why
Interleaved Topology Supports Efficient SIMD</a></li>
<li><a href="#g.3-information-density-and-spatial-locality" id="toc-g.3-information-density-and-spatial-locality">G.3 Information
Density and Spatial Locality</a></li>
<li><a href="#g.4-cross-architecture-expression-parity" id="toc-g.4-cross-architecture-expression-parity">G.4 Cross-Architecture
Expression-Parity</a></li>
<li><a href="#g.5-the-golden-reference-l-1" id="toc-g.5-the-golden-reference-l-1">G.5 The Golden Reference (L =
1)</a></li>
<li><a href="#g.6-divergence-from-stdaccumulate" id="toc-g.6-divergence-from-stdaccumulate">G.6 Divergence from
std::accumulate</a></li>
<li><a href="#g.7-init-placement-determinism" id="toc-g.7-init-placement-determinism">G.7 Init Placement
Determinism</a></li>
</ul></li>
<li><a href="#appendix-h-performance-feasibility-informative" id="toc-appendix-h-performance-feasibility-informative">Appendix H:
Performance Feasibility (Informative)</a>
<ul>
<li><a href="#h.1-prototype-test-conditions" id="toc-h.1-prototype-test-conditions">H.1 Prototype test
conditions</a></li>
<li><a href="#h.2-representative-observations" id="toc-h.2-representative-observations">H.2 Representative
observations</a></li>
<li><a href="#h.3-interpretation" id="toc-h.3-interpretation">H.3
Interpretation</a></li>
</ul></li>
<li><a href="#appendix-i-rationale-for-lane-count-presets-informative" id="toc-appendix-i-rationale-for-lane-count-presets-informative">Appendix
I: Rationale for Lane Count Presets (Informative)</a>
<ul>
<li><a href="#i.1-rationale-for-l-16-narrow-preset" id="toc-i.1-rationale-for-l-16-narrow-preset">I.1 Rationale for L = 16
(Narrow Preset)</a></li>
<li><a href="#i.2-rationale-for-l-128-wide-preset" id="toc-i.2-rationale-for-l-128-wide-preset">I.2 Rationale for L = 128
(Wide Preset)</a></li>
<li><a href="#i.3-cross-domain-verification" id="toc-i.3-cross-domain-verification">I.3 Cross-Domain
Verification</a></li>
</ul></li>
<li><a href="#appendix-j-indicative-api-straw-man-informative" id="toc-appendix-j-indicative-api-straw-man-informative">Appendix J:
Indicative API Straw Man (Informative)</a>
<ul>
<li><a href="#j.1-design-goal" id="toc-j.1-design-goal">J.1 Design
goal</a></li>
<li><a href="#j.2-favored-approach-standard-fixed-preset-constants-option-3" id="toc-j.2-favored-approach-standard-fixed-preset-constants-option-3">J.2
Favored approach: standard-fixed preset constants (Option 3)</a></li>
<li><a href="#j.3-straw-man-algorithm-api" id="toc-j.3-straw-man-algorithm-api">J.3 Straw-man algorithm
API</a></li>
<li><a href="#j.4-notes-on-naming-and-evolution" id="toc-j.4-notes-on-naming-and-evolution">J.4 Notes on naming and
evolution</a></li>
<li><a href="#j.5-range-overloads-straw-man" id="toc-j.5-range-overloads-straw-man">J.5 Range overloads
(straw-man)</a></li>
</ul></li>
<li><a href="#appendix-k-demonstrator-godbolts-informative" id="toc-appendix-k-demonstrator-godbolts-informative">Appendix K:
Demonstrator Godbolts (Informative)</a>
<ul>
<li><a href="#k.1-demonstrator-set-gross-platform-runs" id="toc-k.1-demonstrator-set-gross-platform-runs">K.1 Demonstrator set
(gross platform runs)</a></li>
<li><a href="#k.2-what-the-demonstrators-are-intended-to-prove" id="toc-k.2-what-the-demonstrators-are-intended-to-prove">K.2 What the
demonstrators are intended to prove</a></li>
<li><a href="#k.3-expected-verification-outputs-for-the-published-demonstrators" id="toc-k.3-expected-verification-outputs-for-the-published-demonstrators">K.3
Expected verification outputs (for the published demonstrators)</a></li>
<li><a href="#k.3.1-cancellation-stress-dataset-recommended" id="toc-k.3.1-cancellation-stress-dataset-recommended">K.3.1
Cancellation stress dataset (recommended)</a></li>
<li><a href="#k.4-recommended-compiler-explorer-settings" id="toc-k.4-recommended-compiler-explorer-settings">K.4 Recommended
Compiler Explorer settings</a></li>
<li><a href="#k.5-interpreting-the-performance-tables-gross-impacts" id="toc-k.5-interpreting-the-performance-tables-gross-impacts">K.5
Interpreting the performance tables (gross impacts)</a></li>
<li><a href="#k.6-relationship-to-repository-evidence" id="toc-k.6-relationship-to-repository-evidence">K.6 Relationship to
repository evidence</a></li>
</ul></li>
<li><a href="#appendix-l-ranges-compatibility-informative" id="toc-appendix-l-ranges-compatibility-informative">Appendix L: Ranges
Compatibility (Informative)</a>
<ul>
<li><a href="#l.1-a-range-surface-does-not-change-the-semantic-contract" id="toc-l.1-a-range-surface-does-not-change-the-semantic-contract">L.1 A
range surface does not change the semantic contract</a></li>
<li><a href="#l.2-determining-n-without-hidden-allocation" id="toc-l.2-determining-n-without-hidden-allocation">L.2 Determining N
without hidden allocation</a></li>
<li><a href="#l.3-working-with-non-sized-single-pass-sources" id="toc-l.3-working-with-non-sized-single-pass-sources">L.3 Working with
non-sized, single-pass sources</a></li>
<li><a href="#l.4-projection-parameter" id="toc-l.4-projection-parameter">L.4 Projection parameter</a></li>
</ul></li>
<li><a href="#appendix-m-detailed-motivation-and-use-cases-informative" id="toc-appendix-m-detailed-motivation-and-use-cases-informative">Appendix
M: Detailed Motivation and Use Cases (Informative)</a>
<ul>
<li><a href="#m.1-ci-regression-testing" id="toc-m.1-ci-regression-testing">M.1 CI Regression Testing</a></li>
<li><a href="#m.2-distributed-training-checkpoints" id="toc-m.2-distributed-training-checkpoints">M.2 Distributed Training
Checkpoints</a></li>
<li><a href="#m.3-regulatory-audit-trails" id="toc-m.3-regulatory-audit-trails">M.3 Regulatory Audit
Trails</a></li>
<li><a href="#m.4-scientific-reproducibility" id="toc-m.4-scientific-reproducibility">M.4 Scientific
Reproducibility</a></li>
<li><a href="#m.5-exascale-hpc-with-kokkos" id="toc-m.5-exascale-hpc-with-kokkos">M.5 Exascale HPC with
Kokkos</a></li>
<li><a href="#m.6-cross-platform-development" id="toc-m.6-cross-platform-development">M.6 Cross-Platform
Development</a></li>
<li><a href="#m.7-reference-and-debugging-mode" id="toc-m.7-reference-and-debugging-mode">M.7 Reference and Debugging
Mode</a></li>
</ul></li>
<li><a href="#appendix-n-multi-threaded-implementation-via-ordered-state-merge-informative" id="toc-appendix-n-multi-threaded-implementation-via-ordered-state-merge-informative">Appendix
N: Multi-Threaded Implementation via Ordered State Merge
(Informative)</a>
<ul>
<li><a href="#n.1-overview" id="toc-n.1-overview">N.1 Overview</a></li>
<li><a href="#n.2-stack-state-representation" id="toc-n.2-stack-state-representation">N.2 Stack State
Representation</a></li>
<li><a href="#n.3-the-push-operation" id="toc-n.3-the-push-operation">N.3 The Push Operation</a></li>
<li><a href="#n.4-the-fold-operation" id="toc-n.4-the-fold-operation">N.4 The Fold Operation</a></li>
<li><a href="#n.5-power-of-2-aligned-partitioning" id="toc-n.5-power-of-2-aligned-partitioning">N.5 Power-of-2 Aligned
Partitioning</a></li>
<li><a href="#n.6-partition-strategy" id="toc-n.6-partition-strategy">N.6 Partition Strategy</a></li>
<li><a href="#n.7-the-merge-operation" id="toc-n.7-the-merge-operation">N.7 The Merge Operation</a></li>
<li><a href="#n.8-correctness-argument" id="toc-n.8-correctness-argument">N.8 Correctness argument</a></li>
<li><a href="#n.9-complete-algorithm" id="toc-n.9-complete-algorithm">N.9 Complete Algorithm</a></li>
<li><a href="#n.10-complexity-analysis" id="toc-n.10-complexity-analysis">N.10 Complexity Analysis</a></li>
<li><a href="#n.11-simd-optimization-8-block-unrolling" id="toc-n.11-simd-optimization-8-block-unrolling">N.11 SIMD
Optimization: 8-Block Unrolling</a></li>
<li><a href="#n.12-performance-observations" id="toc-n.12-performance-observations">N.12 Performance
Observations</a></li>
<li><a href="#n.13-implementation-notes" id="toc-n.13-implementation-notes">N.13 Implementation Notes</a></li>
<li><a href="#n.14-summary" id="toc-n.14-summary">N.14 Summary</a></li>
</ul></li>
<li><a href="#appendix-o-recursive-bisection-balanced-tree-construction-informative" id="toc-appendix-o-recursive-bisection-balanced-tree-construction-informative">Appendix
O: Recursive Bisection (“Balanced”) Tree Construction (Informative)</a>
<ul>
<li><a href="#o.1-definition" id="toc-o.1-definition">O.1
Definition</a></li>
<li><a href="#o.2-equivalence-on-power-of-two-sizes" id="toc-o.2-equivalence-on-power-of-two-sizes">O.2 Equivalence on
power-of-two sizes</a></li>
<li><a href="#o.3-differences-on-non-power-of-two-sizes" id="toc-o.3-differences-on-non-power-of-two-sizes">O.3 Differences on
non-power-of-two sizes</a></li>
<li><a href="#o.4-rationale-for-selecting-iterative-pairwise" id="toc-o.4-rationale-for-selecting-iterative-pairwise">O.4 Rationale
for selecting iterative pairwise</a></li>
</ul></li>
<li><a href="#appendix-p-performance-and-regret-informative" id="toc-appendix-p-performance-and-regret-informative">Appendix P:
Performance and Regret (Informative)</a>
<ul>
<li><a href="#p.1-performance-comparison-iterative-pairwise-vs-recursive-bisection" id="toc-p.1-performance-comparison-iterative-pairwise-vs-recursive-bisection">P.1
Performance comparison: iterative pairwise vs recursive
bisection</a></li>
<li><a href="#p.2-standard-regret" id="toc-p.2-standard-regret">P.2
Standard regret</a></li>
<li><a href="#p.3-upper-bound-on-regret" id="toc-p.3-upper-bound-on-regret">P.3 Upper bound on regret</a></li>
<li><a href="#p.4-measured-throughput-cost" id="toc-p.4-measured-throughput-cost">P.4 Measured throughput
cost</a></li>
</ul></li>
<li><a href="#appendix-q-historical-note-on-span-based-topology-informative" id="toc-appendix-q-historical-note-on-span-based-topology-informative">Appendix
Q: Historical Note on Span-Based Topology (Informative)</a></li>
</ul>
</nav>
<h2 id="abstract">Abstract</h2>
<p>C++ today offers two endpoints for reduction:</p>
<ul>
<li><code>std::accumulate</code>: a specified left-fold expression,
inherently sequential.</li>
<li><code>std::reduce</code>: scalable, but permits reassociation; for
non-associative operations (e.g., floating-point addition), the returned
value may differ across conforming evaluations.</li>
</ul>
<p>This paper specifies a <strong>canonical reduction expression
structure</strong>: for a given input order and topology coordinate
(lane count <code>L</code>), the expression — its parenthesization and
operand order (hereafter, the <em>abstract expression</em>) — is unique
and fully specified. Implementations are free to schedule evaluation
using parallelization, vectorization, or any other strategy, provided
the returned value matches that of the specified expression.</p>
<p>The proposal standardizes the expression structure only. API design
is deferred.</p>
<p>The approach generalizes a technique already present in the Standard
Library: <code>std::accumulate</code> obtains determinism by fixing its
abstract expression structure, not by constraining execution strategy or
floating-point arithmetic (see Appendix D). Bitwise identity of results
additionally depends on the floating-point evaluation model (§6).</p>
<p><strong>Reading guide.</strong> The normative content of this paper
is §4–§5 (~8 pages). Everything else is informative rationale and
appendices.</p>
<table>
<colgroup>
<col style="width: 19%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 51%" />
</colgroup>
<thead>
<tr class="header">
<th>Reader</th>
<th>Read</th>
<th>Skim</th>
<th>Reference as needed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>LEWG reviewer</strong></td>
<td>§1, §2, §4, §5, Polls</td>
<td>§3 (design rationale)</td>
<td>Appendices</td>
</tr>
<tr class="even">
<td><strong>Implementer</strong></td>
<td>Add Appendix B, N</td>
<td>§3.8 (tree shape rationale)</td>
<td></td>
</tr>
<tr class="odd">
<td><strong>Numerical analyst</strong></td>
<td>Add §6, Appendix C, K</td>
<td>Appendix P (throughput data)</td>
<td></td>
</tr>
</tbody>
</table>
<p>Appendix Q records an earlier span-based topology formulation
considered during design and explains why it is not used as a semantic
coordinate in this revision.</p>
<h2 id="executive-overview">0. Executive Overview</h2>
<p>There is a hole in the C++ Standard Library. Sequential reductions
(<code>std::accumulate</code>) have a fully specified evaluation order —
and are therefore deterministic — but cannot be parallelized. Parallel
reductions (<code>std::reduce</code>) can be parallelized but leave the
expression unspecified: for non-associative operations such as
floating-point addition, both the order in which operands are combined
and how they are grouped (parenthesized) may produce different
observable results for the same input solely due to execution strategy
selection.</p>
<p>This proposal fills that hole. It specifies an interleaved binary
reduction tree in two stages. The input is distributed across
<code>L</code> parallel lanes (where <code>L</code> is a user-chosen
lane count, typically matching SIMD width); each lane is independently
reduced by a balanced binary tree (Stage 1), and the <code>L</code> lane
results are combined by the same tree rule (Stage 2). The tree is built
bottom-up, permitting implementations to begin evaluation incrementally
as elements arrive; however, the returned value is defined with respect
to the completed input range of length <code>N</code>. Fixing the
parenthesization and operand order gives a deterministic evaluation
expression that makes results reproducible for a fixed input order,
topology coordinate <code>L</code>, and floating-point evaluation model.
Implementations remain free to schedule evaluation using threads, SIMD,
or GPU kernels, provided the returned value matches the specified
expression. API design is deferred; this paper seeks validation of the
expression structure only. No changes to existing algorithms are
proposed in this revision.</p>
<p><strong>What this paper adds</strong></p>
<ul>
<li>Fixed abstract expression for parallel reduction, parameterized by
lane count <code>L</code>.</li>
<li>Two-stage semantic structure: per-lane tree + cross-lane tree.</li>
<li>Execution remains unconstrained; only the expression is
specified.</li>
</ul>
<p><strong>What this paper does not do</strong></p>
<ul>
<li>Guarantee cross-architecture bitwise identity (requires matching
<code>L</code> and floating-point model; see §6).</li>
<li>Constrain execution, scheduling, or parallelism.</li>
<li>Propose a final API surface.</li>
</ul>
<hr />
<h2 id="the-semantic-gap-in-c-reduction-facilities">1. The Semantic Gap
in C++ Reduction Facilities</h2>
<p>The table in §1.1 classifies existing reduction facilities by
expression ownership — whether the Standard fully specifies the abstract
expression or leaves it (partially or wholly) unspecified. The proposed
canonical reduction completes the family by providing a specified
expression that admits parallel evaluation.</p>
<p>[<em>Note:</em> In this paper, <strong>canonical</strong> refers
strictly to the abstract expression structure. The terms “canonical
expression,” “canonical tree,” and “canonical reduction” are used
interchangeably to refer to the fixed abstract expression defined in §4.
Implementations retain freedom in evaluation schedule; the facility
provides a stable, specified baseline topology. The term “canonical” is
descriptive and carries no relation to
<code>std::filesystem::canonical</code>; the facility may be named
<code>specified_reduce</code>, <code>deterministic_reduce</code>, or
otherwise in an API revision. <em>—end note</em>]</p>
<p>This work was presented to SG14 (Low Latency/Games/Embedded/Financial
Trading), who encouraged a formal paper. The approach has been validated
with working implementations across x86 AVX2, ARM NEON, and CUDA
(Appendix K), producing bitwise-identical results for fixed topology
coordinates. A summary comparison with HPC frameworks and industry
practice appears in §7.</p>
<h3 id="why-this-needs-to-be-in-the-standard-library">1.1 Why This Needs
to Be in the Standard Library</h3>
<p>Parallel algorithms literature commonly identifies a small set of
fundamental primitives for combining elements of a sequence, notably
reduction and scan (prefix computation) [Blelloch1989]. These primitives
are defined in terms of combining values using a binary operation over
an ordered sequence.</p>
<p>The C++ Standard Library provides corresponding facilities:</p>
<ul>
<li><code>std::accumulate</code>, which combines all values in a
sequence,</li>
<li><code>std::inclusive_scan</code> and
<code>std::exclusive_scan</code>, which compute prefix combinations of
values, and</li>
<li><code>std::reduce</code>, which combines all values in a sequence
and permits parallel execution.</li>
</ul>
<p>These facilities provide different guarantees about the order in
which operations are performed. <code>std::accumulate</code> specifies
that values are combined from left to right, fully determining the
abstract expression. <code>std::inclusive_scan</code> and
<code>std::exclusive_scan</code> preserve operand order but permit
reassociation (arbitrary parenthesization). <code>std::reduce</code>, by
contrast, permits implementations to both reorder operands and
reassociate to enable parallel execution.</p>
<p>This leads to the following distinction:</p>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="header">
<th>Facility</th>
<th>Specification model</th>
<th>Operand order</th>
<th>Parenthesization</th>
<th>Unique expression?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>std::accumulate</code></td>
<td>Sequential left fold</td>
<td>Fixed (left-to-right)</td>
<td>Fixed (left fold)</td>
<td>Yes</td>
</tr>
<tr class="even">
<td><code>std::ranges::fold_left</code> / <code>fold_right</code></td>
<td>Sequential fold</td>
<td>Fixed</td>
<td>Fixed</td>
<td>Yes</td>
</tr>
<tr class="odd">
<td><code>std::inclusive_scan</code></td>
<td><code>GENERALIZED_NONCOMMUTATIVE_SUM</code></td>
<td>Fixed (left-to-right)</td>
<td>Unspecified</td>
<td>No — reassociation permitted</td>
</tr>
<tr class="even">
<td><code>std::exclusive_scan</code></td>
<td><code>GENERALIZED_NONCOMMUTATIVE_SUM</code></td>
<td>Fixed (left-to-right)</td>
<td>Unspecified</td>
<td>No — reassociation permitted</td>
</tr>
<tr class="odd">
<td><code>std::reduce</code></td>
<td><code>GENERALIZED_SUM</code></td>
<td>Unspecified</td>
<td>Unspecified</td>
<td>No — reordering + reassociation permitted</td>
</tr>
<tr class="even">
<td><code>std::transform_reduce</code></td>
<td><code>GENERALIZED_SUM</code></td>
<td>Unspecified</td>
<td>Unspecified</td>
<td>No — reordering + reassociation permitted</td>
</tr>
<tr class="odd">
<td>Proposed <code>canonical_reduce</code></td>
<td>Canonical reduction expression (§4)</td>
<td>Fixed (lane assignment)</td>
<td>Fixed (canonical tree)</td>
<td>Yes, for a given lane count L</td>
</tr>
</tbody>
</table>
<p>The Standard already provides expression ownership for sequential
reduction (<code>accumulate</code>) and preserves operand order for
parallel prefix computation (scan). Parallel reduction is the remaining
primitive for which no specified expression exists. This proposal
completes that family.</p>
<p>The standard defines the <code>GENERALIZED_NONCOMMUTATIVE_SUM</code>
and <code>GENERALIZED_SUM</code> specification models in
[numerics.defns] (N5032). <code>GENERALIZED_NONCOMMUTATIVE_SUM</code>
preserves operand order but allows arbitrary parenthesization (the split
point K is unspecified). <code>GENERALIZED_SUM</code> additionally
permits operand reordering.</p>
<p>Sequential reduction specifies a unique expression. Parallel prefix
algorithms (scan) preserve operand order but admit multiple conforming
parenthesizations. Parallel reduction (<code>std::reduce</code>) is
unique among these primitives: it specifies neither operand order nor
parenthesization — two independent semantic freedoms that together make
the abstract expression fully unspecified.</p>
<p>For operations that are not associative, such as floating-point
addition, different orders of combination may produce different results.
Controlling the floating-point evaluation model alone is therefore
insufficient to ensure reproducible parallel reduction: without a
specified order of combination, different valid implementations of
<code>std::reduce</code> may compute different results for the same
input.</p>
<p>This proposal introduces a parameter L that selects a specified order
in which values are combined during parallel reduction. For a fixed
input sequence and chosen L, conforming implementations must produce the
same result as if the values were combined in that specified order,
while remaining free to evaluate those combinations using parallel
execution strategies. Providing such a facility enables portable
parallel reduction with well-defined evaluation semantics, analogous to
those already provided for prefix algorithms.</p>
<p><strong>Why this cannot be achieved outside the standard.</strong>
This facility cannot be provided as an external library built on
existing standard components. <code>std::reduce</code> explicitly
permits reassociation via its “Generalized Sum” semantics; no wrapper,
view, or range adaptor can remove that permission. A library-external
<code>tree_reduce</code> built on raw loops would lack composition with
execution policies, senders/receivers, and ranges — and each framework
(Kokkos, TBB, CUB) that provides its own solution does so with
incompatible semantics. Standardization provides a single portable
semantic foundation that library and framework authors can target.</p>
<p><strong>Heterogeneous execution.</strong> Modern C++ programs
increasingly execute on heterogeneous platforms — CPU threads, GPU
kernels, and accelerators within a single application. The
sender/receiver model (P2300) enables scheduling work to different
executors, but without a standardized expression structure, the same
reduction dispatched to different devices may produce different results
simply because each executor chooses a different grouping. By fixing the
abstract expression in the standard, a conforming reduction produces the
same returned value regardless of which executor evaluates it, provided
that the floating-point evaluation model is equivalent and the same
topology coordinate <code>L</code> is selected. This provides a useful
semantic basis for improving reproducibility across heterogeneous
schedulers.</p>
<h3 id="motivating-example-informative">1.2 Motivating example
(informative)</h3>
<p>Consider the same 6 floating-point inputs evaluated with the same
<code>+</code> operator:</p>
<p><code>[1e16, 1, 1, -1e16, 1, 1]</code></p>
<p>Because floating-point addition is not associative, different
parenthesizations can legitimately produce different results.</p>
<ul>
<li>A <strong>left-to-right fold</strong> (as with
<code>std::accumulate</code>) groups as
<code>(((((1e16 + 1) + 1) + -1e16) + 1) + 1)</code> which yields
<code>4</code> on IEEE-754 double implementations with round-to-nearest,
because small terms can survive until after the large cancellation.</li>
<li>A plausible <strong>tree reduction</strong> (as may happen inside
<code>std::reduce</code>) can group as
<code>((1e16 + 1) + (1 + -1e16)) + (1 + 1)</code>, in which some
<code>+1</code> terms are lost early, yielding <code>2</code> under the
same rounding mode.</li>
</ul>
<p>Today, both outcomes are consistent with the Standard because
<code>std::reduce</code> does not fix the abstract expression (it
requires associativity/commutativity to be well-defined). This paper’s
goal is to define a <strong>single standard-fixed expression</strong>
(“canonical”) so that the returned value is reproducible within a fixed
consistency domain (fixed input order, fixed topology coordinate
<code>L</code>, and a fixed floating‑point evaluation model), while
still permitting parallel execution.</p>
<p>Appendix K.3.1 provides a cancellation‑heavy dataset in which
<code>std::reduce</code> exhibits run‑to‑run variability while the
canonical reduction stays stable for fixed <code>L</code>, and in which
varying <code>L</code> intentionally yields different results
(confirming that topology selection is semantic, not a hint).</p>
<h3 id="determinism-via-expression-ownership-in-existing-practice">1.3
Determinism via Expression Ownership in Existing Practice</h3>
<p><code>std::accumulate</code> is the clearest precedent. Its
specification mandates a left‑to‑right fold, fully determining the
abstract expression. Consequently, implementations are not permitted to
reassociate operations, even when the supplied <code>binary_op</code> is
non‑associative. This guarantee is structural, not numerical: the
Standard does not promise bitwise identity across platforms or builds,
but it does fully specify the expression being evaluated.</p>
<p>This proposal applies the same structural technique to parallel
reduction: it standardizes a single canonical expression that admits
parallel evaluation. Appendix D analyzes the
<code>std::accumulate</code> precedent in detail.</p>
<h2 id="scope-and-non-goals">2. Scope and Non-Goals</h2>
<p>This paper proposes <strong>semantics only</strong>. It seeks LEWG
validation of the fixed expression structure defined in §4 before
committing to API design. This proposal introduces no new requirements
on <code>binary_op</code> beyond invocability and convertibility and
does not modify existing algorithms.</p>
<p>This facility differs from <code>std::reduce</code> composed with an
execution policy: <code>std::reduce</code> deliberately leaves the
abstract expression unspecified regardless of policy (§1), whereas this
paper standardizes a single fixed canonical expression for a chosen
topology coordinate.</p>
<p>The facility defines the returned value for a chosen topology
coordinate (lane count <code>L</code>). Execution strategy remains
unconstrained (§4).</p>
<p>Reproducibility under this facility is defined relative to a chosen
topology coordinate <code>L</code>. Different topology selections
intentionally define different abstract expressions and may therefore
produce different results for non-associative operations. Determinism is
guaranteed for a fixed input sequence, fixed topology coordinate
<code>L</code>, and fixed floating-point evaluation model.</p>
<p>For clarity: §4–§5 are normative and define the semantic contract of
the facility. All other sections and all appendices are informative and
do not introduce additional requirements. Appendix K records external
demonstrator programs (Compiler Explorer links) used to validate the
semantics on multiple architectures; they are semantic witnesses, not
reference implementations and not part of the proposal.</p>
<p><strong>In scope (this paper):</strong></p>
<ul>
<li>Semantic definition of the canonical reduction expression (fixed
abstract expression)</li>
<li>Parameterization by lane count <code>L</code> (§9)</li>
<li>Invariance guarantees (§5)</li>
<li>Rationale for design choices</li>
</ul>
<p><strong>Deferred to later revision (pending LEWG
direction):</strong></p>
<ul>
<li>API surface (new algorithm vs execution policy vs both)</li>
<li>Range-based overloads</li>
<li>Scheduler/executor integration (sender/receiver-based execution
models)</li>
</ul>
<p><strong>Scan and reduce differ in their degree of
nondeterminism.</strong> The nondeterminism admitted by scan algorithms
is strictly weaker than that admitted by <code>std::reduce</code>. Scan
preserves the operand sequence and permits only reassociation of the
binary operation (via <code>GENERALIZED_NONCOMMUTATIVE_SUM</code>). By
contrast, <code>std::reduce</code> additionally permits reordering of
operands (<code>GENERALIZED_SUM</code>). A canonical reduction (§4)
therefore fixes two dimensions of variation (operand order and
parenthesization), while a corresponding scan facility would need to fix
only parenthesization for each prefix.</p>
<p><strong>Related algorithms (informative).</strong> Because scan
algorithms already preserve operand order ([numerics.defns]:
<code>GENERALIZED_NONCOMMUTATIVE_SUM</code>), applying the tree rule
(§4) independently to each prefix yields a deterministic scan without
introducing a lane parameter. The missing semantic in scan is therefore
limited to fixed parenthesization; the reduction defined in this paper
provides the necessary machinery. A future <code>canonical_scan</code>
would apply a standard-fixed grouping to each prefix in sequence order;
unlike reduction, this must not permute operands. Scan is out of scope
for this paper.</p>
<h3 id="opt-in-reproducibility-and-performance-trade-off-informative">2.1
Opt-in reproducibility and performance trade-off (informative)</h3>
<p>This facility is an <strong>opt-in</strong> choice for users who
require a stable, specified abstract expression structure (e.g., for
debugging, verification, regression testing, or reproducible numerical
workflows). It is not intended to replace existing high-throughput
facilities. Users who prioritize maximum throughput over expression
identity should continue to use <code>std::reduce</code> (or
domain-specific facilities) where unspecified reassociation is
acceptable.</p>
<h3 id="traversal-and-sizing-requirements-informative">2.2 Traversal and
sizing requirements (informative)</h3>
<p>Evaluation of the canonical expression requires a well-defined
<code>N</code> (the number of elements in the input range). The
iterative pairwise algorithm is single pass, but the normative
definition (§4) is stated in terms of <code>K = ceil(N/L)</code>, so the
specification as written requires <code>N</code> to be known. No
implicit materialization or allocation is performed by the facility.</p>
<p>However, the expression is defined for the completed range of length
<code>N</code>; implementations may consume elements incrementally
(streaming) provided the returned value equals that of the canonical
expression for that final <code>N</code> (see §3.8 for architectural
discussion). See Appendix L for further discussion of ranges
compatibility.</p>
<h3 id="exception-safety">2.3 Exception safety</h3>
<p>Exception handling follows the corresponding Standard Library rules
for the selected execution mode:</p>
<ul>
<li>When evaluated without an execution policy, if
<code>binary_op</code> throws, the exception is propagated to the caller
([algorithms.general]).</li>
<li>When evaluated with an execution policy: if execution of
<code>binary_op</code> exits via an uncaught exception and the policy is
one of the standard execution policies, <code>std::terminate</code> is
called ([algorithms.parallel.exceptions]).</li>
</ul>
<p>The expression-equivalence (returned-value) guarantee applies only
when evaluation completes normally. If <code>std::terminate</code> is
called under a policy-based evaluation, the state of any outputs and any
externally observable side effects is unspecified.</p>
<h3 id="complexity">2.4 Complexity</h3>
<p>Evaluation of the canonical reduction for an input range of
<code>N</code> elements performs <code>O(N)</code> applications of
<code>binary_op</code>. Specifically, the canonical expression contains
exactly <code>N − 1</code> applications of <code>binary_op</code> when
<code>N &gt; 0</code> (the init combination in §4.5, if present, adds
one additional application); absent-operand positions (§4.2.2) do not
induce additional applications.</p>
<p>This matches the work complexity required of
<code>std::reduce</code>. This paper specifies work complexity only;
depth and storage are not normative. The abstract expression has O(log
N) height; this paper does not require implementations to realize that
depth without auxiliary storage. No guarantees are made about evaluation
depth, degree of parallelism, or auxiliary storage.</p>
<p>[<em>Note:</em> The natural shift-reduce evaluation strategy (§4.2.3)
maintains a stack of depth O(log K) per lane, where K = ceil(N/L). For L
lanes this implies O(L · log(N/L)) intermediate values of type A as
working storage. This is modest in practice (e.g., 8 lanes × 30 stack
entries for a billion elements) but is not zero. <em>—end note</em>]</p>
<h3 id="expression-identity-vs.-bitwise-identity">2.5 Expression
identity vs. bitwise identity</h3>
<p>This facility guarantees <strong>expression identity</strong>: for
fixed input order and topology coordinate, the abstract reduction
expression is identical across conforming implementations. Bitwise
identity of results additionally depends on the floating-point
evaluation model; §6 discusses the distinction in detail.</p>
<p>This facility introduces no additional guarantees regarding
floating-point rounding behavior, contraction (e.g., FMA), extended
precision, or evaluation environment beyond those already permitted by
the C++ Standard.</p>
<h3 id="constant-evaluation">2.6 Constant evaluation</h3>
<p>When evaluated in a constant-evaluation context, invocations of
<code>binary_op</code> are sequenced in an order consistent with
evaluation of the canonical expression defined in §4. Parallel execution
is not required for constant evaluation. The expression is a pure
functional specification with no dependence on runtime state, so it is
structurally amenable to constant evaluation provided
<code>binary_op</code> is itself usable in constant expressions.
Detailed <code>constexpr</code> specification (including iterator
constraints) is deferred to the API design paper.</p>
<h3 id="freestanding">2.7 Freestanding</h3>
<p>The canonical reduction requires no heap allocation and has bounded
working storage (O(L · log(N/L)) intermediate values of type A). It is a
candidate for freestanding implementations; detailed freestanding
specification is deferred to the API design paper.</p>
<h2 id="design-space-informative">3. Design Space (Informative)</h2>
<p>This section catalogs key design alternatives for a reproducible
reduction facility and explains why this proposal chooses a
user-selectable, interleaved-lane topology with a standard-defined
canonical expression.</p>
<p>The goal is to close the “grouping gap” for parallel reductions by
providing a facility that:</p>
<ol type="1">
<li>Specifies a single abstract expression (parenthesization and operand
order) for a chosen topology.</li>
<li>Achieves scalable depth (O(log N)) suitable for parallel
execution.</li>
<li>Is topology-stable: the expression is not implicitly determined by
thread count or implementation strategy.</li>
<li>Remains implementable efficiently on modern hardware (SIMD,
multicore, GPU).</li>
</ol>
<p>The following alternatives are evaluated against these
requirements.</p>
<h3 id="expression-algorithm-and-execution">3.0 Expression, Algorithm,
and Execution</h3>
<p>Every reduction computes an <em>abstract expression</em>: a
parenthesized combination of operands with a defined left/right operand
order. This expression exists independently of how it is evaluated in
time. In C++, the abstract expression has historically been implicit —
specified only indirectly through algorithm wording — and has never been
named as a distinct semantic concern.</p>
<p>In practice, three concerns determine the behavior of a
reduction:</p>
<ul>
<li><strong>Expression structure</strong> — the abstract computation
being performed: grouping, parenthesization, and operand order.</li>
<li><strong>Algorithm</strong> — the semantic contract that determines
which aspects of the expression are specified or intentionally left
unspecified.</li>
<li><strong>Execution</strong> — how evaluation of the expression is
scheduled: sequentially, in parallel, vectorized, or otherwise.</li>
</ul>
<p>The C++ Standard Library already relies on this separation but does
not articulate it explicitly. As a result, expression structure is
routinely conflated with execution strategy, producing persistent
confusion.</p>
<p><strong>Existing facilities through this lens.</strong> Seen through
this model, existing facilities differ primarily in <em>expression
ownership</em>, not in parallelism:</p>
<ul>
<li><code>std::accumulate</code> specifies a left-to-right fold. The
algorithm fully defines the abstract expression, yielding a
deterministic result for a given input order and operation.</li>
<li><code>std::reduce</code> explicitly declines to specify the
expression structure. It defines a <em>generalized sum</em>, permitting
reassociation to enable scalable execution.</li>
<li>Execution policies operate exclusively on the execution dimension.
They constrain scheduling and concurrency, but do not define or refine
the abstract expression being evaluated. Even
<code>execution::seq</code> does not impose a specific grouping or
forbid reassociation; it affects <em>how</em> an algorithm runs, not
<em>what</em> expression it computes.</li>
</ul>
<p>This explains why <code>std::reduce(execution::seq, ...)</code> is
still permitted to produce different results for non-associative
operations: the algorithm has not specified the expression, and the
policy does not add semantic guarantees.</p>
<p><strong>Why execution policies cannot carry expression
semantics.</strong> It is natural to ask whether a canonical reduction
could be expressed as an execution policy rather than a new algorithm.
Under the current standard model, execution policies are deliberately
non-semantic with respect to the returned value:</p>
<ol type="1">
<li>Policies are designed to <em>relax</em> constraints (permitting
concurrency, vectorization), not to <em>add</em> new semantic guarantees
about grouping or parenthesization.</li>
<li>Policies are composed freely and orthogonally. Encoding topology in
a policy would require dominance rules to resolve conflicts between
policies that specify different topologies — a semantic hierarchy the
standard does not have.</li>
<li><code>std::reduce</code> already proves the limitation:
<code>std::reduce(execution::seq, ...)</code> still has generalized-sum
semantics. If <code>seq</code> were sufficient to fix the expression,
<code>std::reduce(seq, ...)</code> would collapse into
<code>std::accumulate</code> — but the standard explicitly keeps them
distinct.</li>
<li>A topology parameter is a <em>semantic</em> parameter that
intentionally changes observable results for non-associative operations.
This differs in kind from an execution hint.</li>
</ol>
<p>Encoding expression structure in a policy would therefore require a
fundamental change to the execution-policy model. This proposal
intentionally avoids that scope.</p>
<p><strong>Consequence.</strong> Because expression structure is a
semantic concern that execution policies cannot express, and because
views and range adaptors can reorder traversal but cannot constrain
combination (§3.7), expression ownership must reside in the algorithm.
This proposal makes the abstract expression explicit and assigns
ownership of it to the algorithm, restoring a clean separation between
<em>what</em> is computed (expression), <em>what</em> is guaranteed
(algorithm), and <em>how</em> it is executed (execution policy).</p>
<p>This separation also clarifies the relationship with executors
(§3.10).</p>
<h3 id="why-not-left-fold">3.1 Why Not Left-Fold?</h3>
<p>A strict left-to-right fold has a fixed evaluation order:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Left-fold</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>T result <span class="op">=</span> init<span class="op">;</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="kw">auto</span> it <span class="op">=</span> first<span class="op">;</span> it <span class="op">!=</span> last<span class="op">;</span> <span class="op">++</span>it<span class="op">)</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a> result <span class="op">=</span> op<span class="op">(</span>result<span class="op">,</span> <span class="op">*</span>it<span class="op">);</span></span></code></pre></div>
<p>This is <code>std::accumulate</code>. It provides run-to-run
stability for a given input order, but it cannot parallelize effectively
because each operation depends on the prior result. The reduction depth
is O(N) rather than O(log N), making it unsuitable for scalable parallel
execution.</p>
<p>A left-fold therefore solves stability but not scalability, and it
already exists in the standard library.</p>
<h3 id="why-not-blocked-decomposition">3.2 Why Not Blocked
Decomposition?</h3>
<p>A common parallelization strategy is blocked decomposition: assign
contiguous chunks to workers and reduce each chunk:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Blocked (illustrative)</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co">// Thread 0: E[0..N/4)</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">// Thread 1: E[N/4..N/2)</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">// Thread 2: E[N/2..3N/4)</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">// Thread 3: E[3N/4..N)</span></span></code></pre></div>
<p>Blocked decomposition can be efficient, but it does not by itself
provide a topology-stable semantic contract. To obtain a fully specified
abstract expression, the standard would need to specify:</p>
<ul>
<li>the exact chunk boundaries (including handling of non-power-of-two
sizes),</li>
<li>how chunk results are combined (the second-stage reduction tree),
and</li>
<li>how these choices relate to the execution policy and degree of
parallelism.</li>
</ul>
<p>Absent such specification, the resulting expression is naturally
coupled to execution strategy (thread count, scheduler, partitioner),
which varies across implementations and runs. Fully specifying these
details effectively defines a new algorithm and topology — at which
point the question becomes: which topology should be standardized?</p>
<p>This proposal selects a topology that is simple to specify
canonically and maps well to common hardware structures (see §3.5).</p>
<h3 id="why-not-a-fixed-standard-constant">3.3 Why Not a Fixed Standard
Constant?</h3>
<p>Another approach is to standardize a fixed topology constant. This
can appear attractive for simplicity and uniformity, but it ages
poorly:</p>
<ul>
<li>If the fixed constant is too small, future wider hardware may be
underutilized.</li>
<li>If the fixed constant is too large, current narrow hardware may
incur unnecessary work or overhead.</li>
<li>Any fixed value becomes a standard revision pressure point as
hardware evolves.</li>
</ul>
<p>A fixed constant trades long-term efficiency and flexibility for
initial simplicity and becomes an ongoing “default value” debate. This
proposal instead makes topology selection an explicit part of the
semantic contract (§3.5).</p>
<h3 id="why-not-implementation-defined">3.4 Why Not
Implementation-Defined?</h3>
<p>Allowing implementations to choose the reduction topology is the
status quo problem. For parallel reductions such as
<code>std::reduce</code>, the Standard permits reassociation, and
therefore permits different abstract expressions across implementations
and settings.</p>
<p>An implementation-defined topology does not resolve the grouping gap;
it moves the source of variation from the algorithm’s reassociation
freedom to the implementation’s topology choice. A reproducibility
facility must standardize the expression structure for a chosen
topology, rather than leaving that structure implementation defined.</p>
<h3 id="the-proposed-design">3.5 The proposed design</h3>
<p>The facility proposed here is defined by a <strong>family of
canonical expressions</strong> parameterized by a user-selected topology
coordinate. For a chosen coordinate, the Standard defines a single
abstract expression, and the returned value is the result of evaluating
that expression (as-if), independent of execution strategy.</p>
<p><strong>Primary topology coordinate.</strong> The expression is
defined in terms of the <strong>lane count <code>L</code></strong>. For
intuition: <code>L = 1</code> degenerates to a single tree over the
input sequence (no lane interleaving). Larger <code>L</code> introduces
SIMD/GPU-friendly lane parallelism but still denotes one
Standard-defined expression fully determined by <code>(N, L)</code>. The
normative definition of the two-stage reduction (lane partitioning,
per-lane tree, cross-lane tree) appears in §4.</p>
<p><strong>Why interleaved lanes, not contiguous blocks?</strong> A
contiguous-block partition (elements <code>[0, N/L)</code> to lane 0,
<code>[N/L, 2N/L)</code> to lane 1, etc.) would also be topology-stable.
The interleaved layout is chosen because it maps directly to SIMD
register filling: a single aligned vector load of <code>L</code>
consecutive elements places one element into each lane simultaneously.
Contiguous blocks would require gathering from <code>L</code> distant
memory locations to fill a register. Additionally, interleaving
guarantees that all lanes have the same number of elements (to within
one), so all lanes execute the same tree shape — enabling SIMD lockstep
execution without per-lane branching. This uniform tree shape across
lanes is what makes the two-stage decomposition (§4.4) efficient in
practice.</p>
<p>The lane count <code>L</code> is the sole topology coordinate that
defines the semantics. Earlier explorations considered a byte-span
parameter <code>M</code> (where
<code>L = M / sizeof(value_type)</code>), but this creates a portability
trap: <code>sizeof(value_type)</code> varies across platforms, so the
same <code>M</code> value silently produces different expressions on
different targets. §9.2 discusses this rationale in detail.</p>
<h3 id="industry-precedents-for-constrained-computation-structure">3.6
Industry precedents for constrained computation structure</h3>
<p>A common counter-argument is that because bitwise identity across
different ISAs (e.g., x86 vs. ARM) cannot be guaranteed by the C++
Standard alone, the library should not attempt to provide run-to-run
stability guarantees. However, existing industry practice demonstrates
the value of partial guarantees — specifically, fixing the computation
topology — even when full bitwise identity is not achievable.</p>
<p>A widely used mechanism for improving reproducibility is to
<strong>constrain computation structure</strong> (topology, kernel
choice, or execution path) to remove sources of run-to-run variability
introduced by parallel execution. This proposal targets one such source:
unspecified reassociation inside standard parallel reductions. Fixing
the abstract expression is therefore an important building block for
reproducible parallel reductions; additional conditions (e.g.,
floating-point evaluation model and environment constraints) remain
outside the scope of this paper.</p>
<h4 id="examples-informative">3.6.1 Examples (informative)</h4>
<table>
<colgroup>
<col style="width: 15%" />
<col style="width: 15%" />
<col style="width: 38%" />
<col style="width: 29%" />
</colgroup>
<thead>
<tr class="header">
<th>Library</th>
<th>Feature</th>
<th>Mechanism (documented)</th>
<th>Scope (typical)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Intel oneMKL</td>
<td>Conditional Numerical Reproducibility (CNR)</td>
<td>Constrains execution paths / pins to a logical ISA</td>
<td>Reproducibility across specified CPU configurations</td>
</tr>
<tr class="even">
<td>NVIDIA CUB</td>
<td>Deterministic reduction variants</td>
<td>Uses a fixed reduction order for deterministic mode</td>
<td>Reproducibility on the same architecture</td>
</tr>
<tr class="odd">
<td>PyTorch / TensorFlow</td>
<td>Deterministic algorithms flags</td>
<td>Disables nondeterministic kernels / selects deterministic
kernels</td>
<td>Reproducible training runs (scope varies)</td>
</tr>
</tbody>
</table>
<h4 id="references-informative">3.6.2 References (informative)</h4>
<p><strong>Intel oneMKL CNR:</strong> Intel documents that parallel
algorithms can produce different results based on thread counts and ISA
and provides CNR modes to constrain execution paths. See: Intel oneMKL
Developer Guide, “Introduction to Conditional Numerical Reproducibility”
[IntelCNR].</p>
<p><strong>NVIDIA CUB:</strong> NVIDIA distinguishes between “fast”
nondeterministic reductions and “deterministic” variants that use a
fixed-order reduction. See: NVIDIA CUB API Documentation
[NvidiaCUB].</p>
<p>These libraries address reproducibility through different mechanisms
and with different scope. This proposal does not claim to replicate
their exact guarantees but draws on the same insight: fixing the
computation structure is a useful building block for
reproducibility.</p>
<h4 id="conclusion">3.6.3 Conclusion</h4>
<p>By fixing the abstract expression structure, this proposal provides
an <strong>important building block</strong> for reproducibility — but
not a sufficient one. Sufficient conditions depend on the program’s
floating-point evaluation model and environment (§6).</p>
<h3 id="the-structural-necessity-of-a-new-algorithm">3.7 The structural
necessity of a new algorithm</h3>
<p>A central tenet of this proposal is that run-to-run stability of the
returned value is a <strong>property of the evaluation</strong>, not the
data source. To achieve both logarithmic scalability and topological
determinism, the algorithm itself must “own” the reduction tree. This
logic cannot be injected into existing algorithms via a View or a Range
adaptor.</p>
<h4 id="why-stdreduce-views-cannot-provide-this-contract">3.7.1 Why
std::reduce + Views cannot provide this contract</h4>
<ul>
<li>Views can reorder iteration; they cannot constrain combination. A
view may present elements in a deterministic order, but it cannot force
the algorithm to evaluate a particular parenthesization.</li>
<li><code>std::reduce</code> explicitly permits reassociation.
Therefore, even with a deterministic view and a fixed input order,
<code>std::reduce</code> may legally evaluate a different abstract
expression (see [numeric.ops.reduce]).</li>
</ul>
<div class="sourceCode" id="cb3"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">// A deterministic view does not make std::reduce deterministic:</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> view <span class="op">=</span> data <span class="op">|</span> views<span class="op">::</span>transform<span class="op">(</span>f<span class="op">);</span> <span class="co">// preserves iteration order</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> r1 <span class="op">=</span> <span class="bu">std::</span>reduce<span class="op">(</span><span class="bu">std::</span>execution::par<span class="op">,</span> view<span class="op">.</span>begin<span class="op">(),</span> view<span class="op">.</span>end<span class="op">(),</span> init<span class="op">,</span> op<span class="op">);</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">// r1 may still use an implementation-chosen reassociation.</span></span></code></pre></div>
<h4 id="consequence-the-algorithm-must-own-the-expression">3.7.2
Consequence: the algorithm must own the expression</h4>
<p>To provide the guarantee defined in §4, the reduction facility itself
must specify and “own” the abstract expression tree. As established in
§3.0, expression structure cannot reside in execution policies, views,
or executors — the algorithm is the only standard mechanism that can own
expression semantics.</p>
<h3 id="rationale-for-tree-shape-choice">3.8 Rationale for tree shape
choice</h3>
<p>Given that a new algorithm must own its expression tree (§3.7.2), the
next design question is: which tree construction rule should the
Standard specify? Two well-known candidates exist: iterative pairwise
(shift-reduce) and recursive bisection. This subsection records the
considerations that inform the choice.</p>
<p>The discussion in this subsection concerns implementation strategies
only and does not alter the semantic requirement that the returned value
be that of the canonical expression defined in §4 for the completed
input sequence of length <code>N</code>. The specification in §4–§5 does
not require incremental evaluation, streaming, or any particular
execution strategy.</p>
<h4 id="candidate-comparison">3.8.1 Candidate comparison</h4>
<p><strong>Arguments favoring iterative pairwise
(recommended):</strong></p>
<ol type="1">
<li><p><strong>Industry alignment:</strong> SIMD and GPU implementations
commonly use iterative pairwise because it maps directly to hardware
primitives (warp shuffle-down, vector lane pairing). This includes
NVIDIA CUB’s deterministic reduction modes.</p></li>
<li><p><strong>Implementation naturalness:</strong> For implementers
familiar with GPU and SIMD programming, iterative pairwise matches the
mental model of “pair adjacent lanes, carry the odd one” — the same
pattern used in existing high-performance reduction kernels.</p></li>
<li><p><strong>Adoption ease:</strong> Libraries that already implement
deterministic reductions using iterative pairwise would require no
algorithmic changes to conform to this specification. This lowers the
barrier to adoption.</p></li>
<li><p><strong>Direct SIMD mapping:</strong> The iterative pairwise
pattern corresponds directly to shuffle-down operations on GPU warps and
SIMD vector lanes, enabling efficient implementation without index
remapping.</p></li>
</ol>
<p><strong>Arguments favoring recursive bisection:</strong></p>
<ol type="1">
<li><p><strong>Specification clarity:</strong> The recursive definition
is three lines with no special cases. The iterative definition requires
explicit handling of odd count carry logic.</p></li>
<li><p><strong>Tree symmetry:</strong> For non-power-of-two k, recursive
bisection produces more balanced subtrees. The “heavier” subtree (with
more elements) is always on the right, following the
<code>m = floor(k/2)</code> split.</p></li>
<li><p><strong>Academic precedent:</strong> Recursive bisection
corresponds to the “pairwise summation” algorithm analyzed in numerical
analysis literature ([Higham2002] §4).</p></li>
</ol>
<p><strong>Why the choice matters:</strong></p>
<p>For non-associative operations (e.g., floating-point addition), the
two algorithms produce different numerical results for non-power-of-two
k. Once standardized, the choice cannot be changed without breaking
existing code that depends on specific results.</p>
<p><strong>Why the choice is bounded:</strong></p>
<p>Both algorithms have identical O(log k · ε) error bounds. Both
produce identical trees for power-of-two k. The practical impact is
limited to: - Non-power-of-two per-lane element counts (when
<code>ceil(N/L)</code> is not a power of two) - Non-power-of-two lane
counts (when L itself is not a power of two) - Cross-lane reduction when
N does not evenly divide L</p>
<p>For the common case of power-of-two L and large N, the per-lane trees
are dominated by power-of-two cases where both algorithms agree.</p>
<p><strong>Visual comparison (k = 7):</strong> For power-of-two k, both
algorithms produce identical trees. The difference is visible only for
non-power-of-two k. The following side-by-side comparison for k = 7
illustrates the full extent of the difference:</p>
<pre class="text"><code>Iterative Pairwise (IPR)              Recursive Bisection
─────────────────────────             ─────────────────────────

         op                                    op
       /    \                                /    \
      op      op                           op      op
     /  \    /  \                         /  \    /  \
    op   op  op  e₆                     e₀   op  op   op
   / \  / \  / \                              / \  / \  / \
  e₀ e₁ e₂ e₃ e₄ e₅                      e₁ e₂ e₃ e₄ e₅ e₆</code></pre>
<p>IPR: <code>((e₀⊕e₁)⊕(e₂⊕e₃))⊕((e₄⊕e₅)⊕e₆)</code> · RB:
<code>(e₀⊕(e₁⊕e₂))⊕((e₃⊕e₄)⊕(e₅⊕e₆))</code></p>
<p>Both trees have depth 3 (= ⌈log₂ 7⌉). Both perform exactly 6
applications of <code>binary_op</code>. The error bounds are identical.
The only difference is which element carries: IPR carries
<code>e₆</code> (the last element, locally determined), while recursive
bisection splits at <code>floor(7/2) = 3</code>, changing the pairing of
<code>e₀</code> (a globally determined property — the pairing of the
first element depends on the total count).</p>
<p><strong>This paper specifies iterative pairwise</strong> because it
aligns with existing practice in high-performance libraries that already
provide deterministic reduction modes. This minimizes disruption for
implementers and users who have existing workflows built around these
libraries.</p>
<p>Throughput considerations and regret analysis for this choice are
discussed in Appendix P.</p>
<h4 id="streaming-evaluation">3.8.2 Streaming evaluation</h4>
<p><strong>The fundamental structural difference.</strong> The most
significant distinction between the two candidates is not performance or
symmetry — it is that iterative pairwise is an <em>online</em> algorithm
and recursive bisection is a <em>batch</em> algorithm.</p>
<p>Recursive bisection’s first operation is to compute a midpoint:
<code>mid = N/2</code>. The entire split tree is determined top-down
from the global sequence length. Evaluation cannot begin until N is
known, and the split structure depends on the global property of the
input.</p>
<p>Iterative pairwise (shift-reduce) processes elements incrementally:
each element is shifted onto an O(log N) stack, and reductions are
triggered by a branchless bit-test on the running count
(<code>ntz(count)</code>). The stack captures the complete computation
state at any point. N is not needed until termination, when remaining
stack entries are collapsed.</p>
<p>This structural property has concrete consequences for how C++ is
evolving:</p>
<ol type="1">
<li><strong>Forward ranges without <code>size()</code>:</strong> IPR can
begin reducing immediately with a single forward pass. Recursive
bisection requires either a counting pass or random access to compute
split points.</li>
<li><strong>Chunked executor evaluation:</strong> An executor delivering
data in chunks can feed each chunk into the shift-reduce loop. The stack
state is the continuation — evaluation can pause and resume at any chunk
boundary. Recursive bisection requires the global N before reduction can
begin, forcing a synchronization barrier.</li>
<li><strong>P2300 sender/receiver composition:</strong> P2300
(<code>std::execution</code>, adopted for C++26) explicitly separates
<em>what</em> to execute from <em>where</em> to execute it, and senders
describe composable units of asynchronous work. A streaming reduction
using IPR is naturally expressible as a sender that consumes elements as
they flow through a pipeline. Recursive bisection requires knowing the
complete input before constructing the expression — it cannot compose
incrementally.</li>
<li><strong>Heterogeneous and distributed execution:</strong> Data
arriving asynchronously from GPU kernels, network reduction, or
distributed aggregation can be consumed incrementally by IPR. Recursive
bisection must buffer the complete sequence before computation
begins.</li>
</ol>
<p>In a standard moving toward sender/receiver pipelines and composable
asynchronous execution, the tree shape that can be evaluated without
materializing the entire sequence is the one structurally aligned with
the future of C++ execution. This is an architectural property that
determines whether the canonical expression can participate in streaming
pipelines; the returned value is nonetheless defined with respect to the
final N.</p>
<p>[<em>Note:</em> The current normative definition (§4) is stated in
terms of <code>K = ceil(N/L)</code>, so the specification as written
requires a well-defined N (§2.2). However, the streaming property is
inherent to the iterative pairwise algorithm: lane assignment
(<code>i mod L</code>) is known per-element, and the shift-reduce
procedure within each lane builds the tree incrementally without
foreknowledge of the lane’s element count. N is needed only to determine
when to stop and collapse remaining stack entries. Future API revisions
may exploit this property to weaken iterator and sizing requirements.
<em>—end note</em>]</p>
<p><strong>Lane-locality of streaming evaluation.</strong> Because lane
assignment is determined solely by the input index
(<code>i mod L</code>), each element belongs to exactly one lane, and
the shift-reduce stack maintained for that lane captures the complete
canonical subtree for all elements of that lane observed so far.
Streaming evaluation therefore preserves the expression: inter-lane
ordering is determined only by Stage 2 (§4.4), which is itself evaluated
by the same tree rule after all per-lane reductions complete. Chunked or
incremental delivery of elements does not alter the abstract expression,
provided that elements are presented in increasing input index
order.</p>
<p>These observations describe implementation properties of the
iterative pairwise formulation. They do not impose added semantic
requirements beyond those stated in §4–§5.</p>
<h3 id="deferral-of-api-surface">3.9 Deferral of API surface</h3>
<p>This paper acknowledges the importance of a Ranges-first model in
modern C++, but the specific API surface (new algorithm, new execution
policy, or range-based overload) is deliberately deferred to a later
revision. Expression structure is orthogonal to API surface; fixing it
first allows API alternatives to be evaluated against a stable semantic
baseline. Once LEWG reaches consensus on this semantic foundation, a
follow-on revision can propose an API that aligns with modern library
patterns.</p>
<h3 id="relationship-to-executors">3.10 Relationship to Executors</h3>
<p>The expression/algorithm/execution separation described in §3.0
aligns naturally with the sender/receiver execution model adopted for
C++26 (P2300).</p>
<p>P2300 (<code>std::execution</code>) addresses the execution concern:
<em>where</em> work runs, <em>when</em> it runs, <em>how</em> it is
scheduled, and what progress guarantees apply. Its stated design
principle — “address the concern of <em>what</em> to execute separately
from the concern of <em>where</em>” — is the same separation this
proposal formalizes for reduction expressions. In the absence of a
specified expression, different schedulers may legitimately induce
different groupings for a reduction, leading to scheduler-dependent
results.</p>
<p>By fixing the abstract expression in the algorithm, this proposal
provides executors with a stable semantic target:</p>
<ul>
<li>The <strong>algorithm</strong> defines <em>what</em> computation
must be performed.</li>
<li>The <strong>executor</strong> determines <em>how</em> that
computation is evaluated.</li>
</ul>
<p>Different schedulers — CPU thread pool, GPU stream, or distributed
sender chain — may execute the expression using different physical
strategies, but they must produce the same returned value for a fixed
expression and floating-point evaluation model.</p>
<p>In this sense, the proposal is orthogonal and complementary to P2300.
It does not constrain execution; it enables consistency across
schedulers when the same canonical expression (fixed <code>L</code>) and
floating-point evaluation model are used. The streaming property of
iterative pairwise (§3.8) is particularly relevant here: the canonical
expression can be evaluated incrementally as data flows through a sender
pipeline, without requiring materialization of the entire sequence prior
to evaluation; the returned value is defined with respect to the final
sequence length N.</p>
<h2 id="fixed-expression-structure-canonical-reduction-expression-canonical.reduce">4.
Fixed Expression Structure (Canonical Reduction Expression)
[canonical.reduce]</h2>
<p><strong>Normative.</strong> Sections §4–§5 specify the semantic
contract of this proposal. All other sections are informative.</p>
<p><strong>Summary.</strong> For input sequence <code>X[0..N)</code>,
binary operation <code>op</code>, and lane count <code>L</code>:</p>
<ol type="1">
<li><strong>Partition</strong> <code>X</code> into <code>L</code>
interleaved lanes by <code>i mod L</code>. Element <code>X[i]</code> is
assigned to lane <code>i mod L</code>, preserving input order within
each lane.</li>
<li><strong>Stage 1 — Per-lane reduction.</strong> For each lane
<code>j</code> in <code>[0, L)</code>, evaluate
<code>CANONICAL_TREE_EVAL(op, Y_j)</code> where <code>Y_j</code> has
<code>K = ceil(N/L)</code> positions. Absent trailing positions (when
<code>N</code> is not a multiple of <code>L</code>) propagate without
invoking <code>op</code>.</li>
<li><strong>Stage 2 — Cross-lane reduction.</strong> Reduce the
<code>L</code> lane results <code>R[0..L)</code> using
<code>CANONICAL_TREE_EVAL(op, R[0..L))</code>.</li>
<li><strong>Init integration.</strong> If <code>init</code> is provided,
return <code>op(init, R_all)</code>. Otherwise, return
<code>R_all</code>.</li>
</ol>
<p><code>CANONICAL_TREE_EVAL</code> is an iterative pairwise
(shift-reduce) tree: pair adjacent elements left-to-right, carry any odd
trailing element, repeat. The tree shape is fully determined by the
element count. Only the abstract expression structure is specified. When
evaluated with an execution policy, independent subexpressions may be
evaluated concurrently or vectorized, provided the returned value
matches that of the canonical expression. The remainder of §4 defines
each component precisely.</p>
<p>[<em>Note:</em> The semantic contract in §4 is defined in terms of
the returned value. The invocation count is specified (§4.2.2). <em>—end
note</em>]</p>
<p><strong>Sequencing of <code>binary_op</code> invocations.</strong>
When evaluated without an execution policy, invocations of
<code>binary_op</code> are sequenced in an order consistent with
evaluation of the canonical expression defined in §4. When evaluated
with an execution policy, invocations of <code>binary_op</code> may be
unsequenced or concurrent; the returned value shall match the result of
evaluating the canonical expression defined in §4. Side effects of
<code>binary_op</code> are observable in invocation order only for the
non-policy overload.</p>
<p><strong>Non-commutative operations.</strong> The canonical expression
fixes the left and right operand at every <code>op(a, b)</code> node in
the tree. For non-commutative <code>binary_op</code>, the operand order
is fully determined by the tree structure defined in §4.2–§4.4;
scheduling may vary (under an execution policy) but must produce the
same returned value as evaluating the operand-ordered tree. Init
placement (§4.5) is likewise fixed: <code>op(init, R_all)</code>, with
<code>init</code> as the left operand.</p>
<p><strong>Overview: Two-stage reduction</strong></p>
<p>The reduction proceeds in two stages over an interleaved lane
partition. The input sequence of <code>N</code> elements is distributed
across <code>L</code> lanes by index modulo (element <code>i</code> →
lane <code>i mod L</code>). In Stage 1, each lane independently reduces
its elements using the canonical tree shape. In Stage 2, the
<code>L</code> lane results are themselves reduced using the same tree
rule. Both stages use <code>CANONICAL_TREE_EVAL</code> (§4.2.3),
ensuring a fully determined expression from input to result.</p>
<pre class="text"><code>Example: N = 12, L = 4 (3 elements per lane)

Input:  e₀  e₁  e₂  e₃  e₄  e₅  e₆  e₇  e₈  e₉  e₁₀ e₁₁
Lane:    0   1   2   3   0   1   2   3   0   1   2   3

                    ┌─── Stage 1: Per-lane canonical trees ───┐

        Lane 0          Lane 1          Lane 2          Lane 3
          op              op              op              op
         /  \            /  \            /  \            /  \
        op  e₈          op  e₉          op  e₁₀         op  e₁₁
       / \             / \             / \             / \
      e₀  e₄         e₁  e₅         e₂  e₆         e₃  e₇

        ↓ R₀            ↓ R₁            ↓ R₂            ↓ R₃

                    ┌─── Stage 2: Cross-lane canonical tree ──┐

                              op
                            /    \
                          op      op
                         /  \    /  \
                        R₀  R₁  R₂  R₃

                              ↓
                           result</code></pre>
<p>The lane count <code>L</code> is the single topology parameter that
determines the shape of the entire computation. When <code>L = 1</code>,
the two-stage structure collapses to a single canonical tree over the
entire input sequence (one lane, no cross-lane reduction). Sections
§4.1–§4.4 define each component precisely; §4.3.4 addresses ragged tails
when <code>N</code> is not a multiple of <code>L</code>.</p>
<p>[<em>Note:</em> <code>L = 1</code> denotes a single canonical tree
over the full input, not <code>std::accumulate</code>’s left-fold; for
non-associative operations these expressions yield different results.
<em>—end note</em>]</p>
<p>This specification defines the canonical expression structure only;
it does not prescribe an evaluation strategy. However, the iterative
pairwise formulation can be evaluated efficiently across threads while
preserving the tree. When each thread processes a power-of-two-sized
chunk of the input, the shift-reduce process within that chunk collapses
to a single completed subtree — the smallest possible merge state.
Adjacent chunk results can then be combined in index order, recovering
the expression regardless of thread count. A detailed parallel
realization strategy is described in Appendix N.</p>
<h3 id="canonical-tree-shape-canonical.reduce.tree">4.1 Canonical tree
shape [canonical.reduce.tree]</h3>
<p>To mirror the style used in the Numerics clauses
(e.g. <code>GENERALIZED_SUM</code>), this paper introduces definitional
functions used solely to specify required parenthesization and
left/right operand order. These functions do not propose Standard
Library names.</p>
<p>For a fixed input order, lane count <code>L</code>, and
<code>binary_op</code>, the abstract expression (parenthesization and
left/right operand order) is uniquely determined by:</p>
<ol type="1">
<li><strong>Interleaved lanes</strong>: for a lane count <code>L</code>,
the input positions are partitioned into <code>L</code> logical
subsequences (lanes) based on <code>i % L</code>, preserving input order
within each lane (§4.3), followed by a second canonical reduction over
lane results (§4.4).</li>
<li><strong>A canonical iterative pairwise tree</strong> defined by a
standard-fixed algorithm (§4.2.3).</li>
</ol>
<p>[<em>Note:</em> The iterative pairwise-with-carry evaluation order
used by this paper corresponds to the well-known
<strong>shift-reduce</strong> (carry-chain / binary-counter stack)
formulation of pairwise summation described by Dalton, Wang, and Blainey
[Dalton2014]. <em>—end note</em>]</p>
<p>A near-balanced tree over a non-power-of-two number of leaves
implicitly contains operand positions for which no input element is
present. This paper makes that <em>absence</em> explicit in the semantic
model: when the tree geometry requires combining two operands but one
operand is absent, <code>binary_op</code> is not applied and the present
operand is propagated unchanged (§4.2.2). This avoids padding values and
imposes no identity-element requirements on <code>binary_op</code>.</p>
<h3 id="canonical-tree-reduction-with-absent-operands-canonical.reduce.tree.absent">4.2
Canonical tree reduction with absent operands
[canonical.reduce.tree.absent]</h3>
<h4 id="canonical-split-rule-pairing-count-per-round-canonical.reduce.tree.split">4.2.1
Canonical split rule (pairing count per round)
[canonical.reduce.tree.split]</h4>
<p>Given a working sequence of length <code>n</code>, define the number
of pairs formed in each round:</p>
<ul>
<li>let <code>h = floor(n / 2)</code>.</li>
</ul>
<p>The split rule <code>h = floor(n / 2)</code> is normative and governs
the number of pairs formed in each round of the iterative pairwise
algorithm (§4.2.3). It does not by itself determine the tree shape; the
complete tree is determined by the iterative pairing and carry logic
defined in §4.2.3.</p>
<h4 id="lifted-combine-on-absent-operands-canonical.reduce.tree.combine">4.2.2
Lifted combine on absent operands [canonical.reduce.tree.combine]</h4>
<p>Let <code>maybe&lt;A&gt;</code> be a conceptual domain with values
either <code>present(a)</code> (for <code>a</code> of type
<code>A</code>) or <code>∅</code> (“absent”). The
<code>maybe&lt;A&gt;</code> and <code>∅</code> notation are purely
definitional devices used to specify the handling of non-power-of-two
inputs without requiring an identity element; they do not appear in any
proposed interface and implementations need not model absence
explicitly.</p>
<p>Define the lifted operation <code>COMBINE(op, u, v)</code> on
<code>maybe&lt;A&gt;</code>:</p>
<ul>
<li><code>COMBINE(op, ∅, x) = x</code></li>
<li><code>COMBINE(op, x, ∅) = x</code></li>
<li><code>COMBINE(op, ∅, ∅) = ∅</code></li>
<li><code>COMBINE(op, present(x), present(y)) = present( op(x, y) )</code></li>
</ul>
<p>[<em>Note:</em> In implementation terms, <code>COMBINE</code> is the
familiar SIMD tail-masking or epilogue pattern: when an input sequence
does not fill the last group evenly, the implementation skips the
missing positions rather than fabricating values. The formalism above
specifies this behavior precisely without prescribing the implementation
technique (predicated lanes, scalar epilogue, masked operations, etc.).
<em>—end note</em>]</p>
<p>This lifted operation does not require <code>binary_op</code> to have
an identity element. Absence is a property of the expression geometry
(whether an operator application exists), not a property of
<code>binary_op</code>.</p>
<p>The use of <code>∅</code> is a definitional device. For any given
<code>(N, L)</code>, the locations of absent leaves are fully determined
(they occur only in the ragged tail, §4.3.4). Implementations can
therefore handle the tail using an epilogue or masking, without
introducing per-node conditionals in the main reduction.</p>
<p>[<em>Note:</em> The <code>maybe&lt;A&gt;</code> formalism is a
specification device analogous to <code>std::optional&lt;A&gt;</code>;
it is not a proposed API type and implementations need not materialize
optional objects. <em>—end note</em>]</p>
<p>Implementations need not lift <code>binary_op</code> into an optional
domain. A common strategy is to apply the canonical tree rule over full
SIMD groups and evaluate any ragged tail using a scalar iterative
pairwise loop. This naturally satisfies the invocation constraint
without introducing additional branches into the vectorized
reduction.</p>
<p><strong>Invocation constraint.</strong> A conforming implementation
shall not invoke <code>binary_op</code> for any operand position
corresponding to an absent leaf. Propagation through an absent operand
(the first three cases of <code>COMBINE</code> above) does not
constitute an application of <code>binary_op</code>. For any input
sequence of length <code>N &gt; 0</code>, evaluation of the canonical
expression performs exactly <code>N − 1</code> invocations of
<code>binary_op</code>. This requirement applies only to evaluations
that complete normally. If evaluation exits via an exception or calls
<code>std::terminate</code> under a policy-based execution mode, no
invocation-count guarantee is made.</p>
<p>[<em>Note:</em> This constraint is normative. An implementation that
pads absent positions with a value (e.g., an identity element) and
applies <code>binary_op</code> to the padded operands would produce the
same returned value for <code>binary_op</code> operations that possess
an identity but would invoke <code>binary_op</code> more times than
specified. Such an implementation does not conform to this
specification, because it violates the invocation count and may produce
observable differences for stateful <code>binary_op</code>. <em>—end
note</em>]</p>
<p><strong>What constitutes an invocation.</strong> An invocation of
<code>binary_op</code> occurs when and only when it is evaluated with
two present operands corresponding to nodes of the canonical tree.
Evaluations performed by an implementation on internal padding, identity
elements, or masked operands do not constitute invocations of
<code>binary_op</code> and are not constrained by this
specification.</p>
<h4 id="canonical-tree-evaluation-iterative-pairwise-canonical.reduce.tree.eval">4.2.3
Canonical tree evaluation: Iterative Pairwise
[canonical.reduce.tree.eval]</h4>
<p>This subsection defines the canonical tree-building algorithm using
iterative pairwise reduction. The algorithm pairs adjacent elements
left-to-right in each round, carrying the odd trailing element to the
next round. This corresponds to the shift-reduce summation pattern
described by Dalton, Wang, and Blainey [Dalton2014] and matches the
shuffle-down pattern used in CUDA CUB and GPU warp reductions (see §3.8
for rationale).</p>
<p>Define <code>CANONICAL_TREE_EVAL(op, Y[0..k))</code>, where
<code>k &gt;= 1</code> and each <code>Y[t]</code> is in
<code>maybe&lt;A&gt;</code>:</p>
<pre><code>CANONICAL_TREE_EVAL(op, Y[0..k)):
    if k == 1:
        return Y[0]
    
    // Iteratively pair adjacent elements until one remains
    let W = Y[0..k)  // working sequence (conceptual copy)
    while |W| &gt; 1:
        let n = |W|
        let h = floor(n / 2)
        // Pair elements: W&#39;[i] = COMBINE(op, W[2i], W[2i+1]) for i in [0, h)
        let W&#39; = [ COMBINE(op, W[2*i], W[2*i + 1]) for i in [0, h) ]
        // If n is odd, carry the last element
        if n is odd:
            W&#39; = W&#39; ++ [ W[n-1] ]
        W = W&#39;
    return W[0]</code></pre>
<p>When <code>CANONICAL_TREE_EVAL</code> returns <code>∅</code>, it
denotes that no present operands existed in the evaluated subtree.</p>
<p><strong>Shift-reduce state table (k = 8):</strong></p>
<p>The following table illustrates the iterative pairwise algorithm step
by step for <code>k = 8</code> elements, adapted from [Dalton2014]
Figure 4. Each step either <em>shifts</em> (pushes an element onto the
stack) or <em>reduces</em> (combines the top two stack entries of the
same tree level). Lowercase letters denote intermediate results at
successively higher levels of the tree: <code>a</code> terms are sums of
two elements, <code>b</code> terms are sums of two <code>a</code> terms,
and so on.</p>
<pre class="text"><code>Sequence                    Stack                 Operation
─────────────────────────── ───────────────────── ────────────────────
e₀ e₁ e₂ e₃ e₄ e₅ e₆ e₇   ∅                     shift e₀
e₁ e₂ e₃ e₄ e₅ e₆ e₇      e₀                    shift e₁
e₂ e₃ e₄ e₅ e₆ e₇         e₀ e₁                 reduce a₀ = op(e₀, e₁)
e₂ e₃ e₄ e₅ e₆ e₇         a₀                    shift e₂
e₃ e₄ e₅ e₆ e₇            a₀ e₂                 shift e₃
e₄ e₅ e₆ e₇               a₀ e₂ e₃              reduce a₁ = op(e₂, e₃)
e₄ e₅ e₆ e₇               a₀ a₁                 reduce b₀ = op(a₀, a₁)
e₄ e₅ e₆ e₇               b₀                    shift e₄
e₅ e₆ e₇                  b₀ e₄                 shift e₅
e₆ e₇                     b₀ e₄ e₅              reduce a₂ = op(e₄, e₅)
e₆ e₇                     b₀ a₂                 shift e₆
e₇                         b₀ a₂ e₆              shift e₇
∅                          b₀ a₂ e₆ e₇           reduce a₃ = op(e₆, e₇)
∅                          b₀ a₂ a₃              reduce b₁ = op(a₂, a₃)
∅                          b₀ b₁                 reduce c₀ = op(b₀, b₁)
∅                          c₀                    done</code></pre>
<p>The final result <code>c₀</code> is the value of the canonical
expression. The number of reductions after the <em>n</em>-th shift is
determined by the number of trailing zeros in the binary representation
of <em>n</em> [Dalton2014].</p>
<p><strong>Tree diagram (k = 8):</strong></p>
<p>The state table above produces the following canonical expression
tree — a perfectly balanced binary tree for power-of-two
<code>k</code>:</p>
<pre class="text"><code>                    op (c₀)
                  /        \
           op (b₀)          op (b₁)
           /     \          /     \
       op (a₀) op (a₁)  op (a₂) op (a₃)
        / \     / \       / \     / \
       e₀ e₁  e₂ e₃    e₄ e₅  e₆ e₇</code></pre>
<p>Expression:
<code>((e₀ ⊕ e₁) ⊕ (e₂ ⊕ e₃)) ⊕ ((e₄ ⊕ e₅) ⊕ (e₆ ⊕ e₇))</code></p>
<p><strong>Tree diagram (k = 7, non-power-of-two):</strong></p>
<p>When <code>k</code> is not a power of two, the odd trailing element
is carried forward, producing a slightly unbalanced tree. This is where
the carry logic in the algorithm definition above determines the
canonical shape:</p>
<pre class="text"><code>                 op
               /    \
              op      op
             /  \    /  \
            op   op  op  e₆
           / \  / \  / \
          e₀ e₁ e₂ e₃ e₄ e₅</code></pre>
<p>Expression:
<code>((e₀ ⊕ e₁) ⊕ (e₂ ⊕ e₃)) ⊕ ((e₄ ⊕ e₅) ⊕ e₆)</code></p>
<p>The left subtree is identical to the <code>k = 8</code> case with the
last element removed. The carry of <code>e₆</code> at round 1 (odd
<code>n = 7</code>) places it as the right child of the right subtree’s
right branch.</p>
<h4 id="canonical-tree-diagrams-informative">4.2.4 Canonical tree
diagrams (informative)</h4>
<p>The following diagrams illustrate the fixed abstract expression
structure only; they do not imply any particular evaluation order,
scheduling, or implementation strategy.</p>
<pre class="text"><code>Legend (informative)

present(x)  : a present operand holding value x (type A)
∅           : an absent operand position (no input element exists there)
combine(u,v) : lifted combine:
              - combine(∅, x) = x
              - combine(x, ∅) = x
              - combine(∅, ∅) = ∅
              - combine(x, y) = op(x, y) when both present
op(a,b)     : the user-supplied binary_op, applied only when both operands exist</code></pre>
<p>Example: absence propagation with k = 5, Y = [ X0, X1, X2, ∅, ∅ ]</p>
<pre class="text"><code>              COMBINE          COMBINE(op(op(X0,X1), X2), ∅) = op(op(X0,X1), X2)
             /       \
         COMBINE      ∅       carried from round 2 (odd n=3)
          /    \
        op    COMBINE          COMBINE(X2, ∅) = X2
       / \     / \
      X0  X1  X2  ∅</code></pre>
<p>Result: <code>op(op(X0, X1), X2)</code> — two <code>binary_op</code>
calls from three present elements. The two <code>∅</code> positions at
different tree levels each induce no application of
<code>binary_op</code>; the lifted COMBINE rule absorbs them
uniformly.</p>
<p>A near-balanced tree is not necessarily full; missing leaves may
induce absent operands at internal combine points as the tree reduces.
The lifted combine rule handles this uniformly.</p>
<h3 id="interleaved-topology-canonical.reduce.interleaved">4.3
Interleaved topology [canonical.reduce.interleaved]</h3>
<p>Let <code>E[0..N)</code> denote the input elements in iteration order
and let <code>X[0..N)</code> denote the corresponding conceptual terms
of the reduction expression (materialization and the reduction state
type are defined in §4.6).</p>
<h4 id="lane-partitioning-by-index-modulo-canonical.reduce.interleaved.partition">4.3.1
Lane partitioning by index modulo
[canonical.reduce.interleaved.partition]</h4>
<p>For each lane index <code>j</code> in <code>[0, L)</code>,
define:</p>
<pre class="text"><code>I_j = &lt; i in [0, N) : (i mod L) == j &gt;, ordered by increasing i.
X_j = &lt; X[i] : i in I_j &gt;.</code></pre>
<p>This preserves the original input order within each lane
(equivalently, <code>X_j</code> contains positions
<code>j, j+L, j+2L, ...</code> that are less than <code>N</code>).</p>
<h4 id="fixed-length-lane-leaves-single-shape-per-lane-canonical.reduce.interleaved.leaves">4.3.2
Fixed-length lane leaves (single shape per lane)
[canonical.reduce.interleaved.leaves]</h4>
<p>Define <code>K = ceil(N / L)</code> when <code>N &gt; 0</code>
(equivalently, <code>K = (N + L − 1) / L</code> for integers). (The
<code>N == 0</code> case is handled by §4.5 and does not form
lanes.)</p>
<p>For each lane <code>j</code> in <code>[0, L)</code>, define a
fixed-length conceptual sequence <code>Y_j[0..K)</code> of
<code>maybe&lt;A&gt;</code> leaves:</p>
<ul>
<li>for <code>t</code> in <code>[0, K)</code>:
<ul>
<li>let <code>i = j + t*L</code></li>
<li>if <code>i &lt; N</code>: <code>Y_j[t] = present( X[i] )</code></li>
<li>otherwise: <code>Y_j[t] = ∅</code></li>
</ul></li>
</ul>
<p>Thus <strong>all lanes use the same canonical tree shape over
<code>K</code> leaf positions</strong>. Lanes with fewer than
<code>K</code> elements simply have trailing <code>∅</code> leaves;
these do not introduce padding values and do not require
identity-element properties of <code>binary_op</code>.</p>
<p>[<em>Note:</em> Implementations must not pad absent operand positions
with a constant value (e.g., <code>0.0</code> for addition) unless that
value is the identity element for the specific <code>binary_op</code>
and argument types. The lifted <code>COMBINE</code> rules (§4.2.2)
define the correct handling of absent positions for arbitrary
<code>binary_op</code>. The demonstrators in Appendix K use zero-padding
only because they test <code>std::plus&lt;double&gt;</code> with
non-negative inputs, for which <code>+0.0</code> is a suitable padding
value; this shortcut does not generalize (e.g., IEEE-754 signed zero).
<em>—end note</em>]</p>
<p>When <code>N &lt; L</code>, some lane indices <code>j</code>
correspond to no input positions: <code>Y_j[t] == ∅</code> for all
<code>t</code>, yielding <code>R_j == ∅</code>. No applications of
<code>binary_op</code> are induced for such lanes under the lifted
<code>COMBINE</code> rules.</p>
<p>[<em>Note:</em> When <code>L &gt; N</code>, <code>K = 1</code> and
each lane holds at most one element. Stage 1 performs no applications of
<code>binary_op</code> (each lane result is either a single present
value or <code>∅</code>). Stage 2 then applies
<code>CANONICAL_TREE_EVAL</code> over the <code>L</code> lane results,
of which only <code>N</code> are present; the <code>COMBINE</code> rules
propagate the <code>L − N</code> absent entries without invoking
<code>binary_op</code>. The result is therefore equivalent to
<code>CANONICAL_TREE_EVAL</code> applied directly to the <code>N</code>
input elements. In this regime <code>L</code> has no observable effect
on the returned value. This is intentional: no diagnostic is required,
and implementations need not special case it. In this regime, multiple
lane counts may denote identical abstract expressions; this does not
affect the determinism guarantee. <em>—end note</em>]</p>
<h4 id="interleaving-layout-diagrams-informative">4.3.3 Interleaving
layout diagrams (informative)</h4>
<p>Example: <code>N = 10</code>, <code>L = 4</code> ⇒
<code>K = ceil(10/4) = 3</code></p>
<pre class="text"><code>Input order (i):   0   1   2   3   4   5   6   7   8   9
Elements X[i]:    X0  X1  X2  X3  X4  X5  X6  X7  X8  X9
Lane (i mod L):    0   1   2   3   0   1   2   3   0   1

Lanes preserve input order within each lane:

Lane 0: X0  X4  X8
Lane 1: X1  X5  X9
Lane 2: X2  X6
Lane 3: X3  X7</code></pre>
<p>Fixed-length leaves with absence (no padding values):</p>
<pre class="text"><code>Y_0: [ present(X0), present(X4), present(X8) ]
Y_1: [ present(X1), present(X5), present(X9) ]
Y_2: [ present(X2), present(X6), ∅           ]
Y_3: [ present(X3), present(X7), ∅           ]</code></pre>
<h4 id="ragged-tail-handling-canonical.reduce.interleaved.ragged">4.3.4
Ragged tail handling [canonical.reduce.interleaved.ragged]</h4>
<p>When <code>N</code> is not a multiple of <code>L</code>, the final
group of input elements is incomplete: some lanes receive one fewer
element than others, producing a ragged trailing edge across the lane
partition. The canonical expression handles this uniformly through the
<code>maybe&lt;A&gt;</code> formalism defined in §4.2.2. Every lane uses
the same tree shape over <code>K = ceil(N/L)</code> leaf positions, but
lanes whose element count falls short of <code>K</code> have trailing
<code>∅</code> leaves. The <code>COMBINE</code> rules propagate these
absences without invoking <code>binary_op</code> and without requiring
padding values or identity elements from the caller.</p>
<p><strong>Example:</strong> <code>N = 11</code>, <code>L = 4</code>, so
<code>K = ceil(11/4) = 3</code>.</p>
<p>The input is distributed across lanes by <code>i mod L</code>:</p>
<pre class="text"><code>Input:   X0  X1  X2  X3 | X4  X5  X6  X7 | X8  X9  X10
Block:   ──── full ──── | ──── full ──── | ─ ragged ──</code></pre>
<p>Lane assignment:</p>
<pre class="text"><code>Lane 0:  X0,  X4,  X8       (3 elements — full)
Lane 1:  X1,  X5,  X9       (3 elements — full)
Lane 2:  X2,  X6,  X10      (3 elements — full)
Lane 3:  X3,  X7,  ∅        (2 elements + 1 absent)</code></pre>
<p>All four lanes evaluate the same canonical tree shape over
<code>K = 3</code> leaf positions. For lanes 0–2, every leaf is present
and the tree evaluates normally. For lane 3, the tree encounters an
absent leaf:</p>
<pre class="text"><code>      COMBINE
       /    \
     op      ∅
    /   \
  X3    X7</code></pre>
<p><code>COMBINE(op(X3, X7), ∅) = op(X3, X7)</code> — no
<code>binary_op</code> application occurs for the absent position. The
result is identical to reducing only the present elements
<code>[X3, X7]</code>.</p>
<p>This mechanism generalizes to any <code>(N, L)</code> pair. The
number of ragged lanes is <code>L - (N mod L)</code> when
<code>N mod L ≠ 0</code>; these lanes each have exactly one trailing
<code>∅</code>. The remaining <code>N mod L</code> lanes have all
<code>K</code> positions present. When <code>N</code> is a multiple of
<code>L</code>, no lanes are ragged and no <code>∅</code> entries
arise.</p>
<p>In implementation terms, the ragged tail corresponds to the familiar
SIMD epilogue or tail-masking pattern: the final group of elements is
narrower than the full lane width, and the implementation must avoid
reading or combining nonexistent data. The <code>maybe&lt;A&gt;</code>
formalism specifies the required behavior without prescribing the
implementation technique (masking, scalar epilogue, predicated lanes,
etc.).</p>
<h3 id="two-stage-reduction-semantics-canonical.reduce.twostage">4.4
Two-stage reduction semantics [canonical.reduce.twostage]</h3>
<p>The returned value of the canonical reduction is specified as if by
the following sequence of evaluations. This two-stage decomposition is a
definitional structure; conforming implementations may use any execution
strategy (threading, SIMD, GPU, or otherwise) provided the returned
value matches that of the specified expression.</p>
<p>[<em>Note:</em> This paper uses “Stage 1” and “Stage 2” as conceptual
labels for the per-lane and cross-lane phases of the canonical
expression. Future standard wording may adopt different presentational
conventions (e.g., “steps” or “equivalent to”) per established
[numerics.ops] practice; the normative content — the specified
expression — is independent of the labeling. <em>—end note</em>]</p>
<p>Let <code>L &gt;= 1</code> be the lane count and let <code>op</code>
denote the supplied <code>binary_op</code>.</p>
<h4 id="stage-1-per-lane-canonical-reduction-single-tree-shape-canonical.reduce.twostage.perlane">4.4.1
Stage 1 — Per-lane canonical reduction (single tree shape)
[canonical.reduce.twostage.perlane]</h4>
<p>For each lane index <code>j</code> in <code>[0, L)</code>,
define:</p>
<pre class="text"><code>R_j = CANONICAL_TREE_EVAL(op, Y_j)  // returns maybe&lt;A&gt;</code></pre>
<h4 id="stage-2-canonical-reduction-over-lane-results-in-increasing-lane-index-canonical.reduce.twostage.crosslane">4.4.2
Stage 2 — Canonical reduction over lane results (in increasing lane
index) [canonical.reduce.twostage.crosslane]</h4>
<p>Define a conceptual sequence <code>Z[0..L)</code> by:</p>
<ul>
<li><code>Z[j] = R_j</code> for <code>j</code> in
<code>[0, L)</code>.</li>
</ul>
<p>Then define:</p>
<pre class="text"><code>R_all = CANONICAL_TREE_EVAL(op, Z)  // returns maybe&lt;A&gt;</code></pre>
<p>When <code>N &gt; 0</code>, at least one lane contains a present
operand, therefore <code>R_all</code> is <code>present(r)</code> for
some <code>r</code> of type <code>A</code>. The interleaved reduction
result is that <code>r</code>.</p>
<p>Therefore, the overall expression is uniquely determined by the
canonical tree rule applied first within each lane and then across lanes
in increasing lane index order. When <code>L = 1</code>, there is a
single lane containing all <code>N</code> elements; Stage 2 receives one
input and returns it unchanged, so the result is simply
<code>CANONICAL_TREE_EVAL(op, Y_0)</code>.</p>
<p><strong>Summary definition.</strong> For convenience, define the
composite definitional function:</p>
<pre class="text"><code>CANONICAL_INTERLEAVED_REDUCE(L, op, X[0..N)):
    Partition X into L lanes by index modulo (§4.3.1).
    Form fixed-length leaf sequences Y_j[0..K) for each lane j (§4.3.2).
    For each lane j: R_j = CANONICAL_TREE_EVAL(op, Y_j).
    Form Z[0..L) where Z[j] = R_j.
    Return CANONICAL_TREE_EVAL(op, Z).</code></pre>
<p>When <code>N == 0</code>, <code>CANONICAL_INTERLEAVED_REDUCE</code>
is not invoked; the result is determined by §4.5.</p>
<h4 id="two-stage-diagrams-informative">4.4.3 Two-stage diagrams
(informative)</h4>
<p>Stage 1 summary (<code>N = 10</code>, <code>L = 4</code>,
<code>K = 3</code>; all lanes use the same shape; <code>∅</code>
propagates):</p>
<pre class="text"><code>Y_0 = [X0, X4, X8] --(canonical tree k=3)--&gt; R_0
Y_1 = [X1, X5, X9] --(canonical tree k=3)--&gt; R_1
Y_2 = [X2, X6, ∅ ] --(canonical tree k=3)--&gt; R_2
Y_3 = [X3, X7, ∅ ] --(canonical tree k=3)--&gt; R_3</code></pre>
<p>Stage 2 (cross-lane canonical reduction; same rules apply):</p>
<pre class="text"><code>Example: L = 4, Z = [R0, R1, R2, R3]

       combine
       /      \
    combine   combine
    /    \    /    \
   R0    R1  R2    R3</code></pre>
<p>Conceptual completeness: the same lifted rule handles absence in
Stage 2:</p>
<pre class="text"><code>Example: Z = [ R0, ∅, R2, ∅ ]

       combine
       /      \
    combine   combine
    /    \    /    \
   R0    ∅   R2    ∅

combine(R0, ∅) = R0
combine(R2, ∅) = R2
combine(R0, R2) = op(R0, R2)</code></pre>
<h4 id="equivalence-to-reducing-only-present-terms-informative">4.4.4
Equivalence to reducing only present terms (informative)</h4>
<p>For any lane <code>j</code>,
<code>CANONICAL_TREE_EVAL(op, Y_j)</code> evaluates the same abstract
expression as applying the canonical split rule (§4.2.1) to the
subsequence <code>X_j</code> containing only present terms, with the
understanding that absent operand positions do not create
<code>binary_op</code> applications. The explicit absence notation does
not affect the returned value; it makes the “absent operand” behavior
precise and enables a single tree shape for all lanes.</p>
<h4 id="value-propagation-canonical.reduce.twostage.propagation">4.4.5
Value propagation [canonical.reduce.twostage.propagation]</h4>
<p>Each application of <code>binary_op</code> within the canonical
expression is performed as if by:</p>
<pre class="text"><code>op(std::move(lhs), std::move(rhs))</code></pre>
<p>where <code>lhs</code> and <code>rhs</code> are the results of
evaluating the corresponding child subexpressions, each of type
<code>A</code> (§4.6).</p>
<p>Propagation through an absent operand (§4.2.2) does not create
additional copies or moves of the present operand; the present value is
propagated to the parent node unchanged.</p>
<p>[<em>Note:</em> This is consistent with <code>std::reduce</code>,
which specifies that <code>binary_op</code> is applied to moved subrange
results. The move semantics ensure that move-only accumulator types are
supported and that expensive-to-copy types do not incur unnecessary
overhead. Implementations are free to elide moves where permitted by the
as-if rule. <em>—end note</em>]</p>
<h3 id="integration-of-an-initial-value-if-provided-canonical.reduce.init">4.5
Integration of an initial value (if provided)
[canonical.reduce.init]</h3>
<p>If an initial value <code>init</code> is provided, the abstract
expression is:</p>
<ul>
<li>If <code>N == 0</code>: return <code>init</code>.</li>
<li>Otherwise:
<ul>
<li>let <code>R =</code> the value extracted from
<code>R_all = CANONICAL_TREE_EVAL(op, Z)</code> in §4.4.2</li>
<li>let <code>I</code> be a value of type <code>A</code> initialized
from <code>init</code> (where <code>A</code> is defined in §4.6)</li>
<li>return <code>op(I, R)</code>.</li>
</ul></li>
</ul>
<p>The placement of <code>init</code> is normative. In particular,
<code>init</code> is not interleaved into lanes and does not participate
in the tree expression. Combining <code>init</code> with the tree result
in a single final application of <code>binary_op</code> ensures that the
tree shape is independent of whether an initial value is provided.
Placing <code>init</code> as the left operand preserves lane-assignment
invariance under extension of the input sequence and ensures that the
abstract expression stays stable when additional elements are appended
to the range.</p>
<p>If no initial value is provided and <code>N == 0</code>, the semantic
result is not defined by this expression structure; the behavior (e.g.,
yielding a default-constructed <code>A{}</code>, or rendering the call
ill-formed) is deferred to the API specification.</p>
<p>Appending additional elements to the input sequence does not change
the abstract expression for the original elements under this rule. The
left placement of <code>init</code> preserves this stability property
for non-associative operations.</p>
<p>Treating <code>init</code> as an input element would change the lane
assignment of all subsequent elements and therefore alter the abstract
expression.</p>
<p>The left‑operand placement is consistent with existing fold‑style
conventions; because this proposal does not require commutativity of
<code>binary_op</code>, the position of <code>init</code> is specified.
In particular, <code>std::accumulate</code> places <code>init</code> as
the left operand at every step (<code>op(op(init, x₀), x₁)...</code>);
this proposal preserves that convention so that non-commutative
operations produce consistent results when migrating from
<code>std::accumulate</code> to this reduction.</p>
<p>[<em>Note:</em> Whether a convenience form without an explicit
<code>init</code> is provided, and what default it uses, is an API
decision deferred to a future revision. <em>—end note</em>]</p>
<pre class="text"><code>With init (conceptual):

Result = op( init, CANONICAL_INTERLEAVED_REDUCE(L, op, X[0..N)) )

init is not interleaved into lanes and is combined once with the overall result.</code></pre>
<p><em>Informative contrast:</em></p>
<pre class="text"><code>std::accumulate:
  (((init op X0) op X1) op X2) ... op XN-1

This proposal:
  init op ( fixed canonical tree expression over X0..XN-1 )</code></pre>
<p>[<em>Note:</em> This differs structurally from
<code>std::accumulate</code>, where <code>init</code> is the leftmost
operand of a left-fold and participates in every step of the evaluation.
In this proposal, <code>init</code> is applied exactly once, as the left
operand to the completed tree result. While both conventions place
<code>init</code> on the left, <code>std::accumulate</code> threads
<code>init</code> through <code>N</code> applications of
<code>binary_op</code>, whereas this reduction applies
<code>binary_op(I, R)</code> a single time after the tree evaluation.
This is an intentional design choice (see Appendix E for rationale):
integrating <code>init</code> into the tree would alter the tree shape
and break the independence of the expression from the presence or
absence of an initial value. <em>—end note</em>]</p>
<h3 id="materialization-of-conceptual-terms-and-reduction-state-introducing-v-and-a-canonical.reduce.types">4.6
Materialization of conceptual terms and reduction state (introducing V
and A) [canonical.reduce.types]</h3>
<p>The preceding sections (§4.1–§4.4) define the expression structure
over abstract sequences. This section specifies the type rules that
bridge the abstract expression to C++ evaluation.</p>
<p>Let:</p>
<ul>
<li><code>V</code> be the value type of the input sequence
elements,</li>
<li><code>A</code> be the reduction state type:
<ul>
<li>if an initial value <code>init</code> of type T is provided,
<code>A = remove_cvref_t&lt;T&gt;</code>;</li>
<li>otherwise <code>A = remove_cvref_t&lt;V&gt;</code>.</li>
</ul></li>
</ul>
<p>Define the conceptual term sequence <code>X[0..N)</code> of type A by
converting each input element:</p>
<p><strong><code>X[i] = static_cast&lt;A&gt;(E[i])</code></strong> for
<code>i in [0, N)</code>.</p>
<p>All applications of <code>binary_op</code> within the definitional
functions in §4 operate on values of type A.</p>
<p><strong>Constraints:</strong> Let <code>A</code> be the reduction
state type defined above.</p>
<p>When an initial value <code>init</code> is provided, the
initialization <code>A{init}</code> shall be well-formed.</p>
<ul>
<li>Each conceptual term <code>X[i]</code> is formed by conversion to
<code>A</code> as specified above.</li>
<li>If an initial value <code>init</code> is provided, it is
materialized as a value <code>I</code> of type <code>A</code>
initialized from <code>init</code> and participates in the expression as
<code>op(I, R)</code> per §4.5.</li>
<li><code>binary_op</code> shall be invocable with two arguments of type
<code>A</code>, and the result shall be convertible to
<code>A</code>.</li>
</ul>
<p>[<em>Note:</em> How <code>V</code> is derived from the input —
whether as <code>iter_value_t&lt;InputIt&gt;</code> for an iterator-pair
interface, <code>range_value_t&lt;R&gt;</code> for a range interface, or
otherwise — is an API decision deferred to a future revision. The
semantic contract requires only that <code>V</code> is well-defined and
that the conversion <code>static_cast&lt;A&gt;(E[i])</code> is
well-formed. Proxy reference types (e.g.,
<code>std::vector&lt;bool&gt;::reference</code>) and their interaction
with <code>V</code> are likewise API-level concerns. <em>—end
note</em>]</p>
<h2 id="invariance-properties-canonical.reduce.invariance">5. Invariance
Properties [canonical.reduce.invariance]</h2>
<p>This proposal does <strong>not</strong> impose associativity or
commutativity requirements on <code>binary_op</code>. Instead of
permitting implementations to reassociate or reorder (which can make
results unspecified for non-associative operations), this proposal
defines a single abstract expression for fixed <code>(N, L)</code>.
Determinism is obtained by fixing the expression, not by restricting
<code>binary_op</code>.</p>
<p>For a chosen <strong>lane count <code>L</code></strong>, the fixed
expression structure provides:</p>
<p><strong>Topological determinism:</strong> For fixed input order,
<code>binary_op</code>, lane count <code>L</code>, and <code>N</code>,
the abstract expression (grouping and left/right operand order) is fully
specified by §4. It does not depend on implementation choices, SIMD
width, thread count, or scheduling decisions.</p>
<p><strong>Layout invariance:</strong> Results are independent of memory
alignment and physical placement, given the same input sequence as
observed through the iterator range.</p>
<p><strong>Execution independence:</strong> When evaluated with an
execution policy, implementations may evaluate independent subtrees in
any order or concurrently; only the returned value is constrained. When
evaluated without an execution policy, invocations of
<code>binary_op</code> are sequenced consistently with the canonical
expression (§4).</p>
<p><strong>Cross-invocation reproducibility:</strong> Given the same
lane count <code>L</code>, input sequence, <code>binary_op</code>, and
floating-point evaluation model, the returned value is stable across
invocations (it is the value of the same specified expression under the
same evaluation model).</p>
<p><strong>Scope of guarantee (returned value and side
effects):</strong> The run-to-run stability guarantee applies to the
<strong>returned value</strong> of the reduction. For the non-policy
overload, side effects of <code>binary_op</code> are sequenced in an
order consistent with evaluation of the canonical expression. For
execution-policy overloads, side effects may be unsequenced or
concurrent (as with existing parallel algorithms).</p>
<p><strong>Constraints on <code>binary_op</code>:</strong> Let
<code>A</code> be the reduction state type (§4.6). The only requirements
on <code>binary_op</code> are invocability with two arguments of type
<code>A</code> and convertibility of the result to <code>A</code>. No
associativity, commutativity, or identity-element requirements are
imposed.</p>
<p>See §2.3 for exception/termination behavior; in particular, when
<code>std::terminate</code> is called under a policy-based evaluation,
the state of outputs and any externally observable side effects is
unspecified.</p>
<p>See §2.4 for complexity; guarantees are intentionally limited to work
complexity, consistent with <code>std::reduce</code>.</p>
<p>The remaining requirements on iterators, value types, and side
effects match those of the corresponding <code>std::reduce</code>
facility ([numeric.ops.reduce]):</p>
<ul>
<li>When evaluated without an execution policy, <code>binary_op</code>
shall not invalidate iterators or subranges, nor modify elements in the
input range.</li>
<li>When evaluated with an execution policy, <code>binary_op</code> is
an element access function subject to the requirements in
[algorithms.parallel.exec].</li>
</ul>
<p>When evaluated without an execution policy, <code>binary_op</code> is
invoked as part of a normal library algorithm call; this paper does not
require concurrent evaluation. When evaluated with an execution policy,
the requirements of [algorithms.parallel.exec] additionally apply.</p>
<p>The run-to-run stability guarantee applies to the returned value when
<code>binary_op</code> is functionally deterministic — that is, when it
returns the same result for the same operand values. If
<code>binary_op</code> reads mutable global state, uses random number
generation, or is otherwise non-deterministic, the returned value may
vary even with fixed lane count <code>L</code> and input.</p>
<p>[<em>Note:</em> Functional determinism of <code>binary_op</code> is
not a formal requirement (the standard cannot enforce functional
purity), but an observation about when the stability guarantee is
meaningful. <em>—end note</em>]</p>
<p>Cross-platform reproducibility requires users to ensure an identical
lane count <code>L</code> and an equivalent floating-point evaluation
model (§2.5, §6).</p>
<h2 id="floating-point-considerations-informative">6. Floating-Point
Considerations (Informative)</h2>
<p>This facility removes one specific source of variability in parallel
reductions: implementation-permitted reassociation of the reduction
expression. It does not attempt to address all sources of floating-point
non-determinism.</p>
<p><strong>Terminology:</strong> This paper uses <strong>floating-point
evaluation model</strong> to mean the combination of the program’s
runtime floating-point environment (e.g., <code>&lt;cfenv&gt;</code>
rounding mode) and the translation/target choices that affect how
floating-point expressions are evaluated (e.g., contraction/FMA, excess
precision, subnormal handling, fast-math).</p>
<p><strong>What is specified:</strong> For a given topology coordinate
<code>L</code>, input sequence, <code>binary_op</code>, and
<code>init</code>, the result is the value obtained by evaluating the
canonical expression defined in §4, in the floating-point evaluation
model in effect for the program.</p>
<p><strong>What this enables:</strong> By removing library-permitted
reassociation, repeated executions of the same program under a stable
evaluation model can obtain the same result independent of thread count,
scheduling, or SIMD width.</p>
<p><strong>What it does not attempt to specify:</strong>
Cross-architecture bitwise identity is not a goal of this paper. Users
who require bitwise identity must additionally control the relevant
evaluation-model factors and ensure that <code>sizeof(V)</code> (and
thus lane count) is stable across the intended platforms.</p>
<p><strong>Relationship to P3375 (Reproducible floating-point
results):</strong> Davidson’s P3375 [P3375R3] proposes a
<code>strict_float</code> type that specifies sufficient conformance
with ISO/IEC 60559:2020 to guarantee reproducible floating-point
arithmetic across implementations. This proposal and P3375 are
complementary: this paper fixes the <em>expression structure</em>
(parenthesization and operand order) of a parallel reduction, while
P3375 addresses the <em>evaluation model</em> (rounding, contraction,
intermediate precision) for individual operations. Together, they would
provide two key conditions for cross-platform bitwise reproducibility of
parallel reductions. Neither paper alone is sufficient.</p>
<p><strong>Relationship to <code>std::simd</code> (P1928):</strong> This
proposal is orthogonal to <code>std::simd</code>.
<code>std::simd::reduce</code> performs a horizontal reduction within a
single SIMD value with unspecified order; this facility defines a
deterministic expression structure over an arbitrary-length input range.
An implementation may use <code>std::simd</code> operations internally,
but the semantic contract does not depend on <code>std::simd</code>.</p>
<p><strong>Relationship to P0350 (Integrating simd with parallel
algorithms):</strong> Kretz’s P0350 [P0350R4] proposes an
<code>execution::simd</code> policy that processes data in SIMD-width
chunks, parameterized by a width. A natural question is whether the
facility proposed here could be decomposed into a tree-structured
algorithm composed with a SIMD execution policy whose width parameter
supplies this proposal’s lane count L.</p>
<p>This paper takes no position on API decomposition — §4 defines the
semantic contract (what expression is evaluated), and the question of
how L is supplied (template parameter, policy argument, or composition
of algorithm and policy) is explicitly deferred to an API revision. The
two proposals are complementary in intent: P0350 addresses execution
strategy for SIMD hardware; this paper addresses expression structure
for deterministic results. A future API could potentially compose both,
with P0350’s policy supplying execution width and this paper’s semantics
supplying the canonical expression for that width. In particular, if
P0350’s <code>execution::simd&lt;W&gt;</code> policy is adopted, a
possible compositional design would separate the expression semantics
(this proposal) from the SIMD width parameter (P0350), with the policy
supplying the lane count to the algorithm.</p>
<p>The C++ <code>&lt;cfenv&gt;</code> floating-point environment covers
rounding mode and exception flags; many other factors that affect
floating-point results (such as contraction/FMA and intermediate
precision) are translation- or target-dependent and are not fully
specified by the C++ abstract machine. This proposal therefore
guarantees expression identity, not universal bitwise identity.</p>
<h2 id="relationship-to-existing-facilities-informative">7. Relationship
to Existing Facilities (Informative)</h2>
<p>This paper specifies a canonical expression structure for parallel
reduction. The goal is to complete the reduction “semantic spectrum” in
the Standard Library: from specified but sequential, to parallel but
unspecified, to parallel and specified.</p>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 44%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr class="header">
<th>Facility</th>
<th>Parallel</th>
<th>Expression specified</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>std::accumulate</code></td>
<td>No</td>
<td>Yes (left-fold)</td>
<td>Fully specified; sequential</td>
</tr>
<tr class="even">
<td><code>std::reduce</code></td>
<td>Yes</td>
<td>No (generalized sum)</td>
<td>Unspecified grouping; results may vary for non-associative ops</td>
</tr>
<tr class="odd">
<td>HPC frameworks (Kokkos, etc.)</td>
<td>Yes</td>
<td>Not by default (some offer deterministic modes)</td>
<td>Strategy-dependent grouping by default; FP results may vary</td>
</tr>
<tr class="even">
<td>oneTBB <code>parallel_reduce</code></td>
<td>Yes</td>
<td>No</td>
<td>Join order varies; scheduler-dependent results</td>
</tr>
<tr class="odd">
<td><strong>Canonical reduction</strong> (this proposal)</td>
<td>Yes</td>
<td>Yes (§4 tree)</td>
<td>Fixed parenthesization for chosen topology coordinate L; free
scheduling</td>
</tr>
</tbody>
</table>
<p><strong>What this proposal adds:</strong> a standard-specified
expression for parallel reduction, closing the third cell in the table
above.</p>
<blockquote>
<p><strong>Standard specification models for unordered
summation</strong> ([numerics.defns], N5032)</p>
<p><strong><code>GENERALIZED_NONCOMMUTATIVE_SUM(op, a1, …, aN)</code></strong>
— Returns <code>a1</code> when N = 1; otherwise returns
<code>op(GENERALIZED_NONCOMMUTATIVE_SUM(op, a1, …, aK),  GENERALIZED_NONCOMMUTATIVE_SUM(op, aM, …, aN))</code>
for any K where 1 &lt; K+1 = M ≤ N. Operand order is preserved.
Parenthesization (the choice of split point K) is unspecified.</p>
<p><strong><code>GENERALIZED_SUM(op, a1, …, aN)</code></strong> —
Defined as <code>GENERALIZED_NONCOMMUTATIVE_SUM(op, b1, …, bN)</code>
where b1, …, bN may be any permutation of a1, …, aN. Both operand order
and parenthesization are unspecified.</p>
<p>The canonical reduction expression proposed here fixes both the
operand ordering (via interleaved lane assignment, §4) and the
parenthesization (via the balanced binary reduction tree, §4.1) for
reduction.</p>
</blockquote>
<p><strong>Table: Relationship of proposed facility to existing
<code>&lt;numeric&gt;</code> algorithms</strong></p>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="header">
<th>Property</th>
<th><code>accumulate</code></th>
<th><code>inclusive_scan</code> / <code>exclusive_scan</code></th>
<th><code>reduce</code> / <code>transform_reduce</code></th>
<th>Proposed <code>canonical_reduce</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Specification model</strong></td>
<td>Sequential left fold</td>
<td><code>GENERALIZED_NONCOMMUTATIVE_SUM</code></td>
<td><code>GENERALIZED_SUM</code></td>
<td>Canonical reduction expression (§4)</td>
</tr>
<tr class="even">
<td><strong>Operand order</strong></td>
<td>Fixed (left-to-right)</td>
<td>Fixed (left-to-right)</td>
<td>Unspecified (any permutation)</td>
<td>Fixed (interleaved lane assignment)</td>
</tr>
<tr class="odd">
<td><strong>Parenthesization</strong></td>
<td>Fixed (left fold)</td>
<td>Unspecified (any split)</td>
<td>Unspecified (any split)</td>
<td>Fixed (balanced tree, §4.1)</td>
</tr>
<tr class="even">
<td><strong>Unique expression?</strong></td>
<td>Yes</td>
<td>No — reassociation permitted</td>
<td>No — reordering + reassociation permitted</td>
<td>Yes, for a given lane count L</td>
</tr>
<tr class="odd">
<td><strong>Parallel execution?</strong></td>
<td>No (inherently sequential)</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="even">
<td><strong>User parameter</strong></td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>Lane count L</td>
</tr>
</tbody>
</table>
<p>[<em>Note:</em> “Unique expression” means the standard fully
specifies a single abstract expression — one parenthesization and one
operand ordering of <code>binary_op</code> applications.
<code>accumulate</code> specifies a unique left fold. The proposed
<code>canonical_reduce</code> specifies a unique canonical tree for a
given lane count L. Scan and reduce each admit multiple conforming
abstract expressions; an implementation may choose any member of that
set. For non-associative operations (e.g., floating-point addition),
different members of the set may yield different results. <em>—end
note</em>]</p>
<p>[<em>Note:</em> This paper uses
<code>canonical_reduce&lt;L&gt;(...)</code> as the illustrative
spelling, reflecting that lane count <code>L</code> is the semantic
topology coordinate. No Standard Library API is proposed in this paper.
<em>—end note</em>]</p>
<h2 id="motivation-and-use-cases-informative">8. Motivation and Use
Cases (Informative)</h2>
<p>This proposal is motivated by workloads where run-to-run stability
matters, but existing parallel reductions are intentionally free to
choose an evaluation order (and thus may vary with scheduling). Typical
use cases include:</p>
<ul>
<li><strong>CI regression testing:</strong> A reduction that is stable
across runs eliminates intermittent test failures and enables “golden
value” comparisons.</li>
<li><strong>Debugging and bisection:</strong> A stable result makes it
practical to reproduce and minimize numerical regressions without
chasing schedule-dependent drift.</li>
<li><strong>Auditability / reproducible analytics:</strong> Regulatory
or scientific workflows often require re-running computations and
obtaining the same result within a defined environment.</li>
<li><strong>Large-scale simulation and ML training:</strong> Stable
aggregation of large sums (e.g., gradients, risk factors) improves
repeatability of model training and scenario analysis.</li>
<li><strong>Heterogeneous execution:</strong> A single semantic contract
that can be implemented on CPU and GPU enables consistent verification,
even when the execution strategy differs.</li>
</ul>
<p>Detailed examples (with code) are collected in Appendix M.</p>
<h2 id="api-design-space-informative">9. API Design Space
(Informative)</h2>
<p>This section sketches possible directions for exposing the canonical
expression defined in §4. The intent is to build consensus on the
semantic contract before committing to an API surface.</p>
<p>A foundational design constraint is that the semantic topology
parameter is the lane count <code>L</code>. Any API surface must expose
<code>L</code> explicitly to ensure that the user’s chosen abstract
expression stays stable across different compilers, operating systems,
and architectures.</p>
<p>Two primary approaches exist.</p>
<h3 id="new-algorithm-approach-illustrative">9.1 New Algorithm Approach
(Illustrative)</h3>
<p>We propose exposing the semantics as a new algorithm parameterized by
<code>L</code> via a non-type template parameter (NTTP). This ensures
the chosen expression topology is a compile-time property of the
algorithm call.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Illustrative only: name/signature not proposed in this paper</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="kw">namespace</span> std <span class="op">{</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Sequential / Unsequenced evaluation (constexpr friendly)</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>  <span class="kw">template</span> <span class="op">&lt;</span><span class="dt">size_t</span> L<span class="op">,</span> <span class="kw">class</span> InputIt<span class="op">,</span> <span class="kw">class</span> T<span class="op">,</span> <span class="kw">class</span> BinaryOp<span class="op">&gt;</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>  <span class="kw">constexpr</span> T canonical_reduce<span class="op">(</span>InputIt first<span class="op">,</span> InputIt last<span class="op">,</span> T init<span class="op">,</span> BinaryOp op<span class="op">);</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Execution policy overloads</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>  <span class="kw">template</span> <span class="op">&lt;</span><span class="dt">size_t</span> L<span class="op">,</span> <span class="kw">class</span> ExecutionPolicy<span class="op">,</span> <span class="kw">class</span> ForwardIt<span class="op">,</span> <span class="kw">class</span> T<span class="op">,</span> <span class="kw">class</span> BinaryOp<span class="op">&gt;</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>  T canonical_reduce<span class="op">(</span>ExecutionPolicy<span class="op">&amp;&amp;</span> policy<span class="op">,</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>                     ForwardIt first<span class="op">,</span> ForwardIt last<span class="op">,</span> T init<span class="op">,</span> BinaryOp op<span class="op">);</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p><strong>Rationale:</strong> The choice of topology <code>L</code>
alters the abstract expression, which intentionally affects observable
results for non-associative operations (e.g., floating-point addition).
This argues for an API whose contract explicitly includes the topology
coordinate in the type system, rather than treating topology as an
implementation detail or an execution hint.</p>
<h3 id="rationale-why-lane-count-l-is-the-sole-coordinate-the-portability-trap">9.2
Rationale: Why Lane Count (L) is the Sole Coordinate (The Portability
Trap)</h3>
<p>Earlier explorations of this facility considered offering an added
“numerics convenience” parameter <code>M</code> (representing a span in
bytes), where the algorithm would derive the lane count via
<code>L = M / sizeof(value_type)</code>.</p>
<p>This paper explicitly rejects a byte-span (<code>M</code>) API due to
the portability trap it creates.</p>
<p>If a user requests a 64-byte span (e.g.,
<code>canonical_reduce&lt;64&gt;</code>), the derived <code>L</code>
depends entirely on <code>sizeof(T)</code>. Consider
<code>long double</code>:</p>
<ul>
<li>On MSVC (x86_64), <code>sizeof(long double)</code> is 8 bytes.
<code>L</code> becomes 8.</li>
<li>On GCC/Clang (x86_64 Linux), <code>sizeof(long double)</code> is 16
bytes. <code>L</code> becomes 4.</li>
</ul>
<p>Changing <code>L</code> inherently changes the parenthesization and
operand order of the abstract tree. Therefore, an API based on
<code>M</code> would silently generate fundamentally different
mathematical expressions on different platforms for the exact same
source code, destroying the cross-platform run-to-run stability this
paper seeks to provide.</p>
<p>By enforcing <code>L</code> as the sole topology coordinate, the C++
Standard guarantees expression identity:
<code>canonical_reduce&lt;16&gt;(...)</code> evaluates the exact same
tree on an ARM processor as it does on an x86 processor, regardless of
how the underlying types are represented in memory.</p>
<p>If users wish to align <code>L</code> with a specific cache-line or
hardware width, they can explicitly calculate it at the call site (e.g.,
<code>canonical_reduce&lt;64 / sizeof(T)&gt;(...)</code>), making the
representation-size dependency explicit in source code.</p>
<h3 id="execution-policy-approach-illustrative">9.3 Execution Policy
Approach (Illustrative)</h3>
<p>Expose the semantics as a new execution policy (illustrative spelling
only):</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Illustrative only: policy type/spelling not proposed in this paper</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span> <span class="op">&lt;</span><span class="dt">size_t</span> L<span class="op">&gt;</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="kw">struct</span> canonical_policy <span class="op">{</span> <span class="co">/* ... */</span> <span class="op">};</span></span></code></pre></div>
<p>This approach integrates naturally with the existing parallel
algorithms vocabulary. However, as established in §3.0, execution
policies in the current standard are designed to constrain
<em>scheduling</em>, not <em>expression structure</em>. Encoding
topology in a policy would require the policy to carry semantic
guarantees about the returned value — a role that policies do not
currently play. It also raises unresolved questions about policy
composition: what happens when two policies specify conflicting
topologies, or when a topology-carrying policy composes with one that
permits reassociation?</p>
<h3 id="trade-offs-informative">9.4 Trade-offs (Informative)</h3>
<p>The expression/algorithm/execution analysis in §3.0 informs this
trade-off:</p>
<ul>
<li>A dedicated algorithm places expression ownership where the standard
already locates semantic contracts: in the algorithm. The topology
coordinate is explicit, composition questions do not arise, and the
facility can be taught as “this algorithm computes <em>this</em>
expression” — parallel to how <code>std::accumulate</code> computes a
left fold.</li>
<li>A policy-based approach may reduce algorithm surface area but
conflates expression and execution — the same conflation that §3.0
identifies as a source of confusion around <code>std::reduce</code> and
<code>execution::seq</code>. It would also require new rules for policy
dominance and semantic interaction that do not exist in the current
standard.</li>
</ul>
<h3 id="naming-considerations-informative">9.5 Naming Considerations
(Informative)</h3>
<p>This paper uses <code>canonical_reduce&lt;L&gt;(...)</code> as the
illustrative spelling, reflecting that lane count <code>L</code> is the
semantic topology coordinate. Final naming should communicate that:</p>
<ul>
<li>the return value is defined by a specified abstract expression;
and</li>
<li>the topology coordinate is part of the semantic contract.</li>
</ul>
<h3 id="topology-defaults-and-named-presets-informative">9.6 Topology
Defaults and Named Presets (Informative)</h3>
<p>To prevent silent semantic drift across targets, the Standard should
not leave the default topology implementation-defined. Instead, a small
set of standard-fixed named presets for the lane count <code>L</code>
may be provided.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">namespace</span> std <span class="op">{</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">inline</span> <span class="kw">constexpr</span> <span class="dt">size_t</span> canonical_lanes_narrow <span class="op">=</span> <span class="dv">16</span><span class="op">;</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>  <span class="kw">inline</span> <span class="kw">constexpr</span> <span class="dt">size_t</span> canonical_lanes_wide   <span class="op">=</span> <span class="dv">128</span><span class="op">;</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>  <span class="kw">inline</span> <span class="kw">constexpr</span> <span class="dt">size_t</span> canonical_lanes_single <span class="op">=</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>Typical call sites:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> a <span class="op">=</span> <span class="bu">std::</span>canonical_reduce<span class="op">&lt;</span><span class="bu">std::</span>canonical_lanes_narrow<span class="op">&gt;(</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    v<span class="op">.</span>begin<span class="op">(),</span> v<span class="op">.</span>end<span class="op">(),</span> <span class="fl">0.0</span><span class="op">,</span> <span class="bu">std::</span>plus<span class="op">&lt;&gt;{});</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> b <span class="op">=</span> <span class="bu">std::</span>canonical_reduce<span class="op">&lt;</span><span class="bu">std::</span>canonical_lanes_wide<span class="op">&gt;(</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    v<span class="op">.</span>begin<span class="op">(),</span> v<span class="op">.</span>end<span class="op">(),</span> <span class="fl">0.0</span><span class="op">,</span> <span class="bu">std::</span>plus<span class="op">&lt;&gt;{});</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> reference <span class="op">=</span> <span class="bu">std::</span>canonical_reduce<span class="op">&lt;</span><span class="bu">std::</span>canonical_lanes_single<span class="op">&gt;(</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>    v<span class="op">.</span>begin<span class="op">(),</span> v<span class="op">.</span>end<span class="op">(),</span> <span class="fl">0.0</span><span class="op">,</span> <span class="bu">std::</span>plus<span class="op">&lt;&gt;{});</span></span></code></pre></div>
<p>[<em>Note:</em> Appendix J provides an indicative “straw-man” API
using these presets without committing the committee to a final spelling
or placement. <em>—end note</em>]</p>
<h4 id="practical-topology-selection-guidance-informative">Practical
topology selection guidance (informative)</h4>
<p>The semantic coordinate is <code>L</code>. Performance is often
improved when <code>L</code> aligns with the target’s preferred
execution granularity, but the Standard does not prescribe hardware
behavior.</p>
<ul>
<li><strong>CI/CD and cross-platform verification baseline:</strong>
choose <code>L = 1</code>, yielding a single global canonical tree.</li>
<li><strong>Typical CPU deployment:</strong> choose <code>L</code>
matching the target SIMD lane count (e.g., <code>L = 4</code> for
AVX2/double, <code>L = 8</code> for AVX-512/double).</li>
<li><strong>GPU warp-level consistency:</strong> choose
<code>L = warp_width</code> (e.g., <code>L = 32</code> on NVIDIA
GPUs).</li>
</ul>
<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 28%" />
<col style="width: 28%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr class="header">
<th>Use case</th>
<th>L (double)</th>
<th>L (float)</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Golden reference</strong></td>
<td>1</td>
<td>1</td>
<td>Single tree; for debugging/golden values</td>
</tr>
<tr class="even">
<td><strong>Portability baseline</strong></td>
<td>2</td>
<td>4</td>
<td>SSE/NEON width</td>
</tr>
<tr class="odd">
<td><strong>AVX / AVX2</strong></td>
<td>4</td>
<td>8</td>
<td>Desktop/server AVX2</td>
</tr>
<tr class="even">
<td><strong>AVX-512</strong></td>
<td>8</td>
<td>16</td>
<td>AVX-512 servers</td>
</tr>
<tr class="odd">
<td><strong>CUDA warp (double)</strong></td>
<td>32</td>
<td>—</td>
<td>32-thread warp</td>
</tr>
</tbody>
</table>
<h2 id="what-lewg-is-being-asked-to-agree-to">10. What LEWG Is Being
Asked to Agree To</h2>
<p>This section summarizes what LEWG is being asked to agree to. The
polls follow in §11.</p>
<p>We seek agreement that the semantic gap is real, direction on the
semantic contract and tree shape; API surface is explicitly
deferred.</p>
<ul>
<li>The C++ Standard Library lacks a parallel reduction with a specified
abstract expression. We are asking LEWG to validate this gap (Poll 1A)
and the semantics-first approach (Poll 1B).</li>
<li>We are <strong>not</strong> approving an API.</li>
<li>Authors recommend selecting <strong>iterative pairwise</strong> in
Poll 2A to align the canonical shape with common SIMD/GPU reduction
practice.</li>
<li>We are asking LEWG to approve the <strong>fixed expression contract
in §4</strong> (canonical reduction expression) as a semantic building
block: implementations may schedule using threads/SIMD/work-stealing/GPU
kernels but must return the value <strong>as-if evaluating the
standard-fixed expression</strong>.</li>
<li>We intend the canonical expression to have <strong>O(N)</strong>
applications of <code>binary_op</code> (accounting for carry/propagation
rules) and <strong>O(log N)</strong> depth for the tree shape; the
semantic definition imposes <strong>no workspace requirement</strong>
(implementations may use workspace).</li>
<li>A future revision will return with an <strong>API surface</strong>
and <strong>constraints/range categories</strong> aligned with
<code>std::reduce</code> and execution-policy requirements.</li>
</ul>
<h2 id="polls-for-lewg-direction">11. Polls for LEWG Direction</h2>
<p>[<em>Note:</em> The polls below are written for LEWG review. When
presented to SG6, the chair may rephrase polls to focus on
numerics-specific concerns — for example, whether the canonical tree
structure is appropriate for floating-point error analysis, whether the
lane count parameter is a suitable topology coordinate for numerical
reproducibility, and whether the
<code>GENERALIZED_NONCOMMUTATIVE_SUM</code>/<code>GENERALIZED_SUM</code>
distinction is correctly handled. <em>—end note</em>]</p>
<p><em>Vote categories for all Favor/Against polls:</em>
<strong>Strongly Favor / Weakly Favor / Neutral / Weakly Against /
Strongly Against</strong></p>
<h3 id="poll-1a-problem-validation">Poll 1A — Problem validation</h3>
<p><strong>Question:</strong> We agree that the C++ Standard Library
lacks a parallel reduction facility with a specified abstract
expression, and that this is a gap worth closing.</p>
<p><strong>Vote:</strong> Strongly Favor / Weakly Favor / Neutral /
Weakly Against / Strongly Against</p>
<h3 id="poll-1b-semantics-first-scope">Poll 1B — Semantics-first
scope</h3>
<p><strong>Question:</strong> We agree that validating the expression
structure before committing to API design is the right approach for this
facility.</p>
<p><strong>Vote:</strong> Strongly Favor / Weakly Favor / Neutral /
Weakly Against / Strongly Against</p>
<p><em>Interpretation: Poll 1A validates the problem (the semantic gap);
Poll 1B validates the semantics-first scope. Poll 2 validates the
proposed framework for closing the gap.</em></p>
<h3 id="poll-2-core-semantic-contract">Poll 2 — Core semantic
contract</h3>
<p><strong>Question:</strong> We agree that a canonical reduction
facility should define a single fixed abstract expression for a given
input order and topology coordinate, and that the two-stage semantic
structure in §4 (interleaved lane partition, per-lane canonical tree,
cross-lane canonical tree) is a suitable framework for this contract.
Implementations may execute using threads/SIMD/work-stealing/GPU kernels
but must produce results as-if evaluating the specified expression.
(This poll validates the semantic framework; the specific tree shape is
polled separately in Poll 2A.)</p>
<p>[<em>Note:</em> Init placement is specified in §4.5; if desired, LEWG
can poll init placement separately. <em>—end note</em>]</p>
<p><strong>Vote:</strong> Strongly Favor / Weakly Favor / Neutral /
Weakly Against / Strongly Against</p>
<h3 id="poll-2a-canonical-tree-shape">Poll 2A — Canonical tree
shape</h3>
<p><strong>Question:</strong> We agree that iterative pairwise reduction
with carry (§4.2.3) is an acceptable canonical tree shape for this
proposal.</p>
<p>[<em>Note:</em> Recursive bisection (“balanced”) was considered as an
alternative tree construction. It is documented in Appendix O
(informative) for comparison and historical record, but is not proposed
as an alternative in this paper. <em>—end note</em>]</p>
<p>If consensus is not reached on tree shape, the two-stage semantic
framework (Poll 2) remains valid; authors will return with a follow‑up
comparing tree shape alternatives.</p>
<p><strong>Vote:</strong> Strongly Favor / Weakly Favor / Neutral /
Weakly Against / Strongly Against</p>
<h3 id="poll-3-proceed-to-api-surface">Poll 3 — Proceed to API
surface</h3>
<p><strong>Question:</strong> Contingent on favorable outcomes for Polls
1A–2, we agree the authors may return with one or more API surface
proposals, without committing LEWG to adopt a specific surface.</p>
<p><em>Scope of expected API revision:</em></p>
<ul>
<li>New algorithm and/or execution policy approach</li>
<li>Proposed naming and header placement</li>
<li>Initial overload set (iterators/ranges, with/without execution
policy)</li>
<li>Constraints and mandates</li>
<li>Topology selection mechanism (including any named presets and
defaulting rules)</li>
</ul>
<p><strong>Vote:</strong> Strongly Favor / Weakly Favor / Neutral /
Weakly Against / Strongly Against</p>
<h3 id="poll-4a-straw-poll-api-approach-new-algorithm">Poll 4A — Straw
poll: API approach (new algorithm)</h3>
<p>[<em>Note:</em> Polls 4A and 4B are straw polls to guide the
direction of a future API revision, contingent on favorable outcomes for
Polls 1A–3. They do not approve an API in this paper. <em>—end
note</em>]</p>
<p><strong>Question:</strong> For the API surface, we support pursuing a
<strong>new algorithm</strong> approach (topology parameter is visible
in the algorithm contract;
e.g. <code>canonical_reduce&lt;L&gt;(...)</code>).</p>
<p><strong>Vote:</strong> Strongly Favor / Weakly Favor / Neutral /
Weakly Against / Strongly Against</p>
<h3 id="poll-4b-straw-poll-api-approach-execution-policy">Poll 4B —
Straw poll: API approach (execution policy)</h3>
<p><strong>Question:</strong> For the API surface, we support pursuing
an <strong>execution policy</strong> approach (topology is encoded in a
policy type and composes with existing parallel algorithm forms).</p>
<p><strong>Vote:</strong> Strongly Favor / Weakly Favor / Neutral /
Weakly Against / Strongly Against</p>
<p><em>Interpretation (non-normative):</em> If both Poll 4A and Poll 4B
are favored, return with both surfaces (or an algorithm-first surface
plus a policy mapping) and ask LEWG to converge on a primary surface
after reviewing wording/teachability/composition.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>The author thanks Bryce Adelstein Lelbach for a helpful discussion on
the algorithm-vs-execution-policy design trade-off, Matthias Kretz for
detailed review and technical feedback, and the members of SG14 for
their encouragement to prepare a formal paper.</p>
<h2 id="references">References</h2>
<h3 id="c-standard-references">C++ Standard References</h3>
<p>All section references are to the C++26 working draft
<strong>N5032</strong> (Thomas Köppe, ed., “Working Draft, Standard for
Programming Language C++,” 2025).</p>
<ul>
<li><p><strong>[algorithms.general]</strong> — N5032, General
requirements for algorithms. Specifies exception behavior for sequential
algorithm overloads.</p></li>
<li><p><strong>[algorithms.parallel.exceptions]</strong> — N5032,
Exception handling in parallel algorithms. Specifies that
<code>std::terminate</code> is called when an exception escapes during
parallel execution under standard execution policies.</p></li>
<li><p><strong>[algorithms.parallel.exec]</strong> — N5032, Execution
policies. Specifies requirements for element access functions and
parallel execution semantics.</p></li>
<li><p><strong>[numeric.ops.accumulate]</strong> — N5032, Accumulate.
Specifies the left-fold semantics of
<code>std::accumulate</code>.</p></li>
<li><p><strong>[numeric.ops.reduce]</strong> — N5032, Reduce. Specifies
the generalized sum semantics of <code>std::reduce</code>, which permits
reassociation for parallel execution.</p></li>
</ul>
<h3 id="wg21-papers">WG21 Papers</h3>
<ul>
<li><p><strong>[P0350R4]</strong> — Matthias Kretz. “Integrating simd
with parallel algorithms.” WG21 paper P0350R4, 2020. Proposes an
<code>execution::simd</code> policy that processes data in simd-width
chunks. Available at:
https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2020/p0350r4.pdf</p></li>
<li><p><strong>[P1928]</strong> — Matthias Kretz. “std::simd — merge
data-parallel types from the Parallelism TS 2.” WG21 paper P1928.
Available at:
https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2023/p1928r8.pdf</p></li>
<li><p><strong>[P2300]</strong> — Michał Dominiak, Georgy Evtushenko,
Lewis Baker, Lucian Radu Teodorescu, Lee Howes, Kirk Shoop, Michael
Garland, Eric Niebler, Bryce Adelstein Lelbach.
“<code>std::execution</code>.” WG21 paper P2300R10, adopted for C++26 at
the St. Louis plenary, June 2024. Available at:
https://wg21.link/P2300R10</p></li>
<li><p><strong>[P3375R3]</strong> — Guy Davidson. “Reproducible
floating-point results.” WG21 paper P3375R3, 2025. Proposes a
<code>strict_float</code> type specifying sufficient conformance with
ISO/IEC 60559:2020 to guarantee reproducible floating-point arithmetic
across implementations. Available at:
https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3375r3.html</p></li>
</ul>
<h3 id="industry-references">Industry References</h3>
<ul>
<li><p><strong>[IntelCNR]</strong> — Intel Corporation. “Introduction to
Conditional Numerical Reproducibility (CNR).” <em>Intel oneAPI Math
Kernel Library Developer Guide</em>. Available at:
https://www.intel.com/content/www/us/en/docs/onemkl/developer-guide/current/conditional-numerical-reproducibility.html</p></li>
<li><p><strong>[NvidiaCUB]</strong> — NVIDIA Corporation. “CUB: CUDA
UnBound.” <em>NVIDIA CUB Documentation</em>. Describes deterministic
reduction variants with fixed-order tree reduction. Available at:
https://nvlabs.github.io/cub/</p></li>
<li><p><strong>[KokkosReduce]</strong> — Sandia National Laboratories.
“Custom Reductions: Determinism.” <em>Kokkos Documentation</em>.
Available at:
https://kokkos.github.io/kokkos-core-wiki/API/core/parallel-dispatch/parallel_reduce.html</p></li>
<li><p><strong>[IntelTBB]</strong> — Intel Corporation. “Specifying a
Partitioner.” <em>Intel oneAPI Threading Building Blocks Developer
Guide</em>. Available at:
https://www.intel.com/content/www/us/en/docs/onetbb/developer-guide-reference/current/partitioner.html</p></li>
</ul>
<h3 id="academic-references">Academic References</h3>
<ul>
<li><p><strong>[Blelloch1989]</strong> — Guy E. Blelloch. “Scans as
Primitive Parallel Operations.” <em>IEEE Transactions on Computers</em>,
38(11):1526–1538, November 1989. DOI: https://doi.org/10.1109/12.42122.
Identifies reduction and scan as the fundamental combining primitives
for parallel algorithms.</p></li>
<li><p><strong>[Dalton2014]</strong> — B. Dalton, E. Wang, and R.
Blainey. “SIMDizing pairwise sums: a summation algorithm balancing
accuracy with throughput.” <em>Proceedings of the Workshop on
Programming Models for SIMD/Vector Processing (WPMVP)</em>, 2014. DOI:
https://doi.org/10.1145/2568058.2568070</p></li>
<li><p><strong>[Demmel2013]</strong> — James Demmel and Hong Diep
Nguyen. “Fast Reproducible Floating-Point Summation.” <em>Proceedings of
the 21st IEEE Symposium on Computer Arithmetic (ARITH)</em>, April 2013,
pp. 163–172. DOI: https://doi.org/10.1109/ARITH.2013.9</p></li>
<li><p><strong>[Higham2002]</strong> — Nicholas J. Higham. <em>Accuracy
and Stability of Numerical Algorithms</em>, Second Edition. Society for
Industrial and Applied Mathematics (SIAM), 2002. ISBN:
978-0-89871-521-7. Chapter 4 discusses pairwise summation and error
analysis of floating-point summation algorithms.</p></li>
</ul>
<h2 id="appendix-a-illustrative-wording-informative">Appendix A:
Illustrative Wording (Informative)</h2>
<p>This appendix is illustrative; no API is proposed in this paper (§2).
It shows one way the semantic definition could be expressed for a future
Standard Library facility. Names, headers, and final API shape are
intentionally provisional.</p>
<h3 id="a.1-example-algorithm-specification">A.1 Example Algorithm
Specification</h3>
<p>The following shows how the semantic definition could be expressed
for a hypothetical algorithm that exposes the lane count
<code>L</code>:</p>
<ul>
<li><strong>L (lane count):</strong> the sole semantic topology
coordinate. The canonical expression is fully determined by
<code>(N, L)</code>.</li>
</ul>
<div class="sourceCode" id="cb31"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Lane-based topology (portable across ABIs for a fixed L)</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span><span class="op">&lt;</span><span class="dt">size_t</span> L<span class="op">,</span> <span class="kw">class</span> InputIterator<span class="op">,</span> <span class="kw">class</span> T<span class="op">,</span> <span class="kw">class</span> BinaryOperation<span class="op">&gt;</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="kw">constexpr</span> T canonical_reduce<span class="op">(</span>InputIterator first<span class="op">,</span> InputIterator last<span class="op">,</span> T init<span class="op">,</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>                  BinaryOperation binary_op<span class="op">);</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span><span class="op">&lt;</span><span class="dt">size_t</span> L<span class="op">,</span> <span class="kw">class</span> ExecutionPolicy<span class="op">,</span> <span class="kw">class</span> ForwardIterator<span class="op">,</span> <span class="kw">class</span> T<span class="op">,</span> <span class="kw">class</span> BinaryOperation<span class="op">&gt;</span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>T canonical_reduce<span class="op">(</span>ExecutionPolicy<span class="op">&amp;&amp;</span> policy<span class="op">,</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>            ForwardIterator first<span class="op">,</span> ForwardIterator last<span class="op">,</span> T init<span class="op">,</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>            BinaryOperation binary_op<span class="op">);</span></span></code></pre></div>
<p>[<em>Note:</em> The illustrative wording marks the non-policy
overloads <code>constexpr</code>. The canonical expression is a pure
functional specification with no dependence on runtime state — it is
structurally amenable to constant evaluation. The tree evaluation
consists of a finite sequence of <code>binary_op</code> applications in
a statically determined order, which is compatible with constexpr
evaluation provided <code>binary_op</code> is itself constexpr. Full
constexpr feasibility depends on whether the API ultimately uses
iterators (which have constexpr limitations for some container types) or
ranges. Detailed constexpr specification is deferred to the API design
paper. <em>—end note</em>]</p>
<p><strong>Constraints:</strong> - For overloads with an
ExecutionPolicy, ForwardIterator meets the Cpp17ForwardIterator
requirements.</p>
<p><strong>Mandates:</strong> - <code>L &gt;= 1</code>.</p>
<h3 id="a.2-semantics-as-if">A.2 Semantics (as-if)</h3>
<p>A conforming implementation returns a value that is as-if evaluation
of the canonical abstract expression defined in §4, which may be
constructed conceptually as follows:</p>
<ol type="1">
<li><strong>Materialize terms.</strong> Form the conceptual sequence of
terms <code>X[0..N)</code> by converting each input element to the
reduction state type <code>A</code> as specified in §4.6.</li>
<li><strong>Partition into lanes.</strong> Partition terms into
<code>L</code> logical lanes by index modulo as specified in
§4.3.1.</li>
<li><strong>Form fixed-length lane leaves with absence.</strong> Let
<code>K = ceil(N / L)</code> (for <code>N &gt; 0</code>). For each lane
index <code>j</code> in <code>[0, L)</code>, form a fixed-length leaf
sequence <code>Y_j[0..K)</code> of <code>maybe&lt;A&gt;</code> where
<code>Y_j[t]</code> is <code>present(X[j + t*L])</code> when
<code>j + t*L &lt; N</code>, and <code>∅</code> otherwise (§4.3.2).</li>
</ol>
<p><em>Informative:</em> When <code>N &lt; L</code>, some lane indices
have no corresponding input positions; such lanes are best understood as
“no lane data exists”. In the canonical expression this is represented
by <code>Y_j[t] == ∅</code> for all <code>t</code>, yielding
<code>R_j == ∅</code>.</p>
<ol start="4" type="1">
<li><strong>Per-lane canonical reduction.</strong> For each lane index
<code>j</code>, compute
<code>R_j = CANONICAL_TREE_EVAL(binary_op, Y_j)</code> using the
balanced tree shape and the lifted <code>COMBINE</code> rules for
<code>∅</code> (§4.2–§4.4.1).</li>
<li><strong>Cross-lane canonical reduction.</strong> Form
<code>Z[0..L)</code> where <code>Z[j] = R_j</code> and compute
<code>R_all = CANONICAL_TREE_EVAL(binary_op, Z)</code> (§4.4.2).</li>
<li><strong>Integrate <code>init</code> (if provided).</strong> If an
initial value is provided, the result is <code>init</code> when
<code>N == 0</code>, otherwise
<code>binary_op(init, value(R_all))</code>, as specified in §4.5.</li>
</ol>
<p>[<em>Note:</em> <code>init</code> is combined once with the total
result and is not treated as an additional input element; treating it as
an extra element would shift lane assignments and change the selected
canonical expression topology. <em>—end note</em>]</p>
<p><em>Informative:</em> An implementation may skip forming absent
leaves and lanes that contain no elements, provided the returned value
is as-if evaluation of the canonical expression (since <code>∅</code>
does not induce applications of <code>binary_op</code>).</p>
<h2 id="appendix-b-implementation-guidance-informative">Appendix B:
Implementation Guidance (Informative)</h2>
<p>This appendix offers detailed guidance for implementers. It is
informative only; the normative specification is the canonical
expression defined in §4.</p>
<p><strong>General principle:</strong> Implementations need not
instantiate lanes for empty subsequences; only the logical reduction
order must be preserved. For example, if L = 1000 but N = 5, the
implementation need not allocate 1000 accumulators — only the 5
non-empty subsequences take part in the final reduction.</p>
<h3 id="b.1-two-degrees-of-parallelism">B.1 Two Degrees of
Parallelism</h3>
<p>Real implementations exploit two orthogonal forms of parallelism:</p>
<table>
<thead>
<tr class="header">
<th>Parallelism</th>
<th>Mechanism</th>
<th>Typical Scale</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>SIMD</td>
<td>Vector registers, GPU warps</td>
<td>4–64 lanes</td>
</tr>
<tr class="even">
<td>Threads</td>
<td>CPU cores, GPU blocks</td>
<td>4–thousands</td>
</tr>
</tbody>
</table>
<p>This leads to two levels of reduction:</p>
<pre><code>                 FINAL RESULT
                      |
          +-----------+-----------+
          |   THREAD REDUCTION    | &lt;-- Combine thread results
          +-----------+-----------+
                      |
      +---------------+---------------+
      |               |               |
+-----+-----+   +-----+-----+   +-----+-----+
|   SIMD    |   |   SIMD    |   |   SIMD    |
| REDUCTION |   | REDUCTION |   | REDUCTION |
+-----------+   +-----------+   +-----------+</code></pre>
<p>Both levels must produce results matching the canonical
expression.</p>
<h3 id="b.2-the-efficient-simd-pattern">B.2 The Efficient SIMD
Pattern</h3>
<p>The canonical expression supports efficient vertical addition:</p>
<p>For L = 4, N = 16:</p>
<pre><code>Load V0 = {E[0], E[1], E[2], E[3]} → Acc = V0
Load V1 = {E[4], E[5], E[6], E[7]} → Acc = Acc + V1
Load V2 = {E[8], E[9], E[10], E[11]} → Acc = Acc + V2
Load V3 = {E[12], E[13], E[14], E[15]} → Acc = Acc + V3

Acc now holds {R_0, R_1, R_2, R_3}
Final horizontal reduction: op(op(R_0, R_1), op(R_2, R_3))</code></pre>
<p>This is contiguous loads plus vertical adds — optimal for SIMD.</p>
<h3 id="b.3-thread-level-partitioning">B.3 Thread-Level
Partitioning</h3>
<p>With multiple threads, each processes a contiguous chunk:</p>
<pre><code>Thread 0: E[0..256) → {R_0^T0, R_1^T0, R_2^T0, R_3^T0}
Thread 1: E[256..512) → {R_0^T1, R_1^T1, R_2^T1, R_3^T1}
...

Thread partial results are combined per lane, then across lanes:
Global R_0 = CANONICAL_TREE_EVAL(op, {R_0^T0, R_0^T1, ...})
Global R_1 = CANONICAL_TREE_EVAL(op, {R_1^T0, R_1^T1, ...})
...
Final = CANONICAL_TREE_EVAL(op, {R_0, R_1, R_2, R_3})</code></pre>
<h3 id="b.4-gpu-implementation">B.4 GPU Implementation</h3>
<p>The sequence maps to GPU architectures:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Vertical addition within block</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">size_t</span> base <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> base <span class="op">&lt;</span> n<span class="op">;</span> base <span class="op">+=</span> L<span class="op">)</span> <span class="op">{</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">size_t</span> idx <span class="op">=</span> base <span class="op">+</span> lane<span class="op">;</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span>idx <span class="op">&lt;</span> n<span class="op">)</span> acc <span class="op">=</span> op<span class="op">(</span>acc<span class="op">,</span> input<span class="op">[</span>idx<span class="op">]);</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="co">// Horizontal reduction (assumes L is a power of two)</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">size_t</span> stride <span class="op">=</span> L<span class="op">/</span><span class="dv">2</span><span class="op">;</span> stride <span class="op">&gt;</span> <span class="dv">0</span><span class="op">;</span> stride <span class="op">/=</span> <span class="dv">2</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span>lane <span class="op">&lt;</span> stride<span class="op">)</span> shared<span class="op">[</span>lane<span class="op">]</span> <span class="op">=</span> op<span class="op">(</span>shared<span class="op">[</span>lane<span class="op">],</span> shared<span class="op">[</span>lane <span class="op">+</span> stride<span class="op">]);</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>  __syncthreads<span class="op">();</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a><span class="co">// Cross-block: fixed slots, not atomics</span></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>block_results<span class="op">[</span>blockIdx<span class="op">.</span>x<span class="op">]</span> <span class="op">=</span> shared<span class="op">[</span><span class="dv">0</span><span class="op">];</span> <span class="co">// Deterministic position</span></span></code></pre></div>
<p><strong>Key constraints:</strong> - Static work assignment (no work
stealing) - Fixed block result slots (no atomicAdd) - Canonical tree for
horizontal reduction</p>
<p>[<em>Note:</em> This GPU sketch is illustrative and non-normative; it
demonstrates one possible mapping of the canonical expression to a GPU
kernel and is not intended to constrain implementation strategy.
<em>—end note</em>]</p>
<h3 id="b.5-cross-platform-consistency">B.5 Cross-Platform
Consistency</h3>
<p>The proposal defines the expression structure. All conforming
implementations evaluate as-if by the same canonical abstract expression
for a given topology coordinate <code>L</code>.</p>
<p><strong>Within a single, controlled environment</strong> (same
ISA/backend, same compiler and flags, and equivalent floating-point
evaluation models), this removes the run-to-run variability of
<code>std::reduce</code> by fixing the reduction tree.</p>
<p><strong>Across architectures/backends</strong> (CPU ↔︎ CPU, CPU ↔︎
GPU): the facility guarantees expression/topology equivalence — i.e.,
the same abstract tree is specified. Achieving bitwise identity
additionally requires aligning the floating-point evaluation environment
(§2.5, §6).</p>
<h4 id="b.5.1-requirements-for-cross-platform-bitwise-reproducibility">B.5.1
Requirements for Cross-Platform Bitwise Reproducibility</h4>
<p>This proposal standardizes the abstract expression structure
(parenthesization and operand order). Bitwise-identical results across
different platforms, compilers, or architectures generally require that
fixed expression structure and an equivalent floating-point evaluation
environment.</p>
<p>[<em>Note:</em> For fundamental floating-point types with
non-associative operations (e.g., float, double with
<code>std::plus</code>), factors that commonly affect bitwise results
include (non-exhaustive):</p>
<ol type="1">
<li><strong>Floating-point format and evaluation rules:</strong> whether
the type and operations follow ISO/IEC/IEEE 60559 (IEEE 754) and whether
intermediate precision differs (e.g., extended precision).</li>
<li><strong>Rounding mode:</strong> both executions use the same
rounding mode (typically round-to-nearest ties-to-even).</li>
<li><strong>Contraction / FMA:</strong> whether multiply-add sequences
are fused/contracted, and whether contraction behavior matches across
backends.</li>
<li><strong>Subnormal handling:</strong> whether subnormals are
preserved or flushed to zero (FTZ/DAZ).</li>
<li><strong>Transforming optimizations:</strong> “fast-math” style
options that permit reassociation or relax IEEE behavior.</li>
<li><strong>Library math and user operations:</strong> if
<code>binary_op</code> (or upstream transformations such as
<code>views::transform</code>) call implementation-defined math
libraries, results may differ.</li>
<li><strong>Topology selection and input order:</strong> both sides use
identical topology coordinate (<code>L</code>) and the same input
order.</li>
</ol>
<p><em>—end note</em>]</p>
<p><strong>What this proposal guarantees:</strong> for fixed topology
coordinate, input order, and <code>binary_op</code>, the abstract
expression (parenthesization and operand order) is identical across all
conforming implementations.</p>
<p><strong>What remains platform-specific:</strong> the floating-point
evaluation model (items above). Aligning it may require toolchain
controls (for example, disabling contraction and avoiding fast-math;
exact spellings are toolchain-specific).</p>
<p><strong>Verification workflow:</strong> the demonstrators in Appendix
K use a fixed seed and publish expected hex outputs for representative
topology coordinates. Matching those values across platforms validates
both (a) correct expression structure (this proposal) and (b)
sufficiently aligned floating-point environments (user
responsibility).</p>
<p><strong>Illustrative CPU ↔︎ GPU check:</strong></p>
<div class="sourceCode" id="cb36"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Compile and run both sides under equivalent FP settings (toolchain-specific).</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="dt">double</span> cpu <span class="op">=</span> canonical_reduce<span class="op">&lt;</span><span class="dv">16</span><span class="op">&gt;(</span>data<span class="op">.</span>begin<span class="op">(),</span> data<span class="op">.</span>end<span class="op">(),</span> <span class="fl">0.0</span><span class="op">,</span> <span class="bu">std::</span>plus<span class="op">&lt;&gt;{});</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="dt">double</span> gpu <span class="op">=</span> cuda_canonical_reduce<span class="op">&lt;</span><span class="dv">16</span><span class="op">&gt;(</span>d_data<span class="op">,</span> N<span class="op">,</span> <span class="fl">0.0</span><span class="op">);</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="co">// Bitwise equality is achievable when the FP evaluation environments are aligned:</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="ot">assert</span><span class="op">(</span><span class="bu">std::</span>bit_cast<span class="op">&lt;</span><span class="dt">uint64_t</span><span class="op">&gt;(</span>cpu<span class="op">)</span> <span class="op">==</span> <span class="bu">std::</span>bit_cast<span class="op">&lt;</span><span class="dt">uint64_t</span><span class="op">&gt;(</span>gpu<span class="op">));</span></span></code></pre></div>
<p>This enables “golden result” workflows where a reference evaluation
can act as a baseline for accelerator correctness verification.</p>
<h3 id="b.6-when-physical-width-logical-width">B.6 When Physical Width ≠
Logical Width</h3>
<p><strong>Physical &gt; Logical</strong> (e.g., 256 GPU threads, L =
8): Multiple threads cooperate on each logical lane. Partial results
combine using the canonical tree.</p>
<p><strong>Physical &lt; Logical</strong> (e.g., 4 physical SIMD lanes,
L = 16 logical lanes): Process logical lanes in chunks across multiple
physical iterations. The logical sequence is unchanged; only the
physical execution differs.</p>
<h3 id="b.7-performance-characteristics">B.7 Performance
Characteristics</h3>
<p>Representative measurements appear in Appendix N.12. With proper SIMD
optimization (8-block unrolling), the reference implementation indicates
that the canonical expression structure can be evaluated at throughput
comparable to unconstrained reduction, suggesting that conforming
implementations need not incur prohibitive overhead. Observed overhead
is workload- and configuration-dependent; see also the demonstrators in
Appendix K for platform observations.</p>
<h2 id="appendix-c-prior-art-and-industry-practice-informative">Appendix
C: Prior Art and Industry Practice (Informative)</h2>
<p>The tension between parallel performance and numerical
reproducibility is a well-documented challenge in high-performance
computing (HPC) and distributed systems. This appendix summarizes how
existing frameworks address the “Grouping Gap.”</p>
<h3 id="c.1-kokkos-ecosystem-hpc-portability">C.1 Kokkos Ecosystem (HPC
Portability)</h3>
<p>Kokkos is a widely used C++ programming model for exascale computing.
Its documentation explicitly warns users about the lack of run-to-run
stability (for FP) in reductions:</p>
<blockquote>
<p>“The result of a parallel_reduce is not guaranteed to be bitwise
identical across different runs on the same hardware, nor across
different numbers of threads, unless the joiner operation is associative
and commutative.” [KokkosReduce]</p>
</blockquote>
<p>In practice, HPC users requiring reproducibility in Kokkos must often
implement “two-pass” reductions or use fixed-point arithmetic, which
imposes a significant developer burden. The interleaved topology
proposed in §4.3 of this paper mirrors the “vector-lane” optimization
patterns used in Kokkos’s backend, but elevates it to a deterministic
semantic contract.</p>
<h3 id="c.2-intel-oneapi-threading-building-blocks-onetbb">C.2 Intel
oneAPI Threading Building Blocks (oneTBB)</h3>
<p>Intel TBB is the industry standard for task-parallelism on CPUs. Its
<code>parallel_reduce</code> implementation uses a recursive splitting
strategy that relies on a “Join” pattern:</p>
<ol type="1">
<li><strong>Splitting:</strong> The range is split into sub-ranges until
they reach a “grain size.”</li>
<li><strong>Reduction:</strong> Each sub-range is reduced locally.</li>
<li><strong>Joining:</strong> Sub-results are combined using the
user-provided <code>join()</code> method.</li>
</ol>
<p>Because TBB uses a work-stealing scheduler, the order in which these
<code>join()</code> operations occur is non-deterministic — it depends
on which thread becomes free first. While TBB offers a
<code>static_partitioner</code> to minimize this, it does not guarantee
a canonical tree topology, making bitwise identity fragile across
different thread counts [IntelTBB].</p>
<h3 id="c.3-nvidia-thrust-and-cub">C.3 NVIDIA Thrust and CUB</h3>
<p>For GPU architectures, NVIDIA provides the Thrust and CUB
libraries.</p>
<p><strong>Thrust:</strong> Generally favors throughput and uses atomic
operations in many reduction paths. These atomics are processed in an
order determined by hardware thread scheduling, leading to
non-deterministic floating-point results.</p>
<p><strong>CUB:</strong> Provides more granular control through
“Block-level” and “Warp-level” primitives. The interleaved subsequences
(S_j) defined in §4.3 are mathematically equivalent to the
“blocked-arrangement” and “striped-arrangement” memory patterns used in
CUB to achieve peak bandwidth on SIMD-intensive hardware
[NvidiaCUB].</p>
<h3 id="c.4-academic-and-research-solutions">C.4 Academic and Research
Solutions</h3>
<p>Significant research has been conducted into “Reproducible Summation”
(e.g., [Demmel2013]). These solutions typically fall into two
categories:</p>
<ol type="1">
<li><strong>Exact Summation:</strong> Using very wide fixed-precision
accumulators (e.g., 400+ bits) to ensure associativity. These are
bit-accurate but carry a 2x–10x performance penalty.</li>
<li><strong>Fixed-Topology Summation:</strong> Enforcing a specific
reduction tree.</li>
</ol>
<p>This proposal follows the Fixed-Topology school of thought. It
recognizes that while we cannot easily standardize “Exact Summation” due
to its cost, we can standardize the topology, which provides
reproducibility for a given platform at a much lower performance cost
(10–15% overhead relative to <code>std::reduce</code> in representative
configurations; see Appendix H).</p>
<p>See References for full citations of [KokkosReduce], [IntelTBB],
[NvidiaCUB], and [Demmel2013].</p>
<h2 id="appendix-d-standard-wording-for-sequential-evaluation-order-informative">Appendix
D: Standard Wording for Sequential Evaluation Order (Informative)</h2>
<p>In the C++ Standard, sequential algorithms like
<code>std::accumulate</code> and <code>std::ranges::fold_left</code>
have a mandated evaluation order — the grouping of operations is fully
specified as a left-fold.</p>
<p><strong>Important distinction:</strong> The Standard specifies
evaluation order, not bitwise reproducibility. Even with identical
evaluation order, results may differ across compilers or platforms due
to floating-point evaluation model factors (see §D.5). However, a fixed
evaluation order is a necessary precondition for reproducibility —
without it, reproducibility is impossible even under identical
floating-point evaluation models.</p>
<h3 id="d.1-stdaccumulate">D.1 std::accumulate</h3>
<p>The evaluation order guarantee for <code>std::accumulate</code> is
found in the Numerics library section of the Standard.</p>
<p><strong>Section:</strong> [numeric.ops.accumulate]/2</p>
<p><strong>The Text:</strong> The Standard defines the behavior of
<code>accumulate(first, last, init, binary_op)</code> as:</p>
<blockquote>
<p>“Computes its result by initializing the accumulator acc with the
initial value init and then modifies it with
<code>acc = std::move(acc) + *i</code> or
<code>acc = binary_op(std::move(acc), *i)</code> for every iterator i in
the range [first, last) in order.”</p>
</blockquote>
<p><strong>What this guarantees:</strong> The expression structure is
fixed as <code>(((init + a) + b) + c)...</code>. The grouping is fully
specified — there is no implementation freedom in how operations are
combined.</p>
<p><strong>What this does not guarantee:</strong> Bitwise identical
results across compilers or platforms. The same grouping may produce
different bits due to floating-point evaluation model differences.</p>
<p><strong>Contrast with std::reduce:</strong> <code>std::reduce</code>
uses a generalized sum ([numerics.defns]/1–2, [numeric.ops.reduce]/7),
allowing the implementation to group elements as
<code>((a + b) + (c + d))</code> or any other valid tree. This makes
even the grouping non-deterministic.</p>
<h3 id="d.2-stdrangesfold_left-c23">D.2 std::ranges::fold_left
(C++23)</h3>
<p>For the newer Ranges-based algorithms, the specification is even more
explicit about its algebraic structure.</p>
<p><strong>Section:</strong> [alg.fold]</p>
<p><strong>The Text:</strong></p>
<blockquote>
<p>“The range fold algorithms are sequential operations that perform a
left-fold… <code>ranges::fold_left(R, init, f)</code> is equivalent to:
<code>cpp auto acc = init; for (auto&amp;&amp; e : R) acc = f(std::move(acc), e); return acc;</code>”</p>
</blockquote>
<p><strong>What this guarantees:</strong> By defining the algorithm via
an explicit for loop, the Standard fully specifies the evaluation order.
The state of the accumulator at step N depends exactly and only on the
state at step N-1 and the Nth element.</p>
<h3 id="d.3-sequential-vs.-parallel-contrasting-guarantees">D.3
Sequential vs. Parallel: Contrasting Guarantees</h3>
<table>
<colgroup>
<col style="width: 15%" />
<col style="width: 54%" />
<col style="width: 29%" />
</colgroup>
<thead>
<tr class="header">
<th>Property</th>
<th>Sequential (accumulate/fold_left)</th>
<th>Parallel (reduce)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Standard Section</td>
<td>[numeric.ops.accumulate] / [alg.fold]</td>
<td>[numeric.ops.reduce]</td>
</tr>
<tr class="even">
<td>Grouping</td>
<td>Mandated: Left-to-right</td>
<td>Generalized Sum (Unspecified)</td>
</tr>
<tr class="odd">
<td>Complexity</td>
<td>O(N) operations</td>
<td>O(N) operations</td>
</tr>
<tr class="even">
<td>Evaluation Order</td>
<td>Fully specified</td>
<td>Not specified</td>
</tr>
</tbody>
</table>
<p><strong>Key insight:</strong> <code>std::accumulate</code> and
<code>std::reduce</code> differ in whether the grouping is specified,
not in whether they guarantee “bitwise reproducibility” (neither does,
strictly speaking).</p>
<h3 id="d.4-init-placement-comparison-with-stdaccumulate">D.4 Init
Placement: Comparison with std::accumulate</h3>
<p>This proposal specifies init placement as <code>op(I, R)</code>
(where <code>I</code> is the initial value materialized as type
<code>A</code> per §4.6) — the initial value is combined once at the end
with the tree result. The table below compares this to
<code>std::accumulate</code>:</p>
<table>
<thead>
<tr class="header">
<th>Aspect</th>
<th>std::accumulate</th>
<th>This Proposal (<code>op(I, R)</code>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Init handling</td>
<td>Folded at every step</td>
<td>Combined once at end</td>
</tr>
<tr class="even">
<td>Structure</td>
<td><code>(((init ⊕ a) ⊕ b) ⊕ c)</code></td>
<td><code>init ⊕ tree_reduce(a,b,c,d)</code></td>
</tr>
<tr class="odd">
<td>Init participates in</td>
<td>N operations</td>
<td>1 operation</td>
</tr>
</tbody>
</table>
<p><strong>Implication for non-associative operations:</strong> For
associative operations, these approaches produce equivalent results. For
non-associative operations (e.g., floating-point addition), the results
may differ. Users migrating from <code>std::accumulate</code> should be
aware of this distinction.</p>
<p>See Appendix E for rationale behind the <code>op(I, R)</code> design
choice.</p>
<h3 id="d.5-important-caveat-evaluation-order-bitwise-identity">D.5
Important Caveat: Evaluation Order ≠ Bitwise Identity</h3>
<p>The Standard specifies evaluation order, not bitwise results. Even
when the grouping of operations is fully specified (as in
<code>std::accumulate</code>), bitwise identity across different
compilers, platforms, or even different runs is not guaranteed due
to:</p>
<ol type="1">
<li><strong>Intermediate precision:</strong> The Standard permits
implementations to use higher precision for intermediate results (e.g.,
x87 80-bit registers vs SSE 64-bit). See [cfenv.syn] and
implementation-defined floating-point behavior.</li>
<li><strong>FMA Contraction:</strong> A compiler might contract
<code>a * b + c</code> into a single fused multiply-add instruction on
one platform but not another, changing the result bits. This is
controlled by <code>#pragma STDC FP_CONTRACT</code> and compiler flags
like <code>-ffp-contract</code>.</li>
<li><strong>Rounding mode:</strong> Different default rounding modes
across implementations can affect results.</li>
</ol>
<p><strong>What this proposal provides:</strong> A fully specified
expression structure (grouping and operand order). Combined with user
control of the floating-point evaluation model, this enables
reproducibility.</p>
<p><strong>What this proposal does not provide:</strong> Automatic
cross-platform bitwise identity. Users must also control their
floating-point evaluation model (see §6).</p>
<h3 id="d.6-why-compilers-cannot-reassociate-mandated-evaluation-order">D.6
Why Compilers Cannot Reassociate Mandated Evaluation Order</h3>
<p>A potential concern is whether compilers might reassociate the
reduction operations defined by this proposal, defeating the run-to-run
stability guarantee. This section explains why such reassociation would
be non-conforming.</p>
<h4 id="d.6.1-the-as-if-rule">D.6.1 The As-If Rule</h4>
<p>The C++ Standard permits compilers to perform any transformation that
does not change the observable behavior of a conforming program
([intro.abstract]/1). This is commonly called the “as-if rule.”</p>
<p>For floating-point arithmetic, reassociation (changing
<code>(a + b) + c</code> to <code>a + (b + c)</code>) generally changes
the result due to rounding. Therefore, a compiler cannot reassociate
floating-point operations under the as-if rule unless:</p>
<ol type="1">
<li>It can prove the result is unchanged (generally impossible for FP),
or</li>
<li>The user has explicitly opted into non-standard semantics via
compiler flags</li>
</ol>
<h4 id="d.6.2-existing-precedent-stdaccumulate">D.6.2 Existing
Precedent: std::accumulate</h4>
<p><code>std::accumulate</code> mandates a specific evaluation order
([numeric.ops.accumulate]/2):</p>
<blockquote>
<p>“Computes its result by initializing the accumulator acc with the
initial value init and then modifies it with
<code>acc = std::move(acc) + *i</code> or
<code>acc = binary_op(std::move(acc), *i)</code> for every iterator i in
the range [first, last) in order.”</p>
</blockquote>
<p>This wording constrains implementations to a left-fold:
<code>(((init ⊕ a) ⊕ b) ⊕ c) ...</code></p>
<p>Compilers respect this constraint. A compiler that reassociated
<code>std::accumulate</code> into a tree reduction would be
non-conforming, because the observable result would differ for
non-associative operations (including floating-point addition).</p>
<h4 id="d.6.3-this-proposal-follows-the-same-model">D.6.3 This Proposal
Follows the Same Model</h4>
<p>This proposal mandates a specific expression structure with the same
normative force. The “Generalized Sum” semantics of
<code>std::reduce</code> explicitly grant permission to reassociate;
this proposal removes that permission — exactly as
<code>std::accumulate</code> mandates a specific left-fold
expression.</p>
<p>[<em>Note:</em> For <code>std::accumulate</code>, “expression
structure” and “evaluation order” coincide because the left-fold is
inherently sequential. For this proposal, the expression structure
(parenthesization) is fixed, but independent subexpressions may be
evaluated concurrently. <em>—end note</em>]</p>
<p>The only difference from <code>std::accumulate</code> is the shape of
the mandated expression:</p>
<table>
<thead>
<tr class="header">
<th>Algorithm</th>
<th>Mandated Expression Shape</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>std::accumulate</td>
<td><code>(((init ⊕ a) ⊕ b) ⊕ c)</code> Linear fold</td>
</tr>
<tr class="even">
<td>This proposal</td>
<td><code>init ⊕ ((a ⊕ b) ⊕ (c ⊕ d))</code> Balanced binary tree</td>
</tr>
</tbody>
</table>
<p>Both specifications constrain the parenthesization. Both are subject
to the same as-if rule. A compiler that reassociates one would equally
violate conformance by reassociating the other.</p>
<h4 id="d.6.4-what-about--ffast-math">D.6.4 What About -ffast-math?</h4>
<p>Compiler flags like <code>-ffast-math</code>,
<code>-fassociative-math</code>, or <code>/fp:fast</code> explicitly opt
out of IEEE 754 compliance. Under these flags:</p>
<ul>
<li>The compiler may reassociate floating-point operations</li>
<li><code>std::accumulate</code> may not produce left-fold results</li>
<li><code>std::reduce</code> may not match any particular grouping</li>
<li>This proposal’s guarantee would also not apply</li>
</ul>
<p>This is consistent with existing behavior: these flags override IEEE
754 compliance for all floating-point algorithms, including
<code>std::accumulate</code>. Users who enable <code>-ffast-math</code>
have explicitly traded determinism for performance.</p>
<p><strong>Recommendation for users requiring run-to-run
stability:</strong> Compile with <code>-ffp-contract=off</code> (or
equivalent) and avoid <code>-ffast-math</code>. This applies equally to
<code>std::accumulate</code> and to this proposal.</p>
<h4 id="d.6.5-summary">D.6.5 Summary</h4>
<table>
<colgroup>
<col style="width: 45%" />
<col style="width: 55%" />
</colgroup>
<thead>
<tr class="header">
<th>Concern</th>
<th>Resolution</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>“Compilers might reassociate the tree”</td>
<td>Violates as-if rule, same as for std::accumulate</td>
</tr>
<tr class="even">
<td>“What about -ffast-math?”</td>
<td>User has opted out of IEEE 754; all FP guarantees void</td>
</tr>
<tr class="odd">
<td>“Is this proposal different from existing algorithms?”</td>
<td>No — same conformance model as std::accumulate</td>
</tr>
</tbody>
</table>
<p>The expression-structure guarantee in this proposal has the same
normative force (with respect to parenthesization and operand order) as
the mandated left-fold structure in <code>std::accumulate</code>.
Compilers that respect the latter will respect the former.</p>
<h2 id="appendix-e-init-placement-rationale-informative">Appendix E:
Init Placement Rationale (Informative)</h2>
<p>This appendix provides rationale for the init placement design
choice. This proposal specifies <strong>Option A: op(I, R)</strong> —
the initial value (materialized as a value <code>I</code> of type
<code>A</code> per §4.6) is applied as the left operand to the result of
the canonical tree reduction. This appendix explains why this option was
chosen over alternatives.</p>
<p>[<em>Note:</em> <code>init</code> is combined once with the total
result and is not treated as an additional input element; treating it as
an extra element would shift lane assignments and change the selected
canonical expression topology. <em>—end note</em>]</p>
<h3 id="e.1-design-options-considered">E.1 Design Options
Considered</h3>
<p><strong>Option A: Post-reduction (<code>op(I, R)</code>) —
CHOSEN</strong></p>
<pre><code>canonical_reduce&lt;L&gt;(E[0..N), init, op):
  if N == 0:
    return init
  let R = interleaved_reduce&lt;L&gt;(E[0..N))
  let I = A(init)  // materialize init as the reduction state type
  return op(I, R)</code></pre>
<p><strong>Pros:</strong> - init is not part of the canonical reduction
tree — clear separation of concerns - Simplifies parallel implementation
(tree can complete before init is available) - Matches
<code>std::reduce</code>’s treatment of init ([numeric.ops.reduce]) -
Empty range handling is trivial: return init</p>
<p><strong>Cons:</strong> - Differs from <code>std::accumulate</code>’s
left-fold semantics - For non-associative operations, results differ
from left-fold expectations</p>
<p><strong>Option B: Post-reduction (op(R, I))</strong></p>
<p>Same as Option A but with init as the right operand.</p>
<p><strong>Pros:</strong> - Same implementation simplicity as Option
A</p>
<p><strong>Cons:</strong> - Less intuitive for users expecting init to
be “first” - Still differs from <code>std::accumulate</code></p>
<p><strong>Option C: Treat init as element 0 (prepend to
sequence)</strong></p>
<pre><code>canonical_reduce&lt;L&gt;(E[0..N), init, op):
  let E&#39; = {init, E[0], E[1], ..., E[N-1]}
  return interleaved_reduce&lt;L&gt;(E&#39;[0..N+1))</code></pre>
<p><strong>Pros:</strong> - init participates in the canonical tree with
a known position - More predictable for non-associative operations</p>
<p><strong>Cons:</strong> - Shifts all element indices by 1 - init
assigned to lane 0, changing topology when L &gt; 1 - Complicates the
interleaving definition</p>
<p><strong>Option D: Leave implementation-defined</strong></p>
<p>The standard specifies that init participates in exactly one
<code>binary_op</code> application with the tree result but does not
specify the order.</p>
<p><strong>Pros:</strong> - Maximum implementation flexibility - Avoids
contentious design decision - For associative operations (the common
case), result is unaffected</p>
<p><strong>Cons:</strong> - Non-deterministic for non-associative
operations - Users cannot rely on specific init behavior</p>
<h3 id="e.2-analysis">E.2 Analysis</h3>
<p>For <strong>associative operations</strong> (the vast majority of use
cases), all options produce equivalent results. The choice only matters
for non-associative operations.</p>
<p>For non-associative operations, users already face the fact that the
tree reduction differs from left-fold. The init placement is one
additional degree of freedom that must be specified for full run-to-run
stability.</p>
<h3 id="e.3-why-option-a-was-chosen">E.3 Why Option A Was Chosen</h3>
<p>This proposal specifies Option A (<code>op(I, R)</code>) for the
following reasons:</p>
<ol type="1">
<li><strong>SIMD purity:</strong> Folding init into lanes would require
broadcast and can amplify init influence in non-associative
operations.</li>
<li><strong>Task independence:</strong> A “pure” tree over the range can
be computed before init is available in async/distributed contexts.</li>
<li><strong>Algebraic clarity:</strong> A balanced tree has no
distinguished “first” element; init is most naturally a post-reduction
adjustment.</li>
<li><strong>Precedent:</strong> This matches <code>std::reduce</code>’s
treatment of init ([numeric.ops.reduce]) (as part of the generalized
sum, not folded sequentially).</li>
<li><strong>Full determinism:</strong> Specifying the placement ensures
that the complete evaluation order — including init — is fully
determined, consistent with the paper’s run-to-run stability
guarantee.</li>
</ol>
<h3 id="e.4-why-not-treat-init-as-element-0">E.4 Why Not Treat init as
Element 0?</h3>
<p>Treating init as “element 0” (Option C) introduces complications:</p>
<ul>
<li>With L &gt; 1, init would be assigned to lane 0, but all other
element indices would shift</li>
<li>The meaning of init would depend on lane topology in ways that are
harder to explain and reason about</li>
<li>It conflates two distinct roles: “initial state” vs “operand in the
tree”</li>
</ul>
<p>The post-reduction design keeps these roles separate.</p>
<h3 id="e.5-migration-path-for-users-expecting-accumulate-style-semantics">E.5
Migration Path for Users Expecting accumulate-style Semantics</h3>
<p>Users who require init to participate as a leaf within the tree
(rather than post-reduction) can achieve this through composition:</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co">// To get init as element 0 in the tree:</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> extended <span class="op">=</span> concat_view<span class="op">(</span>single_view<span class="op">(</span>init<span class="op">),</span> data<span class="op">);</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> result <span class="op">=</span> canonical_reduce<span class="op">&lt;</span>L<span class="op">&gt;(</span>extended<span class="op">.</span>begin<span class="op">(),</span> extended<span class="op">.</span>end<span class="op">(),</span></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>                 identity_element<span class="op">,</span> op<span class="op">);</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a><span class="co">// Or manually:</span></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> tree_result <span class="op">=</span> canonical_reduce<span class="op">&lt;</span>L<span class="op">&gt;(</span>data<span class="op">.</span>begin<span class="op">(),</span> data<span class="op">.</span>end<span class="op">(),</span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>                    identity_element<span class="op">,</span> op<span class="op">);</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> result <span class="op">=</span> op<span class="op">(</span>init<span class="op">,</span> tree_result<span class="op">);</span> <span class="co">// explicit post-reduction (matches this proposal)</span></span></code></pre></div>
<p>This flexibility allows users to achieve alternative semantics when
needed while the standard provides a single, well-defined default.</p>
<h2 id="appendix-f-design-evolution-informative">Appendix F: Design
Evolution (Informative)</h2>
<p>This proposal underwent significant internal development before
committee submission. The design space was explored systematically, with
key decisions documented in §3 (Design Space) and the appendices. This
section summarizes the major evolution points for reviewers interested
in the design rationale.</p>
<h3 id="f.1-core-design-decisions">F.1 Core Design Decisions</h3>
<table>
<colgroup>
<col style="width: 16%" />
<col style="width: 38%" />
<col style="width: 27%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr class="header">
<th>Decision</th>
<th>Alternatives Considered</th>
<th>Chosen Approach</th>
<th>Rationale</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Topology</td>
<td>Left-fold, blocked, N-ary tree</td>
<td>Interleaved balanced binary</td>
<td>O(log N) depth, SIMD-friendly</td>
</tr>
<tr class="even">
<td>Parameterization</td>
<td>Fixed constant, byte span, implementation-defined</td>
<td>User-specified lane count L</td>
<td>Portable, ABI-independent (see §9.2)</td>
</tr>
<tr class="odd">
<td>Init placement</td>
<td>As leaf, implementation-defined</td>
<td>Post-reduction <code>op(I, R)</code></td>
<td>Algebraic clarity, matches std::reduce ([numeric.ops.reduce])</td>
</tr>
<tr class="even">
<td>Split rule</td>
<td>⌈k/2⌉, variable</td>
<td>⌊k/2⌋ (normative)</td>
<td>Unique grouping specification</td>
</tr>
</tbody>
</table>
<h3 id="f.2-key-design-iterations">F.2 Key Design Iterations</h3>
<p><strong>Why lane count (L):</strong> The lane count <code>L</code> is
the semantic topology coordinate. It directly determines the abstract
expression tree structure. Earlier designs also considered a byte-span
parameter <code>M</code>, but this was rejected due to the portability
trap described in §9.2: <code>sizeof(value_type)</code> varies across
platforms, so the same <code>M</code> value would silently produce
different expressions on different targets.</p>
<p><strong>Why interleaved, not blocked:</strong> Blocked decomposition
creates topology that varies with thread count and alignment.
Interleaved assignment (index mod L) produces stable topology regardless
of execution strategy. See §3.2.</p>
<p><strong>Why user-specified L:</strong> Fixed L ages poorly as
hardware evolves; implementation-defined L does not provide determinism.
User specification places control where it belongs. See §3.3.</p>
<p><strong>Init placement:</strong> Treating init as “element 0” would
shift all indices and change lane assignment. Post-reduction application
keeps the tree “pure” and matches <code>std::reduce</code> semantics
([numeric.ops.reduce]). See §4.5 and Appendix E.</p>
<h3 id="f.3-industry-context">F.3 Industry Context</h3>
<p>The core design (fixed topology, user-controlled width) draws on
approaches seen in production libraries: - Intel oneMKL CNR (Conditional
Numerical Reproducibility) - NVIDIA CUB deterministic overloads -
PyTorch/TensorFlow deterministic modes</p>
<p>These libraries address similar problems through different
mechanisms. Their existence suggests the design space is viable and
addresses real needs. See §3.6 for references.</p>
<h2 id="appendix-g-detailed-design-rationale-informative">Appendix G:
Detailed Design Rationale (Informative)</h2>
<p>This appendix has detailed rationale for design decisions that were
summarized in Section 4. It is provided for reviewers seeking deeper
understanding of the trade-offs.</p>
<h3 id="g.1-lane-count-and-hardware-alignment">G.1 Lane Count and
Hardware Alignment</h3>
<p>The lane count <code>L</code> is specified directly, making the
topology explicit and portable. Users who wish to align with hardware
SIMD width can compute <code>L</code> at the call site (e.g.,
<code>64 / sizeof(V)</code>). The following tables show how common
<code>L</code> values relate to hardware targets:</p>
<table>
<thead>
<tr class="header">
<th>L</th>
<th>double (8B)</th>
<th>float (4B)</th>
<th>int32_t (4B)</th>
<th>Register width</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2</td>
<td>16 bytes</td>
<td>8 bytes</td>
<td>8 bytes</td>
<td>SSE/NEON (double)</td>
</tr>
<tr class="even">
<td>4</td>
<td>32 bytes</td>
<td>16 bytes</td>
<td>16 bytes</td>
<td>AVX/SSE (float)</td>
</tr>
<tr class="odd">
<td>8</td>
<td>64 bytes</td>
<td>32 bytes</td>
<td>32 bytes</td>
<td>AVX-512 (double)</td>
</tr>
<tr class="even">
<td>16</td>
<td>128 bytes</td>
<td>64 bytes</td>
<td>64 bytes</td>
<td>AVX-512 (float)</td>
</tr>
</tbody>
</table>
<p>Implementations with narrower physical registers execute the
canonical expression through multiple iterations. The logical topology —
which operations combine with which — is unchanged.</p>
<h3 id="g.2-why-interleaved-topology-supports-efficient-simd">G.2 Why
Interleaved Topology Supports Efficient SIMD</h3>
<p>The interleaved topology supports the simplest and most efficient
SIMD implementation pattern:</p>
<pre><code>Memory: E[0] E[1] E[2] E[3] | E[4] E[5] E[6] E[7] | E[8] ...
    └─── Vector 0 ────┘  └─── Vector 1 ────┘

Iteration 1: Load V0 = {E[0], E[1], E[2], E[3]} → Acc = V0
Iteration 2: Load V1 = {E[4], E[5], E[6], E[7]} → Acc = Acc + V1
...
Final: Acc = {R_0, R_1, R_2, R_3}</code></pre>
<p>This pattern achieves: - One contiguous vector load per iteration
(optimal memory access) - One vector add per iteration (single
instruction) - Streaming sequential access (spatially local) - No
shuffles or gathers until final horizontal reduction</p>
<p>A blocked topology would require gather operations or sequential
per-lane processing, losing the SIMD benefit.</p>
<h3 id="g.3-information-density-and-spatial-locality">G.3 Information
Density and Spatial Locality</h3>
<p>When <code>L</code> is chosen to match the target SIMD width (e.g.,
<code>L = 8</code> for AVX-512 with <code>double</code>), the algorithm
maintains exactly <code>L</code> independent partial-accumulator lanes
per logical stride, which maps directly to register-width execution.</p>
<p>For any type, specifying <code>L = 1</code> degenerates to a
single-lane balanced binary tree with perfect spatial locality.</p>
<h3 id="g.4-cross-architecture-expression-parity">G.4 Cross-Architecture
Expression-Parity</h3>
<p>GPU architectures achieve peak efficiency when reduction trees align
with warp width (typically 32 threads). For double (8 bytes), a
warp-level reduction operates on 32 × 8 = 256 bytes.</p>
<p>By specifying L=32, users define a canonical expression that maps to
warp-level operations on GPU while CPU evaluates the same mathematical
expression by iterating over narrower registers.</p>
<p><strong>What expression-parity guarantees:</strong> All platforms
evaluate the same mathematical expression — same parenthesization, same
operand ordering, same reduction tree topology.</p>
<p><strong>What it does not guarantee:</strong> Bitwise reproducibility
requires equivalent floating-point semantics across architectures, which
is difficult due to differences in FTZ/DAZ modes, FMA contraction, and
rounding behavior.</p>
<h3 id="g.5-the-golden-reference-l-1">G.5 The Golden Reference (L =
1)</h3>
<p>Specifying <code>L = 1</code> collapses the algorithm to a single
global balanced binary tree. This serves as a hardware-agnostic baseline
for CI/CD testing and debugging numerical discrepancies.</p>
<h3 id="g.6-divergence-from-stdaccumulate">G.6 Divergence from
std::accumulate</h3>
<p><code>std::accumulate</code> specifies a strict linear left-fold with
depth O(N). <code>canonical_reduce</code> specifies a balanced binary
tree with depth O(log N). For non-associative operations, these are
different algebraic expressions and may produce different results.</p>
<p>For floating-point summation, the tree structure is often numerically
advantageous: error growth is O(log N · ε) versus O(N · ε) for
left-fold. This is well-established as “pairwise summation” in numerical
analysis [Higham2002].</p>
<h3 id="g.7-init-placement-determinism">G.7 Init Placement
Determinism</h3>
<p>If init placement were implementation-defined, two conforming
implementations could produce different results for the same inputs. For
non-commutative operations:</p>
<pre><code>op(I, R) = 5.0 * 2 + 10.0 = 20.0
op(R, I) = 10.0 * 2 + 5.0 = 25.0</code></pre>
<p>Therefore, this proposal normatively specifies <code>op(I, R)</code>
— init as left operand of the final combination.</p>
<h2 id="appendix-h-performance-feasibility-informative">Appendix H:
Performance Feasibility (Informative)</h2>
<p>This appendix provides representative prototype measurements to
support the claim that enforcing a fixed expression structure is
practical. These measurements are not a performance guarantee.</p>
<h3 id="h.1-prototype-test-conditions">H.1 Prototype test
conditions</h3>
<ul>
<li><strong>Floating-point evaluation model controls:</strong>
<code>-ffp-contract=off</code>, <code>-fno-fast-math</code> (GCC/Clang);
<code>/fp:precise</code> (MSVC)</li>
<li><strong>FMA explicitly disabled</strong> via compiler flags where
applicable</li>
<li><strong>Input sizes:</strong> 10K to 10M elements, uniformly
distributed random values</li>
</ul>
<h3 id="h.2-representative-observations">H.2 Representative
observations</h3>
<ul>
<li>Observed overhead of approximately 10-15% compared to
<code>std::reduce</code> for typical inputs in the tested
configurations.</li>
<li>Overhead is primarily attributable to enforcing a fixed
parenthesization (removing reassociation freedom) and maintaining
per-lane state.</li>
<li>Results are configuration-dependent; different compilers, CPUs/ISAs,
and choices of L can shift the trade-off materially.</li>
</ul>
<h3 id="h.3-interpretation">H.3 Interpretation</h3>
<p>Users opt into a canonical reduction when they value reproducibility
and auditability over peak throughput. Users requiring maximum
throughput can continue to use <code>std::reduce</code> (or
domain-specific facilities) where unspecified reassociation is
acceptable.</p>
<h2 id="appendix-i-rationale-for-lane-count-presets-informative">Appendix I:
Rationale for Lane Count Presets (Informative)</h2>
<p>This appendix records rationale for providing a small set of
<strong>standard-fixed lane count preset constants</strong> as
coordination points for topology selection (§9.6). The goal is to
provide readable, stable choices that do not vary with platform
properties, value type, or ABI, and therefore avoid “silent semantic
drift” in returned values for non-associative operations.</p>
<p>The lane count <code>L</code> is the semantic topology coordinate.
Because <code>L</code> is specified directly (not derived from a byte
span — see §9.2), the same preset selects the same abstract expression
regardless of <code>sizeof(value_type)</code>.</p>
<h3 id="i.1-rationale-for-l-16-narrow-preset">I.1 Rationale for L = 16
(Narrow Preset)</h3>
<p>A “narrow” preset should provide useful parallelism for the most
common scalar sizes while staying small enough that overhead does not
dominate for moderate N.</p>
<p>L = 16 is a practical coordination point because: - It matches
AVX-512 lane count for <code>float</code> (16 × 4 bytes = 64 bytes = one
ZMM register). - It provides 2× AVX-512 width for <code>double</code>
(16 × 8 bytes = 128 bytes), which maps to two-register unrolling — a
standard SIMD optimization pattern. - It is a power of two, ensuring the
per-lane tree is perfectly balanced for power-of-two K values. - It is
wide enough for meaningful instruction-level parallelism on all current
architectures yet narrow enough that the cross-lane Stage 2 tree is
shallow (4 levels).</p>
<h3 id="i.2-rationale-for-l-128-wide-preset">I.2 Rationale for L = 128
(Wide Preset)</h3>
<p>A “wide” preset is an explicit opt-in for throughput-oriented
structures and heterogeneous verification workflows.</p>
<p>L = 128 provides substantial independent work per lane and can
support aggressive batching and parallel evaluation of independent
reduction nodes while preserving the same abstract expression. It
accommodates GPU warp widths (32) with room for multi-warp cooperation
and enables deeply unrolled SIMD loops on CPU.</p>
<h3 id="i.3-cross-domain-verification">I.3 Cross-Domain
Verification</h3>
<p>Because these presets are standard-fixed lane counts, a user can
select the same preset when executing on different hardware or in
different deployment environments. When the floating-point evaluation
model is aligned (and the same <code>binary_op</code> and input order
are used), the abstract expression structure is identical; any remaining
divergence is attributable to differences in the underlying arithmetic
environment rather than to reassociation or topology choice.</p>
<p>[<em>Note:</em> This appendix is informative. These values are
coordination points for a stable abstract expression; they do not impose
any particular scheduling, threading, or vectorization strategy.
<em>—end note</em>]</p>
<h2 id="appendix-j-indicative-api-straw-man-informative">Appendix J:
Indicative API Straw Man (Informative)</h2>
<p>This appendix is illustrative; no API is proposed in this paper (§2).
It records one indicative way an eventual Standard Library facility
could expose the semantics defined in §4, while adopting the
standard-fixed named preset approach (Option 3 in §9.5). The intent is
to give LEWG something concrete to react to, while keeping spelling and
header placement explicitly provisional. Sender-returning (P2300)
surfaces are deferred to the follow-on API paper.</p>
<p><strong>Presentation note (informative):</strong> - The
<strong>semantic topology coordinate</strong> is the lane count
<code>L</code>. - The lane count <code>L</code> is the sole semantic
topology coordinate. The canonical expression is fully determined by
<code>(N, L)</code>. - When cross-platform expression stability is
needed, <code>L</code> must be specified directly (not derived from
hardware properties).</p>
<h3 id="j.1-design-goal">J.1 Design goal</h3>
<ul>
<li>Preserve the core semantic contract: for fixed topology selection,
the returned value is as-if evaluating the canonical expression.</li>
<li>Avoid “silent semantic drift” for a no-argument default: defaults
must be stable across targets/toolchains.</li>
<li>Make “coordination choices” readable: users should be able to say
“narrow / wide” in code reviews, not “16 / 128”.</li>
</ul>
<h3 id="j.2-favored-approach-standard-fixed-preset-constants-option-3">J.2
Favored approach: standard-fixed preset constants (Option 3)</h3>
<p>This approach exposes preset names as standard-fixed constants for
lane count <code>L</code>, and (optionally) defines a default in terms
of one of those preset constants.</p>
<p><strong>Illustrative (provisional) names:</strong></p>
<p>Placement in <code>namespace std</code> is illustrative only; a
follow-on API paper may place any such presets in a dedicated scope
(e.g., a nested namespace or tag type) to avoid adding new top-level
<code>std</code> names.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">namespace</span> std <span class="op">{</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Standard-fixed lane presets. Values never change.</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>  <span class="kw">inline</span> <span class="kw">constexpr</span> <span class="dt">size_t</span> canonical_lanes_narrow <span class="op">=</span> <span class="dv">16</span><span class="op">;</span>  <span class="co">// baseline lane preset</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>  <span class="kw">inline</span> <span class="kw">constexpr</span> <span class="dt">size_t</span> canonical_lanes_wide <span class="op">=</span> <span class="dv">128</span><span class="op">;</span>   <span class="co">// wide lane preset</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>  <span class="co">// This paper does not propose a default. A follow-on API paper may propose one.</span></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p><strong>Clarification (informative):</strong> The preset lane values
above are intended to be fixed, <code>constexpr</code> constants, not
implementation-tunable parameters, so that the same canonical expression
shape is selected across implementations. This does not imply
bitwise-identical results across different floating-point evaluation
models.</p>
<h3 id="j.3-straw-man-algorithm-api">J.3 Straw-man algorithm API</h3>
<p>The simplest surface is an algorithm family parameterized by lane
count <code>L</code>.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="kw">namespace</span> std <span class="op">{</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">template</span><span class="op">&lt;</span><span class="dt">size_t</span> L<span class="op">,</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>       <span class="kw">class</span> InputIterator<span class="op">,</span> <span class="kw">class</span> T<span class="op">,</span> <span class="kw">class</span> BinaryOperation<span class="op">&gt;</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>  <span class="kw">constexpr</span> T canonical_reduce<span class="op">(</span>InputIterator first<span class="op">,</span> InputIterator last<span class="op">,</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>                 T init<span class="op">,</span> BinaryOperation binary_op<span class="op">);</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>  <span class="kw">template</span><span class="op">&lt;</span><span class="dt">size_t</span> L<span class="op">,</span></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>       <span class="kw">class</span> ExecutionPolicy<span class="op">,</span> <span class="kw">class</span> ForwardIterator<span class="op">,</span> <span class="kw">class</span> T<span class="op">,</span> <span class="kw">class</span> BinaryOperation<span class="op">&gt;</span></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>  T canonical_reduce<span class="op">(</span>ExecutionPolicy<span class="op">&amp;&amp;</span> policy<span class="op">,</span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>           ForwardIterator first<span class="op">,</span> ForwardIterator last<span class="op">,</span></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>           T init<span class="op">,</span> BinaryOperation binary_op<span class="op">);</span></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p><strong>Typical call sites:</strong></p>
<div class="sourceCode" id="cb44"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Readable coordination points (narrow vs wide)</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> a <span class="op">=</span> <span class="bu">std::</span>canonical_reduce<span class="op">&lt;</span><span class="bu">std::</span>canonical_lanes_narrow<span class="op">&gt;(</span>v<span class="op">.</span>begin<span class="op">(),</span> v<span class="op">.</span>end<span class="op">(),</span> <span class="fl">0.0</span><span class="op">,</span> <span class="bu">std::</span>plus<span class="op">&lt;&gt;{});</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> b <span class="op">=</span> <span class="bu">std::</span>canonical_reduce<span class="op">&lt;</span><span class="bu">std::</span>canonical_lanes_wide<span class="op">&gt;(</span>v<span class="op">.</span>begin<span class="op">(),</span> v<span class="op">.</span>end<span class="op">(),</span> <span class="fl">0.0</span><span class="op">,</span> <span class="bu">std::</span>plus<span class="op">&lt;&gt;{});</span></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="co">// Explicit literal for hardware-specific alignment</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> c <span class="op">=</span> <span class="bu">std::</span>canonical_reduce<span class="op">&lt;</span><span class="dv">8</span><span class="op">&gt;(</span>v<span class="op">.</span>begin<span class="op">(),</span> v<span class="op">.</span>end<span class="op">(),</span> <span class="fl">0.0</span><span class="op">,</span> <span class="bu">std::</span>plus<span class="op">&lt;&gt;{});</span> <span class="co">// L = 8</span></span></code></pre></div>
<p><strong>Rationale for this shape (informative):</strong> - Keeps the
topology selection in the type system (NTTP), which is consistent with
“semantic selection”, not a performance hint. - Supports compile-time
specialization for a fixed topology coordinate without requiring new
execution-policy machinery. - Makes the coordination presets show up in
diagnostics and in code review as names. - For fixed <code>L</code>, the
canonical tree is identical across all targets, even if
<code>sizeof(V)</code> differs.</p>
<h3 id="j.4-notes-on-naming-and-evolution">J.4 Notes on naming and
evolution</h3>
<ul>
<li>The values of existing preset constants must never change. Future
standards may add new presets under new names.</li>
<li>Whether the facility lives as a new algorithm (e.g.,
<code>canonical_reduce</code>) or as a <code>std::reduce</code>-adjacent
customization is an LEWG design choice; this appendix only shows one
straightforward spelling.</li>
<li>If LEWG prefers to mirror [numerics.defns] (“GENERALIZED_SUM”-style
definitional functions), the same preset constants can still be used as
stable topology selectors.</li>
</ul>
<p>[<em>Note:</em> This appendix is illustrative. It is intended to
reduce “API haze” during discussion while keeping the semantic core of
the paper independent of any particular surface spelling. <em>—end
note</em>]</p>
<h3 id="j.5-range-overloads-straw-man">J.5 Range overloads
(straw-man)</h3>
<p>Range overloads are deferred per §2. The following sketches are
included only to reduce “API haze” and to indicate one illustrative
direction consistent with the semantic requirements of §4.</p>
<p><strong>Key observation:</strong> the canonical expression in §4 is
defined over N elements, so a range surface generally needs N prior to
evaluation. A surface can obtain N without allocation (e.g.,
<code>sized_range</code> or a counting pass over a multipass range), or
it can require explicit materialization for single-pass sources.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="kw">namespace</span> <span class="bu">std::</span>ranges <span class="op">{</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Lane coordinate — sequential/reference mode</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Non-sized forward ranges can be supported via a counting pass to determine N.</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>  <span class="kw">template</span><span class="op">&lt;</span><span class="dt">size_t</span> L<span class="op">,</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>       forward_range R<span class="op">,</span> <span class="kw">class</span> T<span class="op">,</span> <span class="kw">class</span> BinaryOperation<span class="op">&gt;</span></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>  <span class="kw">requires</span> <span class="op">(</span>L <span class="op">&gt;=</span> <span class="dv">1</span><span class="op">)</span></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>  <span class="kw">constexpr</span> T canonical_reduce<span class="op">(</span>R<span class="op">&amp;&amp;</span> r<span class="op">,</span> T init<span class="op">,</span> BinaryOperation op<span class="op">);</span></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Lane coordinate — execution policy overload (illustrative, conservative constraints)</span></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>  <span class="kw">template</span><span class="op">&lt;</span><span class="dt">size_t</span> L<span class="op">,</span></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>       <span class="kw">class</span> ExecutionPolicy<span class="op">,</span></span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>       random_access_range R<span class="op">,</span> <span class="kw">class</span> T<span class="op">,</span> <span class="kw">class</span> BinaryOperation<span class="op">&gt;</span></span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>  <span class="kw">requires</span> sized_range<span class="op">&lt;</span>R<span class="op">&gt;</span> <span class="op">&amp;&amp;</span></span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>       is_execution_policy_v<span class="op">&lt;</span><span class="dt">remove_cvref_t</span><span class="op">&lt;</span>ExecutionPolicy<span class="op">&gt;&gt;</span> <span class="op">&amp;&amp;</span></span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>       <span class="op">(</span>L <span class="op">&gt;=</span> <span class="dv">1</span><span class="op">)</span></span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>  T canonical_reduce<span class="op">(</span>ExecutionPolicy<span class="op">&amp;&amp;</span> policy<span class="op">,</span> R<span class="op">&amp;&amp;</span> r<span class="op">,</span> T init<span class="op">,</span> BinaryOperation op<span class="op">);</span></span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>[<em>Note:</em> The final constraints for a range surface (e.g.,
whether to permit a counting pass for non-sized multipass ranges, and
whether to reject single-pass <code>input_range</code> to avoid implicit
allocation) are design questions for a subsequent API-focused revision
once LEWG has accepted the semantic contract in §4. Appendix L records
the relevant trade-offs. <em>—end note</em>]</p>
<h2 id="appendix-k-demonstrator-godbolts-informative">Appendix K:
Demonstrator Godbolts (Informative)</h2>
<p>This appendix is not part of the proposal. It records the
demonstrator Compiler Explorer (“Godbolt”) programs used to validate the
semantics described in §4 on multiple architectures, and to show the
gross performance impact of enforcing a fixed abstract expression.</p>
<p><strong>The programs referenced here are semantic witnesses</strong>,
not reference implementations. They exist to demonstrate that the
canonical expression defined in §4 can be evaluated on real hardware
(SIMD, GPU, multi-threaded). They are not normative examples and are not
intended as implementation guidance for general <code>binary_op</code>.
All demonstrators except GB-SEQ test only
<code>std::plus&lt;double&gt;</code> and may use <code>0.0</code> for
absent operand positions; conforming implementations must handle
arbitrary <code>binary_op</code> via the lifted <code>COMBINE</code>
rules of §4.2.2.</p>
<h4 id="k.0-golden-reference-values">§K.0 Golden reference values</h4>
<p>The following reference results are used throughout the demonstrators
to validate bitwise reproducibility under the specified canonical
expression and fixed PRNG seed:</p>
<ul>
<li><strong>NARROW</strong> (<code>L = 16</code>):
<code>0x40618f71f6379380</code></li>
<li><strong>WIDE</strong> (<code>L = 128</code>):
<code>0x40618f71f6379397</code></li>
</ul>
<p>[<em>Note:</em> These values validate conformance to the canonical
expression for the specific demonstrator environment. Bitwise agreement
across platforms additionally requires aligned floating‑point evaluation
models (§2.5, §6). <em>—end note</em>]</p>
<p>The intent is pragmatic: give reviewers a “click-run-inspect”
artifact that: - validates semantic invariants (tree identity and
“golden hex” outputs) rather than trying to “win benchmarks”, - prints a
short verification block (seed, N, and the resulting hex value), -
checks run-to-run stability and translational invariance (address-offset
invariance), - and shows a coarse benchmark table comparing against
<code>std::accumulate</code> and <code>std::reduce</code> variants.</p>
<p><strong>Important caveat:</strong> Compiler Explorer run times vary
substantially with VM load, CPU model, and throttling. These tables are
illustrative only; the repository benchmarks (multi-thread and CUDA) are
the authoritative numbers.</p>
<h3 id="k.1-demonstrator-set-gross-platform-runs">K.1 Demonstrator set
(gross platform runs)</h3>
<p>The following Compiler Explorer (“Godbolt”) demonstrators are
intended to be click-run-inspect artifacts for reviewers. Each link has:
- a determinism/verification block (seed, N, and printed result hex), -
run-to-run stability checks, - a coarse timing table comparing against
<code>std::accumulate</code> and <code>std::reduce</code> variants under
comparable FP settings.</p>
<table>
<colgroup>
<col style="width: 21%" />
<col style="width: 15%" />
<col style="width: 14%" />
<col style="width: 37%" />
<col style="width: 10%" />
</colgroup>
<thead>
<tr class="header">
<th>Demonstrator</th>
<th>Platform</th>
<th>Purpose</th>
<th>Arbitrary <code>binary_op</code>?</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>[GB-SEQ] single-thread reference</td>
<td>Portable</td>
<td>Sequential evaluation of the canonical expression (§4)</td>
<td>Yes (faithful §4 reference)</td>
<td>Debugger-friendly “golden” comparator; single-threaded only.</td>
</tr>
<tr class="even">
<td>[GB-x86-AVX2] single-file x86</td>
<td>x86-64</td>
<td>Canonical reduction on x86 with AVX2 codegen; compares against
<code>std::accumulate</code> and <code>std::reduce</code></td>
<td>No (<code>std::plus&lt;double&gt;</code> only)</td>
<td>Uses safe feature gating; suitable for “Run”.</td>
</tr>
<tr class="odd">
<td>[GB-x86-MT] multi-threaded x86</td>
<td>x86-64</td>
<td>Deterministic multi-threaded reduction using shift-reduce stack
state + deterministic merge</td>
<td>No (<code>std::plus&lt;double&gt;</code> only)</td>
<td>Demonstrates schedule-independent, thread-count-independent
results.</td>
</tr>
<tr class="even">
<td>[GB-x86-MT-PERF] multi-threaded perf</td>
<td>x86-64</td>
<td>Production-quality multi-threaded reduction with thread pool and
per-lane carry pairwise</td>
<td>No (<code>std::plus&lt;double&gt;</code> only)</td>
<td>Thread pool amortizes creation cost; shows throughput scaling.</td>
</tr>
<tr class="odd">
<td>[GB-NEON] single-file NEON</td>
<td>AArch64</td>
<td>Canonical reduction on ARM64 using NEON (exact tree preserved)</td>
<td>No (<code>std::plus&lt;double&gt;</code> only)</td>
<td>Prints build-proof macros (<code>__aarch64__</code>,
<code>__ARM_NEON</code>, <code>__ARM_NEON_FP</code>).</td>
</tr>
<tr class="even">
<td>[GB-NEON-PERF] NEON performance</td>
<td>AArch64</td>
<td>Shift-reduce with 8-block NEON pre-reduction for near-bandwidth
throughput</td>
<td>No (<code>std::plus&lt;double&gt;</code> only)</td>
<td>8-block straight-line NEON hot loop; carry cascade fires 8× less
often.</td>
</tr>
<tr class="odd">
<td>[GB-CUDA] (optional) CUDA / CUB comparison</td>
<td>NVCC</td>
<td>Illustrates canonical topology evaluation on GPU and compares
against CUB reduction</td>
<td>No (<code>std::plus&lt;double&gt;</code> only)</td>
<td>Heterogeneous “golden result” workflow demonstrator.</td>
</tr>
</tbody>
</table>
<p><strong>Godbolt links:</strong></p>
<ul>
<li><p>[GB-SEQ] = https://godbolt.org/z/8EEhEqrz6<br />
(single-threaded reference; faithful §4 implementation including
<code>COMBINE</code> rules; supports arbitrary
<code>binary_op</code>)</p></li>
<li><p>[GB-x86-AVX2] = https://godbolt.org/z/Eaa3vWYqb<br />
(tests <code>std::plus&lt;double&gt;</code> only; uses <code>0.0</code>
for absent positions; proves SIMD vertical-add matches §4 tree)</p></li>
<li><p>[GB-x86-MT] = https://godbolt.org/z/7a11r9o95<br />
(tests <code>std::plus&lt;double&gt;</code> only; uses <code>0.0</code>
for absent positions; proves thread-count and schedule
invariance)</p></li>
<li><p>[GB-x86-MT-PERF] = https://godbolt.org/z/sdxMohT48<br />
(tests <code>std::plus&lt;double&gt;</code> only; thread pool + per-lane
carry pairwise; proves throughput scaling with thread count)</p></li>
<li><p>[GB-NEON] = https://godbolt.org/z/Pxzc3YM7q<br />
(Arm v8 / AArch64; tests <code>std::plus&lt;double&gt;</code> only; uses
<code>0.0</code> for absent positions; proves NEON vector reduction
matches §4 tree)</p></li>
<li><p>[GB-NEON-PERF] = https://godbolt.org/z/sY9W78rze<br />
(Arm v8 / AArch64; tests <code>std::plus&lt;double&gt;</code> only;
shift-reduce with 8-block NEON pre-reduction; near-bandwidth
throughput)</p></li>
<li><p>[GB-CUDA] = https://godbolt.org/z/5n9EvGoeb<br />
(CUDA/NVCC; tests <code>std::plus&lt;double&gt;</code> only; uses
<code>0.0</code> for absent positions; proves warp shuffle matches §4
tree; includes L=16 and L=128)</p></li>
</ul>
<p>All demonstrators use the same dataset generation (seeded RNG) and
report the same canonical results for the two topology coordinates used
throughout this paper:</p>
<ul>
<li><strong>Small / narrow:</strong> L = 16 for double</li>
<li><strong>Large / wide:</strong> L = 128 for double</li>
</ul>
<h3 id="k.2-what-the-demonstrators-are-intended-to-prove">K.2 What the
demonstrators are intended to prove</h3>
<p>Each demonstrator prints:</p>
<ol type="1">
<li><strong>Data verification:</strong> the first few generated elements
(as hex) match a known sequence for the fixed seed.</li>
<li><strong>Correctness:</strong> the returned value for L=16 and L=128
matches known “golden” hex outputs.</li>
<li><strong>Run-to-run stability:</strong> repeated evaluation yields
identical results.</li>
<li><strong>Translational invariance:</strong> copying the same values
to different memory offsets does not change the result.</li>
</ol>
<p>These are concrete checks that: - the canonical expression is
independent of execution schedule (single-thread vs vectorized), - the
topology is a function of the chosen lane count <code>L</code> and input
order, - and the implementation does not accidentally depend on address
alignment or allocator behavior.</p>
<h3 id="k.3-expected-verification-outputs-for-the-published-demonstrators">K.3
Expected verification outputs (for the published demonstrators)</h3>
<p>For the canonical seed and N = 1,000,000 doubles, the demonstrators
are configured to print the following expected result hex values:</p>
<ul>
<li><strong>L = 16, “NARROW”:</strong>
<code>0x40618f71f6379380</code></li>
<li><strong>L = 128, “WIDE”:</strong>
<code>0x40618f71f6379397</code></li>
</ul>
<p>The demonstrators treat a mismatch as a test failure.</p>
<h3 id="k.3.1-cancellation-stress-dataset-recommended">K.3.1
Cancellation stress dataset (recommended)</h3>
<p>The PRNG dataset validates implementation correctness but may not
demonstrate <em>why</em> canonical reduction matters. Uniform random
values of similar size can produce nearly identical results under
different tree shapes, making the golden match appear trivially
achievable.</p>
<p>To demonstrate the facility’s core value proposition, demonstrators
should additionally include a <strong>cancellation-heavy
dataset</strong> where evaluation order visibly affects the result. The
pattern from §1.2, scaled to large N:</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Cancellation stress: [+1e16, +1.0, -1e16, +1.0, ...] repeated N/4 times</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="bu">std::</span>vector<span class="op">&lt;</span><span class="dt">double</span><span class="op">&gt;</span> data<span class="op">;</span></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">size_t</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> N<span class="op">/</span><span class="dv">4</span><span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>    data<span class="op">.</span>push_back<span class="op">(+</span><span class="fl">1e16</span><span class="op">);</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>    data<span class="op">.</span>push_back<span class="op">(+</span><span class="fl">1.0</span><span class="op">);</span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>    data<span class="op">.</span>push_back<span class="op">(-</span><span class="fl">1e16</span><span class="op">);</span></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>    data<span class="op">.</span>push_back<span class="op">(+</span><span class="fl">1.0</span><span class="op">);</span></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>For this dataset, the demonstrators should show all three of:</p>
<ol type="1">
<li><strong><code>std::reduce</code> is non-deterministic:</strong>
repeated runs (or varying thread counts) produce different hex values,
because the scheduler changes the effective parenthesization.</li>
<li><strong><code>canonical_reduce</code> with fixed <code>L</code> is
deterministic:</strong> repeated runs produce identical hex values,
because the tree is fixed.</li>
<li><strong>Different <code>L</code> values produce different
results:</strong> confirming that the topology coordinate controls the
abstract expression and is not an implementation hint.</li>
</ol>
<p>This triple is the paper’s entire motivation in one test output. If
the PRNG dataset is the “does the implementation work?” test, the
cancellation dataset is the “does the facility matter?” test.</p>
<h3 id="k.4-recommended-compiler-explorer-settings">K.4 Recommended
Compiler Explorer settings</h3>
<h4 id="k.4.1-x86-demonstrator-gb-x86-avx2">K.4.1 x86 demonstrator
([GB-x86-AVX2])</h4>
<p><strong>Compiler:</strong> x86-64 clang (trunk) (or GCC)</p>
<p><strong>Recommended flags (for running):</strong></p>
<pre><code>-O3 -std=c++20 -ffp-contract=off -fno-fast-math</code></pre>
<h4 id="k.4.2-aarch64-demonstrator-gb-neon">K.4.2 AArch64 demonstrator
([GB-NEON])</h4>
<p><strong>Compiler:</strong> AArch64 clang (trunk) (or GCC)</p>
<p><strong>Recommended flags (for running):</strong></p>
<pre><code>-O3 -std=c++20 -march=armv8-a -ffp-contract=off -fno-fast-math</code></pre>
<p>If <code>std::execution::par</code> is unavailable or unreliable in
the CE environment, the demonstrator can be built with:</p>
<pre><code>-DNO_PAR_POLICIES=1</code></pre>
<h3 id="k.5-interpreting-the-performance-tables-gross-impacts">K.5
Interpreting the performance tables (gross impacts)</h3>
<p>The demonstrators typically report: - <code>std::accumulate</code> as
a deterministic, sequential baseline; - <code>std::reduce</code>
variants (no policy, seq, unseq, and optionally par*) as “existing
practice” comparators; - deterministic/canonical narrow and wide
presets.</p>
<p>Readers should interpret the results as: - <strong>Gross cost of
structure:</strong> overhead (or sometimes speedup) relative to
<code>std::reduce(seq)</code> under similar FP settings. -
<strong>Configuration sensitivity:</strong> narrow vs wide presets can
trade off cache behavior, vectorization, and bandwidth. -
<strong>Non-authoritative:</strong> results on CE do not replace proper
benchmarking; they are a convenience for quick inspection.</p>
<h3 id="k.6-relationship-to-repository-evidence">K.6 Relationship to
repository evidence</h3>
<p>The accompanying repository (referenced in the paper’s artifacts
section) contains: - controlled micro-benchmarks on pinned CPUs
(repeatable measurements), - multi-threaded execution model comparisons,
- and a CUDA demonstrator evaluating the same canonical expression (for
chosen topology) to support heterogeneous “golden result” workflows.</p>
<p>[<em>Note:</em> The paper’s semantic contract is independent of any
specific demonstrator; these artifacts exist to make the semantics
tangible and reviewable. <em>—end note</em>]</p>
<p>[<em>Note:</em> These demonstrators depend on specific compiler
versions and Compiler Explorer VM configurations available at the time
of writing. The normative specification is §4; demonstrator links
provide supplementary illustration only. If a link becomes unavailable
or produces results inconsistent with the published golden values due to
toolchain changes, the specification remains unaffected. <em>—end
note</em>]</p>
<h2 id="appendix-l-ranges-compatibility-informative">Appendix L: Ranges
Compatibility (Informative)</h2>
<p>This appendix is not part of the proposal. It records design
considerations for eventual C++20/23 ranges overloads that expose the
semantics of §4 in a composable way.</p>
<h3 id="l.1-a-range-surface-does-not-change-the-semantic-contract">L.1 A
range surface does not change the semantic contract</h3>
<p>A range-based surface would not change the semantic contract: for a
fixed lane count <code>L</code>, input order, and
<code>binary_op</code>, the returned value is as-if evaluating the
canonical expression defined in §4.</p>
<p>This paper intentionally defers the ranges surface to keep first
discussion focused on the semantic contract (see §2). Appendix J.5
contains a straw-man sketch.</p>
<h3 id="l.2-determining-n-without-hidden-allocation">L.2 Determining N
without hidden allocation</h3>
<p>The canonical expression in §4 is defined over N elements in a fixed
input order. A range-based surface therefore needs a way to determine N
and to preserve the element order used by the expression.</p>
<p>Three implementation strategies exist:</p>
<ul>
<li><strong>sized_range:</strong> obtain N in O(1) via
<code>ranges::size(r)</code>.</li>
<li><strong>Non-sized, multipass (forward_range):</strong> determine N
via a counting pass (e.g., <code>ranges::distance(r)</code>), then
evaluate the canonical expression in a second pass. This avoids
allocation but may traverse the range twice.</li>
<li><strong>Single-pass (input_range):</strong> the count is not
available without consuming the range; this specification does not
support single-pass ranges without explicit materialization by the
caller.</li>
</ul>
<p>One conservative design point (for a subsequent API revision) is to
avoid implicit allocation:</p>
<ul>
<li><strong>Sequential/reference overloads:</strong> accept
<code>forward_range</code> and permit a counting pass when N is not
otherwise known.</li>
<li><strong>Execution-policy overloads:</strong> require a stronger
range category (e.g., <code>random_access_range</code>) and may also
require <code>sized_range</code>, keeping buffering explicit.</li>
</ul>
<h3 id="l.3-working-with-non-sized-single-pass-sources">L.3 Working with
non-sized, single-pass sources</h3>
<p>For sources that are single-pass (or otherwise cannot be traversed
twice), users requiring canonical reduction can materialize
explicitly:</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> filtered <span class="op">=</span> data</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>  <span class="op">|</span> <span class="bu">std::</span>views::filter<span class="op">(</span>pred<span class="op">)</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>  <span class="op">|</span> <span class="bu">std::</span>ranges::to<span class="op">&lt;</span><span class="bu">std::</span>vector<span class="op">&gt;();</span></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> r <span class="op">=</span> <span class="bu">std::</span>ranges::canonical_reduce<span class="op">&lt;</span><span class="bu">std::</span>canonical_lanes_narrow<span class="op">&gt;(</span></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>  filtered<span class="op">,</span></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>  <span class="fl">0.0</span><span class="op">,</span></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>  <span class="bu">std::</span>plus<span class="op">&lt;&gt;{});</span></span></code></pre></div>
<p>This makes allocation explicit and keeps the cost model under user
control.</p>
<h3 id="l.4-projection-parameter">L.4 Projection parameter</h3>
<p>This paper does not attempt to design a projection parameter. Users
can express projection by composing with <code>views::transform</code>,
keeping the algorithm surface orthogonal and consistent with ranges
composition:</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> sum <span class="op">=</span> <span class="bu">std::</span>ranges::canonical_reduce<span class="op">&lt;</span><span class="bu">std::</span>canonical_lanes_narrow<span class="op">&gt;(</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>  data <span class="op">|</span> <span class="bu">std::</span>views::transform<span class="op">([](</span><span class="at">const</span> <span class="kw">auto</span><span class="op">&amp;</span> x<span class="op">)</span> <span class="op">{</span> <span class="cf">return</span> x<span class="op">.</span>value<span class="op">;</span> <span class="op">}),</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>  <span class="fl">0.0</span><span class="op">,</span></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>  <span class="bu">std::</span>plus<span class="op">&lt;&gt;{});</span></span></code></pre></div>
<p>[<em>Note:</em> This appendix is informative. It does not commit the
proposal to a particular ranges overload set; it documents constraints
and trade-offs to inform a subsequent API-focused revision. <em>—end
note</em>]</p>
<h2 id="appendix-m-detailed-motivation-and-use-cases-informative">Appendix
M: Detailed Motivation and Use Cases (Informative)</h2>
<p>This appendix is not part of the proposal. It collects the longer
use-case narratives (with examples) to keep the main document focused on
the semantic contract in §4.</p>
<p>The following use cases illustrate the value of run-to-run stable
parallel reduction.</p>
<p>[<em>Note:</em> Code examples use a placeholder spelling. No specific
API is proposed in this paper. <em>—end note</em>]</p>
<h3 id="m.1-ci-regression-testing">M.1 CI Regression Testing</h3>
<div class="sourceCode" id="cb52"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Test that fails intermittently with std::reduce</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="dt">double</span> baseline <span class="op">=</span> load_golden_value<span class="op">(</span><span class="st">&quot;gradient_sum.bin&quot;</span><span class="op">);</span> <span class="co">// Previously computed</span></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="dt">double</span> computed <span class="op">=</span> <span class="bu">std::</span>reduce<span class="op">(</span><span class="bu">std::</span>execution::par<span class="op">,</span></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>               gradients<span class="op">.</span>begin<span class="op">(),</span> gradients<span class="op">.</span>end<span class="op">(),</span> <span class="fl">0.0</span><span class="op">);</span></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>EXPECT_DOUBLE_EQ<span class="op">(</span>baseline<span class="op">,</span> computed<span class="op">);</span> <span class="co">// FAILS: result varies by thread scheduling</span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a><span class="co">// With canonical_reduce: no run-to-run variation</span></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a><span class="dt">double</span> computed <span class="op">=</span> canonical_reduce<span class="op">&lt;</span><span class="dv">8</span><span class="op">&gt;(</span>           <span class="co">// L = 8</span></span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>  gradients<span class="op">.</span>begin<span class="op">(),</span> gradients<span class="op">.</span>end<span class="op">(),</span> <span class="fl">0.0</span><span class="op">);</span></span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>EXPECT_DOUBLE_EQ<span class="op">(</span>baseline<span class="op">,</span> computed<span class="op">);</span> <span class="co">// PASSES: expression structure is fixed</span></span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a><span class="co">// (baseline must also have been computed with same L on same platform)</span></span></code></pre></div>
<p><strong>Value:</strong> Eliminate spurious test failures caused by
run-to-run variation from unspecified reduction order. Within a
consistent build/test environment, the returned value is stable across
invocations.</p>
<h3 id="m.2-distributed-training-checkpoints">M.2 Distributed Training
Checkpoints</h3>
<div class="sourceCode" id="cb53"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Machine learning gradient aggregation</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="co">// Expression structure fixed by the chosen lane count L, enabling reproducible checkpoints</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> gradient_sum <span class="op">=</span> canonical_reduce<span class="op">&lt;</span><span class="dv">8</span><span class="op">&gt;(</span></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>  local_gradients<span class="op">.</span>begin<span class="op">(),</span> local_gradients<span class="op">.</span>end<span class="op">(),</span> <span class="fl">0.0</span><span class="op">);</span></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a><span class="co">// Later, on same or equivalent hardware:</span></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> restored_sum <span class="op">=</span> canonical_reduce<span class="op">&lt;</span><span class="dv">8</span><span class="op">&gt;(</span></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>  restored_gradients<span class="op">.</span>begin<span class="op">(),</span> restored_gradients<span class="op">.</span>end<span class="op">(),</span> <span class="fl">0.0</span><span class="op">);</span></span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a><span class="co">// Reproducible: same inputs + same L + the same floating-point evaluation model → same result</span></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a><span class="co">// No longer subject to thread-count or scheduling variation</span></span></code></pre></div>
<p><strong>Value:</strong> Enable checkpoint/restore with reproducible
gradient aggregation, eliminating run-to-run variation from unspecified
reduction order. Cross-platform restore requires matching floating-point
semantics.</p>
<h3 id="m.3-regulatory-audit-trails">M.3 Regulatory Audit Trails</h3>
<div class="sourceCode" id="cb54"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Financial risk calculation that must be reproducible for auditors</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="kw">struct</span> RiskCalculation <span class="op">{</span></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">static</span> <span class="kw">constexpr</span> <span class="dt">size_t</span> L <span class="op">=</span> <span class="dv">8</span><span class="op">;</span> <span class="co">// Documented in audit trail</span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">double</span> compute_var<span class="op">(</span><span class="at">const</span> <span class="bu">std::</span>vector<span class="op">&lt;</span><span class="dt">double</span><span class="op">&gt;&amp;</span> scenarios<span class="op">)</span> <span class="op">{</span></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> canonical_reduce<span class="op">&lt;</span>L<span class="op">&gt;(</span></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>      scenarios<span class="op">.</span>begin<span class="op">(),</span> scenarios<span class="op">.</span>end<span class="op">(),</span> <span class="fl">0.0</span><span class="op">,</span></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>      <span class="op">[](</span><span class="dt">double</span> a<span class="op">,</span> <span class="dt">double</span> b<span class="op">)</span> <span class="op">{</span> <span class="cf">return</span> a <span class="op">+</span> b <span class="op">*</span> b<span class="op">;</span> <span class="op">});</span></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a><span class="op">};</span></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a><span class="co">// Auditor can reproduce exact result years later with same L</span></span></code></pre></div>
<p><strong>Value:</strong> Meet regulatory requirements for reproducible
risk calculations (Basel III, Solvency II).</p>
<h3 id="m.4-scientific-reproducibility">M.4 Scientific
Reproducibility</h3>
<div class="sourceCode" id="cb55"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Climate model energy conservation check</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="co">// Paper claims: &quot;Energy drift &lt; 1e-12 per century&quot;</span></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a><span class="co">// Reviewers must reproduce this result</span></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a><span class="kw">auto</span> total_energy <span class="op">=</span> canonical_reduce<span class="op">&lt;</span><span class="dv">8</span><span class="op">&gt;(</span></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>  grid_cells<span class="op">.</span>begin<span class="op">(),</span> grid_cells<span class="op">.</span>end<span class="op">(),</span> <span class="fl">0.0</span><span class="op">,</span></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>  <span class="op">[](</span><span class="dt">double</span> sum<span class="op">,</span> <span class="at">const</span> Cell<span class="op">&amp;</span> c<span class="op">)</span> <span class="op">{</span> <span class="cf">return</span> sum <span class="op">+</span> c<span class="op">.</span>kinetic <span class="op">+</span> c<span class="op">.</span>potential<span class="op">;</span> <span class="op">});</span></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a><span class="co">// Published with: &quot;Results computed with L=8, compiled with -ffp-contract=off&quot;</span></span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a><span class="co">// Reviewer with the same L and evaluation-model settings gets same result</span></span></code></pre></div>
<p><strong>Value:</strong> Enable peer verification of published
computational results when floating-point evaluation model is documented
and matched.</p>
<h3 id="m.5-exascale-hpc-with-kokkos">M.5 Exascale HPC with Kokkos</h3>
<div class="sourceCode" id="cb56"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co">// DOE exascale application using Kokkos</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="co">// Current Kokkos parallel_reduce is non-deterministic</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="co">// Today: Results vary across runs, making debugging nearly impossible</span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>Kokkos<span class="op">::</span>parallel_reduce<span class="op">(</span><span class="st">&quot;EnergySum&quot;</span><span class="op">,</span> N<span class="op">,</span> KOKKOS_LAMBDA<span class="op">(</span><span class="dt">int</span> i<span class="op">,</span> <span class="dt">double</span><span class="op">&amp;</span> sum<span class="op">)</span> <span class="op">{</span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>  sum <span class="op">+=</span> compute_energy<span class="op">(</span>i<span class="op">);</span></span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a><span class="op">},</span> total_energy<span class="op">);</span></span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a><span class="co">// With standardized canonical reduction semantics, frameworks could expose a canonical mode</span></span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a><span class="co">// (illustrative — actual Kokkos API would be determined by Kokkos maintainers)</span></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>Kokkos<span class="op">::</span>parallel_reduce<span class="op">&lt;</span>Kokkos<span class="op">::</span>CanonicalLanes<span class="op">&lt;</span><span class="dv">8</span><span class="op">&gt;&gt;(</span><span class="st">&quot;EnergySum&quot;</span><span class="op">,</span> N<span class="op">,</span> KOKKOS_LAMBDA<span class="op">(</span><span class="dt">int</span> i<span class="op">,</span> <span class="dt">double</span><span class="op">&amp;</span> sum<span class="op">)</span> <span class="op">{</span></span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>  sum <span class="op">+=</span> compute_energy<span class="op">(</span>i<span class="op">);</span></span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a><span class="op">},</span> total_energy<span class="op">);</span></span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a><span class="co">// Reproducible across runs on same platform</span></span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a><span class="co">// Cross-platform reproducibility requires matching FP semantics</span></span></code></pre></div>
<p><strong>Value:</strong> Enable reproducible physics in production HPC
codes, at least within a consistent execution environment.</p>
<h3 id="m.6-cross-platform-development">M.6 Cross-Platform
Development</h3>
<div class="sourceCode" id="cb57"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Game physics: aim for consistency between client platforms</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="kw">constexpr</span> <span class="dt">size_t</span> PHYSICS_L <span class="op">=</span> <span class="dv">16</span><span class="op">;</span> <span class="co">// 16 lanes</span></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="dt">float</span> compute_collision_impulse<span class="op">(</span><span class="at">const</span> <span class="bu">std::</span>vector<span class="op">&lt;</span>Contact<span class="op">&gt;&amp;</span> contacts<span class="op">)</span> <span class="op">{</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> canonical_reduce<span class="op">&lt;</span>PHYSICS_L<span class="op">&gt;(</span></span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>    contacts<span class="op">.</span>begin<span class="op">(),</span> contacts<span class="op">.</span>end<span class="op">(),</span> <span class="fl">0.0</span><span class="bu">f</span><span class="op">,</span></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>    <span class="op">[](</span><span class="dt">float</span> sum<span class="op">,</span> <span class="at">const</span> Contact<span class="op">&amp;</span> c<span class="op">)</span> <span class="op">{</span> <span class="cf">return</span> sum <span class="op">+</span> c<span class="op">.</span>impulse<span class="op">;</span> <span class="op">});</span></span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a><span class="co">// Expression structure is fixed.</span></span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a><span class="co">// Cross-platform match requires controlling floating-point evaluation model</span></span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a><span class="co">// (e.g., disabling FMA, matching rounding modes)</span></span></code></pre></div>
<p><strong>Value:</strong> Fix expression structure as one component of
cross-platform consistency. Full cross-platform bitwise identity
additionally requires matching hardware-level FP behaviors (e.g.,
contraction, intermediate precision, and subnormal handling), which are
outside the scope of this proposal.</p>
<h3 id="m.7-reference-and-debugging-mode">M.7 Reference and Debugging
Mode</h3>
<p>A practical requirement for reproducibility is the ability to
validate and debug operator logic on a single thread while preserving
the same abstract expression as production parallel execution. For that
reason, if an eventual Standard Library API is adopted, the overload
without an ExecutionPolicy should be specified to evaluate the identical
canonical expression tree as the execution-policy overloads for the same
L and inputs.</p>
<p>This “reference mode” enables: - Reproducing “golden results” for
audit or regression testing on a single thread, - Debugging
<code>binary_op</code> with conventional tools while guaranteeing
expression-equivalence to the parallel workload, - Cross-platform
verification (e.g., CPU verification of accelerator-produced results)
when evaluation models are aligned.</p>
<p>[<em>Note:</em> This paper does not propose a specific API shape;
this subsection records a design requirement that follows from the
semantic goal. <em>—end note</em>]</p>
<h2 id="appendix-n-multi-threaded-implementation-via-ordered-state-merge-informative">Appendix
N: Multi-Threaded Implementation via Ordered State Merge
(Informative)</h2>
<p>This appendix describes a multi-threaded implementation strategy that
achieves parallel execution while preserving bitwise identity with the
single-threaded canonical expression. A reference implementation is
available at <strong>[GB-x86-MT]</strong>.</p>
<h3 id="n.1-overview">N.1 Overview</h3>
<p>The single-threaded shift-reduce algorithm (§4.2) processes blocks
sequentially, maintaining a binary-counter stack that implicitly encodes
the canonical pairwise tree. To parallelize this while preserving
determinism, we observe that:</p>
<ol type="1">
<li>The stack state after processing any prefix of blocks is a complete
representation of that prefix’s reduction</li>
<li>Two stack states can be merged to produce the state that would
result from processing their blocks sequentially</li>
<li>Power-of-2 aligned partition boundaries ensure merges are
algebraically equivalent to sequential processing</li>
</ol>
<p>This leads to a three-phase algorithm: parallel local reduction,
deterministic merge, and final fold.</p>
<h3 id="n.2-stack-state-representation">N.2 Stack State
Representation</h3>
<p>An <strong>ordered reduction state</strong> summarizes a contiguous
range as an ordered sequence of fully reduced power-of-two blocks.</p>
<p>For a contiguous range <code>R</code>, the state may be viewed as a
sequence:</p>
<pre><code>[B0, B1, ..., Bm]</code></pre>
<p>where each <code>Bi</code> is the canonical reduction of a contiguous
subrange of <code>R</code>, the subranges are disjoint and appear in
strictly increasing stream order, each block size is <code>2^k</code>,
and the block sizes are strictly decreasing. The concatenation of these
subranges equals <code>R</code>.</p>
<p>The stack/bucket representation below is one concrete way to store
this ordered block sequence and to implement the canonical “coalesce
equal-sized adjacent blocks” rule used by shift-reduce.</p>
<p>A <strong>stack state</strong> compactly represents a partially
reduced sequence as a collection of buckets:</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span><span class="op">&lt;</span><span class="dt">size_t</span> L<span class="op">&gt;</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="kw">struct</span> StackState <span class="op">{</span></span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">static</span> <span class="kw">constexpr</span> <span class="dt">size_t</span> MAX_DEPTH <span class="op">=</span> <span class="dv">32</span><span class="op">;</span></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span> buckets<span class="op">[</span>MAX_DEPTH<span class="op">][</span>L<span class="op">];</span>  <span class="co">// buckets[k] holds 2^k blocks worth of reduction</span></span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>    <span class="dt">size_t</span> counts<span class="op">[</span>MAX_DEPTH<span class="op">];</span>      <span class="co">// lane counts (L for full, &lt;L for partial)</span></span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>    <span class="dt">uint32_t</span> mask<span class="op">;</span>                 <span class="co">// bit k set iff buckets[k] is occupied</span></span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a><span class="op">};</span></span></code></pre></div>
<p><strong>Invariant:</strong> If <code>mask</code> has bits set at
positions {k₀, k₁, …, kₘ} where k₀ &lt; k₁ &lt; … &lt; kₘ, then the
state represents a prefix of length:</p>
<pre><code>num_blocks = 2^k₀ + 2^k₁ + ... + 2^kₘ</code></pre>
<p>Each <code>buckets[kᵢ]</code> holds the fully reduced L-lane vector
of a contiguous block of 2^kᵢ input blocks. <strong>Lower-indexed
buckets represent more recent (rightward) blocks; higher-indexed buckets
represent older (leftward) blocks.</strong></p>
<h3 id="n.3-the-push-operation">N.3 The Push Operation</h3>
<p>The push operation incorporates a new L-lane vector into the stack
state, implementing binary-counter carry propagation:</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> push<span class="op">(</span>StackState<span class="op">&amp;</span> S<span class="op">,</span> <span class="at">const</span> <span class="dt">double</span><span class="op">*</span> vec<span class="op">,</span> <span class="dt">size_t</span> count<span class="op">,</span> <span class="dt">size_t</span> level <span class="op">=</span> <span class="dv">0</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span> current<span class="op">[</span>L<span class="op">];</span></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>    copy<span class="op">(</span>vec<span class="op">,</span> current<span class="op">,</span> count<span class="op">);</span></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>    <span class="dt">size_t</span> current_count <span class="op">=</span> count<span class="op">;</span></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="op">(</span>S<span class="op">.</span>mask <span class="op">&amp;</span> <span class="op">(</span><span class="dv">1</span><span class="bu">u</span> <span class="op">&lt;&lt;</span> level<span class="op">))</span> <span class="op">{</span></span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Bucket exists: combine (older on left)</span></span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>        current <span class="op">=</span> combine<span class="op">(</span>S<span class="op">.</span>buckets<span class="op">[</span>level<span class="op">],</span> S<span class="op">.</span>counts<span class="op">[</span>level<span class="op">],</span> </span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>                          current<span class="op">,</span> current_count<span class="op">);</span></span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a>        S<span class="op">.</span>mask <span class="op">&amp;=</span> <span class="op">~(</span><span class="dv">1</span><span class="bu">u</span> <span class="op">&lt;&lt;</span> level<span class="op">);</span>  <span class="co">// Clear bucket</span></span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>        <span class="op">++</span>level<span class="op">;</span>                    <span class="co">// Carry to next level</span></span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a>    S<span class="op">.</span>buckets<span class="op">[</span>level<span class="op">]</span> <span class="op">=</span> current<span class="op">;</span></span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a>    S<span class="op">.</span>counts<span class="op">[</span>level<span class="op">]</span> <span class="op">=</span> current_count<span class="op">;</span></span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a>    S<span class="op">.</span>mask <span class="op">|=</span> <span class="op">(</span><span class="dv">1</span><span class="bu">u</span> <span class="op">&lt;&lt;</span> level<span class="op">);</span></span>
<span id="cb61-17"><a href="#cb61-17" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p><strong>Key property:</strong> Processing element i triggers carries
corresponding to the trailing 1-bits in the binary representation of i.
This exactly mirrors the canonical pairwise tree structure.</p>
<h3 id="n.4-the-fold-operation">N.4 The Fold Operation</h3>
<p>After all blocks are pushed, the stack is collapsed to a single
vector:</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="dt">double</span><span class="op">*</span> fold<span class="op">(</span><span class="at">const</span> StackState<span class="op">&amp;</span> S<span class="op">)</span> <span class="op">{</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span> acc<span class="op">[</span>L<span class="op">];</span></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>    <span class="dt">size_t</span> acc_count <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>    <span class="dt">bool</span> have <span class="op">=</span> <span class="kw">false</span><span class="op">;</span></span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">// CRITICAL: Iterate low-to-high, bucket on LEFT</span></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">size_t</span> k <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> k <span class="op">&lt;</span> MAX_DEPTH<span class="op">;</span> <span class="op">++</span>k<span class="op">)</span> <span class="op">{</span></span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="op">(!(</span>S<span class="op">.</span>mask <span class="op">&amp;</span> <span class="op">(</span><span class="dv">1</span><span class="bu">u</span> <span class="op">&lt;&lt;</span> k<span class="op">)))</span> <span class="cf">continue</span><span class="op">;</span></span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="op">(!</span>have<span class="op">)</span> <span class="op">{</span></span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a>            acc <span class="op">=</span> S<span class="op">.</span>buckets<span class="op">[</span>k<span class="op">];</span></span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a>            acc_count <span class="op">=</span> S<span class="op">.</span>counts<span class="op">[</span>k<span class="op">];</span></span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a>            have <span class="op">=</span> <span class="kw">true</span><span class="op">;</span></span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span> <span class="cf">else</span> <span class="op">{</span></span>
<span id="cb62-15"><a href="#cb62-15" aria-hidden="true" tabindex="-1"></a>            <span class="co">// Higher bucket (older) goes on LEFT</span></span>
<span id="cb62-16"><a href="#cb62-16" aria-hidden="true" tabindex="-1"></a>            acc <span class="op">=</span> combine<span class="op">(</span>S<span class="op">.</span>buckets<span class="op">[</span>k<span class="op">],</span> S<span class="op">.</span>counts<span class="op">[</span>k<span class="op">],</span> acc<span class="op">,</span> acc_count<span class="op">);</span></span>
<span id="cb62-17"><a href="#cb62-17" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb62-18"><a href="#cb62-18" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb62-19"><a href="#cb62-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> acc<span class="op">;</span></span>
<span id="cb62-20"><a href="#cb62-20" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p><strong>Critical detail:</strong> The fold must iterate from low to
high indices, placing each bucket on the <strong>left</strong> of the
accumulator. This reconstructs the canonical tree’s final merges where
older (leftward) partial results combine with newer (rightward)
ones.</p>
<h3 id="n.5-power-of-2-aligned-partitioning">N.5 Power-of-2 Aligned
Partitioning</h3>
<p>For multi-threaded execution, we partition the block index space
among threads. <strong>The critical requirement is that partition
boundaries fall on power-of-2 aligned indices.</strong></p>
<p><strong>Definition:</strong> A block index b is <em>k-aligned</em> if
b is a multiple of 2^k.</p>
<p><strong>Observation:</strong> After processing blocks [0, b) where b
= m × 2^k, the stack state has no occupied buckets below level k.</p>
<p>The binary representation of b has zeros in positions 0 through k-1.
Since bucket[j] is occupied iff bit j is set in the count of processed
blocks, buckets 0 through k-1 are empty.</p>
<p><strong>Consequence:</strong> When thread boundaries are power-of-2
aligned, the “receiving” state A has no low-level buckets that could
collide incorrectly with the “incoming” state B’s buckets during
merge.</p>
<h3 id="n.6-partition-strategy">N.6 Partition Strategy</h3>
<p>Given B total blocks and T threads, choose chunk size C = 2^k where k
is the largest integer such that B / 2^k ≥ T:</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="dt">size_t</span> choose_chunk_size<span class="op">(</span><span class="dt">size_t</span> num_blocks<span class="op">,</span> <span class="dt">size_t</span> T<span class="op">)</span> <span class="op">{</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>num_blocks <span class="op">&lt;=</span> T<span class="op">)</span> <span class="cf">return</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>    <span class="dt">size_t</span> k <span class="op">=</span> bit_width<span class="op">(</span>num_blocks <span class="op">/</span> T<span class="op">)</span> <span class="op">-</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dt">size_t</span><span class="op">{</span><span class="dv">1</span><span class="op">}</span> <span class="op">&lt;&lt;</span> k<span class="op">;</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>This yields ceil(B / C) chunks, each having C blocks (except possibly
the last). Thread t processes chunks assigned to it, producing a local
stack state for each.</p>
<p><strong>Example:</strong> For B = 1000 blocks and T = 4 threads, C =
256 (2^8):</p>
<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th>Thread</th>
<th>Chunks</th>
<th>Blocks</th>
<th>Output</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
<td>[0, 256)</td>
<td>Single bucket at level 8</td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>[256, 512)</td>
<td>Single bucket at level 8</td>
</tr>
<tr class="odd">
<td>2</td>
<td>2</td>
<td>[512, 768)</td>
<td>Single bucket at level 8</td>
</tr>
<tr class="even">
<td>3</td>
<td>3</td>
<td>[768, 1000)</td>
<td>Buckets at levels {7, 6, 5, 3} for remainder 232</td>
</tr>
</tbody>
</table>
<p>Full chunks produce exactly one bucket at level k (since 2^k blocks
reduce to one bucket). The remainder chunk (232 = 128 + 64 + 32 + 8)
naturally decomposes via shift-reduce into multiple buckets.</p>
<h3 id="n.7-the-merge-operation">N.7 The Merge Operation</h3>
<p>To combine two stack states where A represents an earlier (leftward)
portion of the sequence and B represents a later (rightward)
portion:</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> merge_into<span class="op">(</span>StackState<span class="op">&amp;</span> A<span class="op">,</span> <span class="at">const</span> StackState<span class="op">&amp;</span> B<span class="op">)</span> <span class="op">{</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Process B&#39;s buckets in increasing level order</span></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">size_t</span> k <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> k <span class="op">&lt;</span> MAX_DEPTH<span class="op">;</span> <span class="op">++</span>k<span class="op">)</span> <span class="op">{</span></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="op">(</span>B<span class="op">.</span>mask <span class="op">&amp;</span> <span class="op">(</span><span class="dv">1</span><span class="bu">u</span> <span class="op">&lt;&lt;</span> k<span class="op">))</span> <span class="op">{</span></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>            <span class="co">// Push B&#39;s bucket into A, starting at level k (NOT level 0!)</span></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>            push<span class="op">(</span>A<span class="op">,</span> B<span class="op">.</span>buckets<span class="op">[</span>k<span class="op">],</span> B<span class="op">.</span>counts<span class="op">[</span>k<span class="op">],</span> k<span class="op">);</span></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p><strong>Critical detail:</strong>
<code>push(A, B.buckets[k], B.counts[k], k)</code> starts at level k,
not level 0. This correctly reflects that B.buckets[k] represents 2^k
already-reduced blocks.</p>
<p><strong>Why low-to-high order:</strong> Within B’s stack state,
lower-indexed buckets represent more recent (rightward) elements within
B’s range. Processing them first ensures they combine before
higher-indexed (older) buckets, matching the canonical left-to-right
order.</p>
<h3 id="n.8-correctness-argument">N.8 Correctness argument</h3>
<p><strong>Claim:</strong> The multi-threaded algorithm produces a
result bitwise identical to single-threaded shift-reduce for any number
of threads T ≥ 1 and any input size N.</p>
<p><em>Argument:</em></p>
<ol type="1">
<li><p><strong>Shift-reduce correctness:</strong> Single-threaded
shift-reduce evaluates the canonical pairwise-with-carry tree. The push
operation’s carry pattern exactly mirrors binary increment,
corresponding to completing subtrees of size 2^k.</p></li>
<li><p><strong>Alignment property:</strong> For a prefix of length b = m
× 2^k blocks, the stack state has no occupied buckets below level k (see
observation above).</p></li>
<li><p><strong>Merge correctness:</strong> <code>merge_into(A, B)</code>
produces the same state as processing A’s blocks followed by B’s blocks
sequentially. By the alignment property, when B starts at an aligned
boundary, A has no buckets below the alignment level. B’s buckets, when
pushed at their native levels, trigger exactly the carries that would
occur from processing B’s blocks after A’s blocks. The low-to-high
iteration order preserves within-B ordering.</p></li>
<li><p><strong>Combining these:</strong> Thread boundaries are aligned,
so merge correctness applies to each merge. The left-to-right merge
order (thread 0, then 1, then 2, …) matches sequential block
order.</p></li>
</ol>
<h3 id="n.9-complete-algorithm">N.9 Complete Algorithm</h3>
<div class="sourceCode" id="cb65"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="kw">template</span><span class="op">&lt;</span><span class="dt">size_t</span> L<span class="op">&gt;</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="dt">double</span> deterministic_reduce_MT<span class="op">(</span><span class="at">const</span> <span class="dt">double</span><span class="op">*</span> input<span class="op">,</span> <span class="dt">size_t</span> N<span class="op">,</span> <span class="dt">size_t</span> T<span class="op">)</span> <span class="op">{</span></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="dt">size_t</span> num_blocks <span class="op">=</span> <span class="op">(</span>N <span class="op">+</span> L <span class="op">-</span> <span class="dv">1</span><span class="op">)</span> <span class="op">/</span> L<span class="op">;</span></span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="dt">size_t</span> C <span class="op">=</span> choose_chunk_size<span class="op">(</span>num_blocks<span class="op">,</span> T<span class="op">);</span></span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="dt">size_t</span> num_chunks <span class="op">=</span> <span class="op">(</span>num_blocks <span class="op">+</span> C <span class="op">-</span> <span class="dv">1</span><span class="op">)</span> <span class="op">/</span> C<span class="op">;</span></span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>    vector<span class="op">&lt;</span>StackState<span class="op">&lt;</span>L<span class="op">&gt;&gt;</span> states<span class="op">(</span>num_chunks<span class="op">);</span></span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Phase 1: Parallel local reductions</span></span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a>    parallel_for<span class="op">(</span><span class="dv">0</span><span class="op">,</span> num_chunks<span class="op">,</span> <span class="op">[&amp;](</span><span class="dt">size_t</span> chunk<span class="op">)</span> <span class="op">{</span></span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a>        <span class="dt">size_t</span> b0 <span class="op">=</span> chunk <span class="op">*</span> C<span class="op">;</span></span>
<span id="cb65-12"><a href="#cb65-12" aria-hidden="true" tabindex="-1"></a>        <span class="dt">size_t</span> b1 <span class="op">=</span> min<span class="op">(</span>b0 <span class="op">+</span> C<span class="op">,</span> num_blocks<span class="op">);</span></span>
<span id="cb65-13"><a href="#cb65-13" aria-hidden="true" tabindex="-1"></a>        states<span class="op">[</span>chunk<span class="op">]</span> <span class="op">=</span> replay_range<span class="op">(</span>input<span class="op">,</span> N<span class="op">,</span> b0<span class="op">,</span> b1<span class="op">);</span>  <span class="co">// sequential push() over blocks [b0, b1)</span></span>
<span id="cb65-14"><a href="#cb65-14" aria-hidden="true" tabindex="-1"></a>    <span class="op">});</span></span>
<span id="cb65-15"><a href="#cb65-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-16"><a href="#cb65-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Phase 2: Serial merge (left-to-right order)</span></span>
<span id="cb65-17"><a href="#cb65-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">size_t</span> i <span class="op">=</span> <span class="dv">1</span><span class="op">;</span> i <span class="op">&lt;</span> num_chunks<span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb65-18"><a href="#cb65-18" aria-hidden="true" tabindex="-1"></a>        merge_into<span class="op">(</span>states<span class="op">[</span><span class="dv">0</span><span class="op">],</span> states<span class="op">[</span>i<span class="op">]);</span></span>
<span id="cb65-19"><a href="#cb65-19" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb65-20"><a href="#cb65-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-21"><a href="#cb65-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Phase 3: Final fold + cross-lane reduction</span></span>
<span id="cb65-22"><a href="#cb65-22" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span><span class="op">*</span> lane_result <span class="op">=</span> fold<span class="op">(</span>states<span class="op">[</span><span class="dv">0</span><span class="op">]);</span></span>
<span id="cb65-23"><a href="#cb65-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cross_lane_pairwise_reduce<span class="op">(</span>lane_result<span class="op">);</span></span>
<span id="cb65-24"><a href="#cb65-24" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<h3 id="n.10-complexity-analysis">N.10 Complexity Analysis</h3>
<p>The semantics do not require parallel scalability. This section
explains that the canonical expression <em>admits</em> scalable
multi-threaded realizations.</p>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 17%" />
<col style="width: 62%" />
</colgroup>
<thead>
<tr class="header">
<th>Phase</th>
<th>Work</th>
<th>Span (critical path)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Local reduction</td>
<td><code>O(N)</code> total</td>
<td><code>O(N/T)</code></td>
</tr>
<tr class="even">
<td>Ordered merge (sequential)</td>
<td><code>O(T · log N)</code></td>
<td><code>O(T · log N)</code></td>
</tr>
<tr class="odd">
<td>Ordered merge (tree-structured)</td>
<td><code>O(T · log N)</code></td>
<td><code>O(log T · log N)</code></td>
</tr>
<tr class="even">
<td>Final fold</td>
<td><code>O(log N)</code></td>
<td><code>O(log N)</code></td>
</tr>
</tbody>
</table>
<p><strong>Total work:</strong> <code>O(N + T·log N)</code> (typically
<code>O(N)</code> when <code>T &lt;&lt; N</code>)</p>
<p><strong>Span:</strong> With a tree-structured merge,
<code>O(N/T + log T · log N)</code>.</p>
<p><strong>Space:</strong> <code>O(T · L · log N)</code> for per-thread
states (typically a few KB per thread, depending on <code>L</code>).</p>
<h3 id="n.11-simd-optimization-8-block-unrolling">N.11 SIMD
Optimization: 8-Block Unrolling</h3>
<p>A significant optimization reduces stack operation overhead by
processing 8 blocks at once:</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Instead of pushing one block at a time:</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">size_t</span> b <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> b <span class="op">&lt;</span> num_blocks<span class="op">;</span> <span class="op">++</span>b<span class="op">)</span> <span class="op">{</span></span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>    push<span class="op">(</span>S<span class="op">,</span> load_block<span class="op">(</span>b<span class="op">),</span> L<span class="op">,</span> <span class="dv">0</span><span class="op">);</span>  <span class="co">// O(num_blocks) stack operations</span></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a><span class="co">// Process 8 blocks in one SIMD reduction, push at level 3:</span></span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">size_t</span> g <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> g <span class="op">&lt;</span> num_groups_of_8<span class="op">;</span> <span class="op">++</span>g<span class="op">)</span> <span class="op">{</span></span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span> result<span class="op">[</span>L<span class="op">];</span></span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a>    reduce_8_blocks_simd<span class="op">(</span>input <span class="op">+</span> g <span class="op">*</span> <span class="dv">8</span> <span class="op">*</span> L<span class="op">,</span> result<span class="op">);</span>  <span class="co">// 8 blocks → 1 vector</span></span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a>    push<span class="op">(</span>S<span class="op">,</span> result<span class="op">,</span> L<span class="op">,</span> <span class="dv">3</span><span class="op">);</span>  <span class="co">// Start at level 3 (since 8 = 2^3)</span></span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>This reduces stack operations from N/L to N/(8L), yielding
substantial performance improvements. The reference implementation
achieves throughput exceeding <code>std::reduce</code> while maintaining
full determinism.</p>
<h3 id="n.12-performance-observations">N.12 Performance
Observations</h3>
<p>The reference implementation at <strong>[GB-x86-MT]</strong>
demonstrates:</p>
<table>
<colgroup>
<col style="width: 21%" />
<col style="width: 28%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="header">
<th>Variant</th>
<th>Throughput</th>
<th>vs <code>std::accumulate</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>std::accumulate</code></td>
<td>5.4 GB/s</td>
<td>baseline</td>
</tr>
<tr class="even">
<td><code>std::reduce</code></td>
<td>21.4 GB/s</td>
<td>+297%</td>
</tr>
<tr class="odd">
<td><strong>Deterministic ST (L=16, 8-block unroll)</strong></td>
<td><strong>26.5 GB/s</strong></td>
<td><strong>+391%</strong></td>
</tr>
<tr class="even">
<td>Deterministic MT (L=16, T=2)</td>
<td>21.2 GB/s</td>
<td>+293%</td>
</tr>
</tbody>
</table>
<p><strong>Key finding:</strong> With proper SIMD optimization,
deterministic reduction is <strong>faster</strong> than
non-deterministic <code>std::reduce</code> while guaranteeing bitwise
reproducibility. The 8-block unrolling minimizes stack overhead, and the
interleaved lane structure enables efficient vectorized combines.</p>
<h3 id="n.13-implementation-notes">N.13 Implementation Notes</h3>
<p><strong>Thread pool reuse:</strong> For production implementations,
reuse a thread pool rather than spawning threads per invocation. The
reference limits thread creation for Godbolt compatibility.</p>
<p><strong>Parallel merge (optional):</strong> The merge phase can
itself be parallelized using a tree structure:</p>
<pre><code>Initial:    +---+ +---+ +---+ +---+ +---+ +---+ +---+ +---+
            |S₀ | |S₁ | |S₂ | |S₃ | |S₄ | |S₅ | |S₆ | |S₇ |
            +-+-+ +-+-+ +-+-+ +-+-+ +-+-+ +-+-+ +-+-+ +-+-+
              |     |     |     |     |     |     |     |
              +-----+     +-----+     +-----+     +-----+
                |           |           |           |
Stride 1:     +---+       +---+       +---+       +---+
              |S₀₁|       |S₂₃|       |S₄₅|       |S₆₇|  (parallel)
              +-+-+       +-+-+       +-+-+       +-+-+
                |           |           |           |
                +-----------+           +-----------+
                    |                       |
Stride 2:         +-------+               +-------+
                  |S₀₁₂₃  |               |S₄₅₆₇  |      (parallel)
                  +---+---+               +---+---+
                      |                       |
                      +-----------------------+
                              |
Stride 4:                 +-----------+
                          |S₀₁₂₃₄₅₆₇  |                    (final)
                          +-----------+</code></pre>
<p>This reduces the merge critical path from O(T) to O(log T), though
for typical T values the improvement is marginal compared to the
dominant O(N/T) local reduction phase.</p>
<p><strong>Memory efficiency:</strong> Stack states are fixed-size (O(L
× log N) per state) with no heap allocation required during the hot
path. The merge operation modifies A in-place, requiring only
register-level temporaries for the carry chain.</p>
<h3 id="n.14-summary">N.14 Summary</h3>
<p>The multi-threaded stack ordered state merge algorithm achieves:</p>
<table>
<thead>
<tr class="header">
<th>Property</th>
<th>Guarantee</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Determinism</strong></td>
<td>Bitwise identical to single-threaded for any T</td>
</tr>
<tr class="even">
<td><strong>Correctness</strong></td>
<td>Provably equivalent to canonical pairwise tree</td>
</tr>
<tr class="odd">
<td><strong>Efficiency</strong></td>
<td>O(N) work, O(N/T + T log N) span</td>
</tr>
<tr class="even">
<td><strong>Practicality</strong></td>
<td>Demonstrated faster than <code>std::reduce</code> with SIMD</td>
</tr>
</tbody>
</table>
<p>The key insights enabling this approach are:</p>
<ol type="1">
<li>Power-of-2 aligned partitioning eliminates boundary ambiguity</li>
<li>Stack states are complete representations of partial reductions</li>
<li>Pushing at native levels during merge preserves the canonical tree
structure</li>
<li>8-block SIMD unrolling amortizes stack overhead</li>
</ol>
<p>This shows that a fixed evaluation order is compatible with
high-throughput SIMD execution. The constraint of a canonical tree
enables aggressive unrolling and predictable memory access, achieving
throughput comparable to unconstrained implementations in the tested
configurations.</p>
<p>GPU implementations may place the O(log N) working state per lane in
shared memory rather than thread-local registers, avoiding register
pressure while preserving evaluation of the identical canonical
expression. This is a standard GPU reduction pattern and does not
require changes to the semantic contract.</p>
<h2 id="appendix-o-recursive-bisection-balanced-tree-construction-informative">Appendix
O: Recursive Bisection (“Balanced”) Tree Construction (Informative)</h2>
<p>This appendix records a previously-considered alternative canonical
tree construction based on recursive bisection. It is
<strong>not</strong> part of the normative contract in §4; this paper
specifies iterative pairwise (§4.2.3) as the sole canonical tree
definition.</p>
<h3 id="o.1-definition">O.1 Definition</h3>
<p>Define <code>CANONICAL_TREE_EVAL_RECURSIVE(op, Y[0..k))</code>, where
<code>k &gt;= 1</code> and each <code>Y[t]</code> is in
<code>maybe&lt;A&gt;</code>:</p>
<pre><code>CANONICAL_TREE_EVAL_RECURSIVE(op, Y[0..k)):
    if k == 1:
        return Y[0]
    let m = floor(k / 2)
    return COMBINE(op,
        CANONICAL_TREE_EVAL_RECURSIVE(op, Y[0..m)),
        CANONICAL_TREE_EVAL_RECURSIVE(op, Y[m..k))
    )</code></pre>
<h3 id="o.2-equivalence-on-power-of-two-sizes">O.2 Equivalence on
power-of-two sizes</h3>
<p>For <code>k = 2^n</code> (k = 2, 4, 8, …), recursive bisection and
iterative pairwise produce identical trees, and therefore define
identical abstract expressions.</p>
<h3 id="o.3-differences-on-non-power-of-two-sizes">O.3 Differences on
non-power-of-two sizes</h3>
<p>For non-power-of-two <code>k</code>, the trees differ in structure
(but keep the same asymptotic depth bounds).</p>
<p>Example (<code>k = 5</code>), writing <code>+</code> for
<code>op</code>:</p>
<ul>
<li>Recursive bisection:
<ul>
<li><code>(e0 + e1) + (e2 + (e3 + e4))</code></li>
</ul></li>
<li>Iterative pairwise:
<ul>
<li><code>((e0 + e1) + (e2 + e3)) + e4</code></li>
</ul></li>
</ul>
<h3 id="o.4-rationale-for-selecting-iterative-pairwise">O.4 Rationale
for selecting iterative pairwise</h3>
<p>This paper selects iterative pairwise as the sole canonical tree
definition for the following reasons:</p>
<ul>
<li><strong>Industry alignment:</strong> SIMD/GPU deterministic
reductions commonly use adjacent pairwise combination patterns.</li>
<li><strong>Direct hardware mapping:</strong> iterative pairing
corresponds directly to SIMD-lane and warp-lane pairing operations.</li>
<li><strong>Adoption continuity:</strong> existing deterministic
implementations can conform without structural change.</li>
<li><strong>Specification focus:</strong> once a single canonical tree
is selected, keeping alternatives in the normative contract increases
complexity without adding semantic value.</li>
</ul>
<p>No other grouping is permitted.</p>
<h2 id="appendix-p-performance-and-regret-informative">Appendix P:
Performance and Regret (Informative)</h2>
<p>This appendix records performance and regret considerations for the
choice of iterative pairwise (shift-reduce) as the canonical tree
construction rule (§3.8). These considerations are informative and do
not affect the semantic contract specified in §4–§5.</p>
<h3 id="p.1-performance-comparison-iterative-pairwise-vs-recursive-bisection">P.1
Performance comparison: iterative pairwise vs recursive bisection</h3>
<p>Because both trees have identical depth for power-of-two sizes and
nearly identical depth otherwise, their throughput is similar on modern
hardware. Recursive bisection can be implemented with a direct unrolled
mapping for small sub-problems (e.g., a flat switch for N ≤ 32
eliminates recursive call overhead), and unaligned SIMD loads on
contemporary microarchitectures are essentially free when data does not
cross a cache line boundary — reducing the alignment advantage that
earlier analyses relied on [Dalton2014]. The iterative formulation keeps
structural advantages (loop-based with a branchless bit-test for
reduction, natural streaming order), but these translate to modest
rather than dramatic throughput differences against a well-engineered
recursive implementation.</p>
<p>This approximate throughput equivalence between the two candidates
means that performance alone cannot distinguish them — and the decision
appropriately falls to the axes where they do differ: industry practice
alignment, executor compatibility, and the fact that iterative pairwise
is what existing SIMD and GPU reduction libraries already ship. This
paper does not claim iterative pairwise is faster than recursive
bisection; it claims that, at equivalent performance and identical error
bounds, the tree that matches existing practice is the lower-risk
standard choice.</p>
<p><strong>Executor compatibility.</strong> Once the expression is
separated from execution (§3.0), the question becomes: which expression
can executors naturally evaluate? Iterative pairwise produces a local,
lane-structured expression that maps directly to executor chunking and
work-graph execution without requiring the full tree to be materialized.
Recursive bisection produces a globally-recursive structure that is
harder to realize incrementally. In executor terms, iterative pairwise
defines an expression that executors can evaluate without first building
the tree.</p>
<h3 id="p.2-standard-regret">P.2 Standard regret</h3>
<p>Once standardized, the tree shape becomes part of the language
contract and cannot be changed without breaking code that depends on
specific results. The consequence of choosing iterative pairwise is
commitment to a specific non-power-of-two boundary behavior. For
power-of-two sizes the two candidates are identical; the commitment
applies only to non-power-of-two boundary cases, where iterative
pairwise matches existing SIMD/GPU practice. Of the two candidates,
iterative pairwise is most closely aligned with existing
implementations.</p>
<h3 id="p.3-upper-bound-on-regret">P.3 Upper bound on regret</h3>
<p>Regardless of tree shape, any balanced binary reduction has the same
O(log N · ε) error bound [Higham2002]. No alternative tree can improve
the asymptotic accuracy. On the throughput side, iterative pairwise
already achieves 89% of the theoretical peak [Dalton2014]. Even if a
superior tree shape were discovered in the future, the largest possible
throughput improvement over IPR is bounded at approximately 11%, with
identical error bounds.</p>
<h3 id="p.4-measured-throughput-cost">P.4 Measured throughput cost</h3>
<p>The practical performance cost of iterative pairwise (shift-reduce)
summation has been measured by Dalton, Wang &amp; Blainey [Dalton2014].
Their SIMD-optimized implementation achieves 89% of the throughput of
unconstrained naïve summation when data is not resident in L1 cache (86%
from L2, 90% streaming from memory), while providing the O(log N · ε)
error bound of pairwise summation — and twice the throughput of the
best-tuned compensated sums (Kahan-Babuška).</p>
<p>The more telling comparison is against the current deterministic
baseline. Today, the only standard facility with a fully specified
expression is <code>std::accumulate</code>, which is a strict left fold
with a loop-carried dependency chain. It cannot use SIMD parallelism: on
AVX-512 hardware, a left fold occupies 1 of 8 <code>double</code> lanes
(~12% SIMD utilization) or 1 of 16 <code>float</code> lanes (~6%). The
canonical iterative pairwise tree, by contrast, is inherently
SIMD-friendly — all lanes active, with measured throughput at ~89% of
peak. This represents approximately a <strong>7× improvement</strong> in
the deterministic reduction path for <code>double</code> on AVX-512 (and
~14× for <code>float</code>). The relevant comparison is not “what
throughput is lost relative to unconstrained reduction” but “what
throughput is gained relative to the only specified expression available
today (<code>std::accumulate</code>).” By that measure, the canonical
tree recovers nearly all of the available SIMD parallelism.</p>
<p>The throughput and regret analyses above establish that neither
candidate has a decisive performance advantage. The distinguishing
property is architectural: iterative pairwise is the only common
candidate that can be evaluated incrementally without global sizing — a
structural alignment with the sender/receiver execution model adopted
for C++26 (see §3.8.2 and §3.10).</p>
<h2 id="appendix-q-historical-note-on-span-based-topology-informative">Appendix
Q: Historical Note on Span-Based Topology (Informative)</h2>
<p>Earlier explorations of this facility considered specifying the
reduction topology in terms of a fixed byte span <code>M</code>, with
the corresponding lane count derived as:</p>
<pre><code>L = M / sizeof(value_type)</code></pre>
<p>This formulation aligns with common implementation concerns such as
cache-line size, SIMD register width, or hardware batch size, and may be
convenient for numerics workflows that target a specific memory or
vector width.</p>
<p>However, because <code>sizeof(value_type)</code> is
implementation-defined, a span-based topology does not, in general,
select the same abstract expression across different platforms or ABIs.
For example, <code>sizeof(long double)</code> may differ between
implementations on the same architecture, causing the same source-level
span value <code>M</code> to produce different lane counts
<code>L</code> and therefore different parenthesizations of the
reduction expression.</p>
<p>Since the topology coordinate intentionally determines the abstract
expression evaluated for non-associative operations, such variation
would undermine the cross-platform expression identity this proposal
seeks to provide.</p>
<p>For this reason, the semantic topology parameter in §4 is defined
solely in terms of the lane count <code>L</code>. API designs that
derive <code>L</code> from a byte span may be considered in future
revisions as layout-oriented conveniences for environments where
representation size is known and fixed, but such facilities would not,
in general, denote the same abstract expression across
implementations.</p>
</body>
</html>
